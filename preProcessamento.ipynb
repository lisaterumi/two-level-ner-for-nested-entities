{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerando arquivos para NER-NestedClinBr\n",
    "\n",
    "Deixar sempre o arquivo de test para fazer o teste completo (com o pipeline todo)\n",
    "1o. -> passar pelo modelo de NER normal (binario), pegar entidades nivel externo\n",
    "2o. -> novo modelo de NER para entidades internas\n",
    "\n",
    "Descontinuas: tentar RBERT - relação entre entidades... pensar em como fazer.. pra nao ficar pesado... se estiver desbalanceado, dar menor peso para label O no crossentropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pickle\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTiposEntidade():\n",
    "    return ['Problema','Teste','Tratamento','Anatomia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceWhiteSpaces(str):\n",
    "    return re.sub('\\s{2,}',' ',str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(name, obj):\n",
    "    existeDir = os.path.exists('obj')\n",
    "    if not existeDir:\n",
    "        os.makedirs('obj')\n",
    "    with open('obj/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_obj(name):\n",
    "    existeDir = os.path.exists('obj')\n",
    "    if not existeDir:\n",
    "        os.makedirs('obj')\n",
    "    try:\n",
    "        with open('obj/' + name + '.pkl', 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    except:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pastasCorpus = [r'C:\\Users\\lisat\\OneDrive\\jupyter notebook\\spanclassification\\preProcessamento\\corpus\\test',r'C:\\Users\\lisat\\OneDrive\\jupyter notebook\\spanclassification\\preProcessamento\\corpus\\train']\n",
    "#pastasCorpus = [r'corpus\\test']\n",
    "#pastasCorpus = [r'corpus\\TESTE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make the World a 2y4Better Place2y0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "s = 'Make the World a 2.4Better Place2.0'\n",
    "pattern = r'([0-9])\\.([0-9])'\n",
    "replacement = r'\\1y\\2'\n",
    "html = re.sub(pattern, replacement, s)\n",
    "\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIndicesString(indices):\n",
    "    tam = len(indices)\n",
    "    return float(str(indices[-1])+'.'+str(tam))\n",
    "\n",
    "def ordenaEntidades(matrix_ficheiro):\n",
    "    # Recebendo a primeira linha da matriz (Nome, Coordenada X, Coordenada Y, Peso)\n",
    "    # Salvando em uma nova matriz que estará ordenada\n",
    "    matrix_ordenada = []\n",
    "    \n",
    "    matrix_ficheiro = [[m[0], m[1], m[2], getIndicesString(m[1])] for m in matrix_ficheiro]\n",
    "\n",
    "    # Ordenando pela segunda coluna da matrix \n",
    "    sub_matrix_ordenada = sorted(matrix_ficheiro[0:], key=itemgetter(3))\n",
    "\n",
    "    # Percorrendo a sub matriz e adicionando cada elemento na nova matriz ordenada\n",
    "    for elem in sub_matrix_ordenada:\n",
    "        matrix_ordenada.append([elem[0],elem[1],elem[2]])\n",
    "\n",
    "    return matrix_ordenada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# com todas entidades corretamente (descontinuas)\n",
    "def getDicSentencesGabarito(pastaCorpus):\n",
    "    #devePrintar=True\n",
    "    devePrintar=False\n",
    "    dic_sentences = {}\n",
    "    numMaxTokensPorFrase=0 # numero tokens da maior frase\n",
    "    numMaxTokensPorEntidade=0 # numero tokens da maior entidade\n",
    "    num=0\n",
    "    listaEntidades=[]\n",
    "    frasesComDescontinuas=[]\n",
    "    for filename in os.listdir(pastaCorpus):\n",
    "        f = os.path.join(pastaCorpus, filename)\n",
    "        if os.path.isfile(f):\n",
    "            fileNameSemExtensao=os.path.splitext(filename)[0]\n",
    "            if devePrintar:\n",
    "                print('\\n\\n--fileName (sem extensao):--', fileNameSemExtensao)\n",
    "                pass\n",
    "            extension = os.path.splitext(filename)[1][1:]\n",
    "            if extension=='ann':\n",
    "                # tokens da frase\n",
    "                fileTxt = open(os.path.join(pastaCorpus, fileNameSemExtensao)+'.txt', \"r\", encoding='utf-8')\n",
    "                linha=fileTxt.readlines()\n",
    "                fileTxt.close()\n",
    "                if devePrintar:\n",
    "                    print('linha:', linha)\n",
    "                    pass\n",
    "                frases=[]\n",
    "                allFrasesString=''\n",
    "                numL=0\n",
    "                for l in linha:\n",
    "                    numL=numL+1\n",
    "                    allFrasesString = allFrasesString+l\n",
    "                    if numL==1: # descarta primeira frase, que é data de criação do doc\n",
    "                        #print('descartando frase:', l)\n",
    "                        continue\n",
    "                    if l.strip() and l.strip()!='\\n':\n",
    "                        #print('l:', l)\n",
    "                        pattern = r'([0-9])\\.([0-9])'\n",
    "                        replacement = r'\\1==\\2'\n",
    "                        novoL = re.sub(pattern, replacement, l.strip())\n",
    "                        l2 = novoL.split('.') # quebrando frases\n",
    "                        for l3 in l2:\n",
    "                            if l3.strip() and l3.strip()!='\\n':\n",
    "                                novaFrase = l3.replace('\\n','').replace('==','.').strip()+'.'\n",
    "                                frases.append(novaFrase)\n",
    "                #print('frases:', frases)\n",
    "                # para cada frase\n",
    "                # para tokenizar nesses tokens\n",
    "                #print('allFrasesString:', allFrasesString)\n",
    "                frasesTokens={}\n",
    "                #numCaracteresTotal=42 # primeira frase ignorada\n",
    "                numIndiceAnterior=0\n",
    "                for frase in frases:\n",
    "                    tokens=[]\n",
    "                    frase2 = frase.strip().replace('/',' / ').replace(')',' ) ').replace('(',' ( ').replace(']',' ] ').replace('[',' [ ').replace(',',' , ').replace('.',' . ').replace(';',' ; ').replace('-',' - ').replace('+',' + ').replace(\"'\",\" ' \").replace(\" = \",\" = \").replace(\"#\",\" # \").replace(\" $ \",\" $ \").replace(\" ! \",\" ! \").replace(\"?\",\" ? \").replace(\"%\",\" % \").replace(\":\",\" : \").replace(\">\",\" > \").replace(\"<\",\" < \")\n",
    "                    frase2 = replaceWhiteSpaces(frase2)\n",
    "                    frase2 = frase2.split()\n",
    "                    for numtoken, token in enumerate(frase2):\n",
    "                        if devePrintar:\n",
    "                            print('token:', token)\n",
    "                            print('numIndiceAnterior:', numIndiceAnterior)\n",
    "                        if token!='.':\n",
    "                            numCaracteresTotal = allFrasesString.find(token, numIndiceAnterior, len(allFrasesString))\n",
    "                        else:\n",
    "                            numCaracteresTotal=numIndiceAnterior+1\n",
    "                        #tokens.append([token,numtoken])\n",
    "                        tokens.append([token,numtoken, numCaracteresTotal])\n",
    "                        numIndiceAnterior = numCaracteresTotal+len(token)-1\n",
    "                        if numMaxTokensPorFrase<len(tokens):\n",
    "                            numMaxTokensPorFrase = len(tokens)\n",
    "                    frasesTokens[frase]=tokens\n",
    "                    #linhaTokens=linha.copy()    \n",
    "                if devePrintar:\n",
    "                    print('frasesTokens:', frasesTokens)\n",
    "                # agora, as entidades\n",
    "                fileAnn = open(f, \"r\", encoding='utf-8')\n",
    "                linha=fileAnn.readlines()\n",
    "                fileAnn.close()\n",
    "\n",
    "                dicEntidades={}\n",
    "                dicEntidadesDescontinuas={}\n",
    "                listaEntidadesDescontinuas={}\n",
    "                numIndiceDesc=0\n",
    "                for entidade_linha in linha:\n",
    "                    if ';' not in entidade_linha: # nao é descontinua\n",
    "                        entidade = entidade_linha.split('\\t')\n",
    "                        tipo_entidade = entidade[1]\n",
    "                        inicio, fim = tipo_entidade.split()[1:3]\n",
    "                        tipo_entidade = tipo_entidade.split()[0]\n",
    "                        termos_entidade = entidade[2].replace('\\n','')\n",
    "                        termos_entidade = termos_entidade.strip().replace('/',' / ').replace(')',' ) ').replace('(',' ( ').replace(']',' ] ').replace('[',' [ ').replace(',',' , ').replace('.',' . ').replace(';',' ; ').replace('-',' - ').replace('+',' + ').replace(\"'\",\" ' \").replace(\" = \",\" = \").replace(\"#\",\" # \").replace(\" $ \",\" $ \").replace(\" ! \",\" ! \").replace(\"?\",\" ? \").replace(\"%\",\" % \").replace(\":\",\" : \").replace(\">\",\" > \").replace(\"<\",\" < \")\n",
    "                        dicEntidades[(int(inicio), int(fim))]=[tipo_entidade, replaceWhiteSpaces(termos_entidade)]\n",
    "                    else:\n",
    "                        frasesComDescontinuas.append(num)\n",
    "                        #print('descontinua, linha: {}, num: {}'.format(linha, num))\n",
    "                        #print('descontinua, file: {}, num: {}'.format(filename, num))\n",
    "                        \n",
    "                        # grava com os indices corretoss\n",
    "                        entidade = entidade_linha.split('\\t')\n",
    "                        # ex T10\tProblema 244 252;279 306\tdispneia aos mdoeardos-leves esforço\n",
    "                        #Problema 244 252;279 306\n",
    "                        entidade_temp=entidade[1].split(';')\n",
    "                        entidade1=entidade_temp[0]\n",
    "                        tipo_entidade = entidade1\n",
    "                        inicio1, fim1 = tipo_entidade.split()[1:3]\n",
    "                        tipo_entidade_string = tipo_entidade.split()[0]\n",
    "                        # mandar só os termos referentes...\n",
    "                        tamTermo1=int(fim1)-int(inicio1)\n",
    "                        termos_entidade = entidade[2].replace('\\n','')\n",
    "                        termos_entidade=termos_entidade[:tamTermo1]\n",
    "                        termos_entidade = termos_entidade.strip().replace('/',' / ').replace(')',' ) ').replace('(',' ( ').replace(']',' ] ').replace('[',' [ ').replace(',',' , ').replace('.',' . ').replace(';',' ; ').replace('-',' - ').replace('+',' + ').replace(\"'\",\" ' \").replace(\" = \",\" = \").replace(\"#\",\" # \").replace(\" $ \",\" $ \").replace(\" ! \",\" ! \").replace(\"?\",\" ? \").replace(\"%\",\" % \").replace(\":\",\" : \").replace(\">\",\" > \").replace(\"<\",\" < \")\n",
    "                        #print('aaaaaaaaaaaaaaaaa')\n",
    "                        \n",
    "                        dicEntidadesDescontinuas[(int(inicio1), int(fim1))]=[tipo_entidade_string, replaceWhiteSpaces(termos_entidade)]\n",
    "                        #print(\"(int(inicio1), int(fim1)):\", (int(inicio1), int(fim1)))\n",
    "                        #print(\"tipo_entidade_string, replaceWhiteSpaces(termos_entidade):\", tipo_entidade_string, replaceWhiteSpaces(termos_entidade))\n",
    "                        \n",
    "                        entidade2=entidade_temp[1]\n",
    "                        inicio2, fim2 = entidade2.split()[0:2]\n",
    "                        termos_entidade = entidade[2].replace('\\n','')\n",
    "                        termos_entidade=termos_entidade[tamTermo1:len(termos_entidade)]\n",
    "                        termos_entidade = termos_entidade.strip().replace('/',' / ').replace(')',' ) ').replace('(',' ( ').replace(']',' ] ').replace('[',' [ ').replace(',',' , ').replace('.',' . ').replace(';',' ; ').replace('-',' - ').replace('+',' + ').replace(\"'\",\" ' \").replace(\" = \",\" = \").replace(\"#\",\" # \").replace(\" $ \",\" $ \").replace(\" ! \",\" ! \").replace(\"?\",\" ? \").replace(\"%\",\" % \").replace(\":\",\" : \").replace(\">\",\" > \").replace(\"<\",\" < \")\n",
    "                        dicEntidadesDescontinuas[(int(inicio2), int(fim2))]=[tipo_entidade_string, replaceWhiteSpaces(termos_entidade)]\n",
    "                        #print(\"(int(inicio2), int(fim2)):\", (int(inicio2), int(fim2)))\n",
    "                        #print(\"tipo_entidade_string, replaceWhiteSpaces(termos_entidade):\", tipo_entidade_string, replaceWhiteSpaces(termos_entidade))\n",
    "                        listaEntidadesDescontinuas[numIndiceDesc] = dicEntidadesDescontinuas\n",
    "                        dicEntidadesDescontinuas={}\n",
    "                        numIndiceDesc=numIndiceDesc+1\n",
    "                        \n",
    "                        \n",
    "                #print('--listaEntidadesDescontinuas:--', listaEntidadesDescontinuas)\n",
    "                        \n",
    "                #print('frasesTokens:', frasesTokens)\n",
    "                indicesDic = sorted(dicEntidades.keys(), key = lambda item: item[0])\n",
    "                #indicesDicDescontinuas = sorted(dicEntidadesDescontinuas.keys(), key = lambda item: item[0])\n",
    "                listaIndicesJaUsados = []\n",
    "                #list_students.sort(key = lambda x: x[1])   #index 1 means second element\n",
    "                #print('indicesDicDescontinuas:', indicesDicDescontinuas)\n",
    "                #print('dicEntidades:', dicEntidades)\n",
    "                \n",
    "                for key, value in frasesTokens.items():\n",
    "                    #print('key:', key)\n",
    "                    #print('value:', value)\n",
    "                    for i in indicesDic:\n",
    "                        tipo_entidade,termos_entidade = dicEntidades[i]\n",
    "                        ##key (i) = indices old a comparar\n",
    "                        #print('i:', i)\n",
    "                        for token in value:\n",
    "                            #print('token:', token)\n",
    "                            if i[0]==token[2]:\n",
    "                                novo_inicio, novo_fim = [token[1],token[1]+len(termos_entidade.split())]\n",
    "                                novos_indices=[]\n",
    "                                for k in range(novo_inicio,novo_fim):\n",
    "                                    novos_indices.append(k)\n",
    "                                listaEntidades.append([termos_entidade, novos_indices, tipo_entidade])\n",
    "                                if len(termos_entidade.split()) > numMaxTokensPorEntidade:\n",
    "                                    numMaxTokensPorEntidade = len(termos_entidade.split())\n",
    "                            else:\n",
    "                                #print('else, i[0]:',i[0])\n",
    "                                pass\n",
    "                            \n",
    "                    # agora, descontinuas\n",
    "                    for keyD, valueD in listaEntidadesDescontinuas.items():\n",
    "                        #print('valueD>', valueD)\n",
    "                        listTemp= list(valueD)\n",
    "                        indice1=listTemp[0]\n",
    "                        indice2=listTemp[1]\n",
    "                        tipo_entidade1,termos_entidade1 = valueD[indice1]\n",
    "                        tipo_entidade2,termos_entidade2 = valueD[indice2]\n",
    "                        #print('indice1:{},tipo_entidade1:{},termos_entidade1:{},indice2:{},tipo_entidade2:{},termos_entidade2:{}'.format(indice1, tipo_entidade1,termos_entidade1,indice2, tipo_entidade2,termos_entidade2))\n",
    "                        achou=0\n",
    "                        token1=''\n",
    "                        for token in value:\n",
    "                            #print('token:', token)\n",
    "                            if indice1[0]==token[2]: # indice igual na primeira palavra\n",
    "                                achou=1\n",
    "                                token1 = token\n",
    "                            if achou==1:\n",
    "                                if indice2[0]==token[2]: # segunda palavra bate tb\n",
    "                                    novo_inicio1, novo_fim1 = [token1[1],token1[1]+len(termos_entidade1.split())]\n",
    "                                    novo_inicio2, novo_fim2 = [token[1],token[1]+len(termos_entidade2.split())]\n",
    "                                    novos_indices=[]\n",
    "                                    for k in range(novo_inicio1,novo_fim1):\n",
    "                                        novos_indices.append(k)\n",
    "                                    for k in range(novo_inicio2,novo_fim2):\n",
    "                                        novos_indices.append(k)\n",
    "                                    listaEntidades.append([termos_entidade1+' '+termos_entidade2, novos_indices, tipo_entidade1])\n",
    "                                    #print('[termos_entidade1+' '+termos_entidade2, novos_indices, tipo_entidade1]', [termos_entidade1+' '+termos_entidade2, novos_indices, tipo_entidade1])\n",
    "                        \n",
    "                    if len(value)>0:\n",
    "                        #print('incluindo:', key)\n",
    "                        #dic_sentences[num]=[value, listaEntidades]\n",
    "                        dic_sentences[num]=[value, ordenaEntidades(listaEntidades)]\n",
    "                        listaEntidades=[]\n",
    "                        num=num+1 \n",
    "\n",
    "        #print(num)\n",
    "        #if num>318:\n",
    "        #    break\n",
    "\n",
    "        #if num>5:\n",
    "        #    break\n",
    "    print('numMaxTokensPorFrase:', numMaxTokensPorFrase)\n",
    "    print('numMaxTokensPorEntidade:', numMaxTokensPorEntidade)\n",
    "    \n",
    "    return dic_sentences, frasesComDescontinuas\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tam: 1\n",
      "100,1\n",
      "tam: 4\n",
      "107,4\n",
      "tam: 1\n",
      "109,1\n",
      "tam: 4\n",
      "53,4\n",
      "tam: 1\n",
      "113,1\n",
      "tam: 1\n",
      "118,1\n",
      "tam: 2\n",
      "46,2\n",
      "tam: 2\n",
      "53,2\n",
      "[['MDRD', [100], 'Teste', 100.1], ['ICC diastólica classe I', [104, 105, 106, 107], 'Problema', 107.4], ['HAS', [109], 'Problema', 109.1], ['DMII', [50, 51, 52, 53], 'Problema', 53.4], ['Dislipidemia', [113], 'Problema', 113.1], ['medicação', [118], 'Tratamento', 118.1], ['abdome massas', [37, 46], 'Problema', 46.2], ['MMII edema', [51, 53], 'Problema', 53.2]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['abdome massas', [37, 46], 'Problema'],\n",
       " ['MMII edema', [51, 53], 'Problema'],\n",
       " ['DMII', [50, 51, 52, 53], 'Problema'],\n",
       " ['MDRD', [100], 'Teste'],\n",
       " ['ICC diastólica classe I', [104, 105, 106, 107], 'Problema'],\n",
       " ['HAS', [109], 'Problema'],\n",
       " ['Dislipidemia', [113], 'Problema'],\n",
       " ['medicação', [118], 'Tratamento']]"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "def getIndicesString(indices):\n",
    "    tam = len(indices)\n",
    "    #print('tam:', tam)\n",
    "    #print(str(indices[-1])+','+str(tam))\n",
    "    return float(str(indices[-1])+'.'+str(tam))\n",
    "\n",
    "matrix_ficheiro=[['ATRIO ESQUERDO E DIREITO DILATADOS', [0, 1, 2, 3, 4], 'Problema'],\n",
    "   ['ATRIO ESQUERDO', [0, 1], 'Anatomia'],\n",
    "   ['ATRIO DIREITO', [0, 3], 'Anatomia']]\n",
    "\n",
    "matrix_ficheiro=[['MMII', [0], 'Anatomia'],\n",
    "  ['panturrilhas', [1], 'Anatomia'],\n",
    "  ['MMII edema', [0, 2], 'Problema']]\n",
    "\n",
    "matrix_ficheiro = [['MDRD', [100], 'Teste'],\n",
    "   ['ICC diastólica classe I', [104, 105, 106, 107], 'Problema'],\n",
    "   ['HAS', [109], 'Problema'],\n",
    "   ['DMII', [50, 51, 52, 53], 'Problema'],\n",
    "   ['Dislipidemia', [113], 'Problema'],\n",
    "   ['medicação', [118], 'Tratamento'],\n",
    "   ['abdome massas', [37, 46], 'Problema'],\n",
    "   ['MMII edema', [51, 53], 'Problema']]\n",
    "\n",
    "# Recebendo a primeira linha da matriz (Nome, Coordenada X, Coordenada Y, Peso)\n",
    "# Salvando em uma nova matriz que estará ordenada\n",
    "matrix_ordenada = []\n",
    "\n",
    "matrix_ficheiro = [[m[0], m[1], m[2], getIndicesString(m[1])] for m in matrix_ficheiro]\n",
    "print(matrix_ficheiro)\n",
    "\n",
    "# Ordenando pela segunda coluna da matrix (desconsiderando a primeira linha)\n",
    "sub_matrix_ordenada = sorted(matrix_ficheiro[0:], key=itemgetter(3))\n",
    "\n",
    "# Percorrendo a sub matriz e adicionando cada elemento na nova matriz ordenada\n",
    "for elem in sub_matrix_ordenada:\n",
    "    matrix_ordenada.append([elem[0],elem[1],elem[2]])\n",
    "    \n",
    "matrix_ordenada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['MMII', [0], 'Anatomia'],\n",
       " ['MMII edema', [0, 2], 'Problema'],\n",
       " ['panturrilhas', [0, 4], 'Anatomia']]"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordenaEntidades([['MMII', [0], 'Anatomia'],\n",
    "  ['panturrilhas', [0, 4], 'Anatomia'],\n",
    "  ['MMII edema', [0, 2], 'Problema']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Treinamento:-- C:\\Users\\lisat\\OneDrive\\jupyter notebook\\spanclassification\\preProcessamento\\corpus\\train\n",
      "numMaxTokensPorFrase: 192\n",
      "numMaxTokensPorEntidade: 27\n",
      "len(sentences): 1736\n",
      "len(Descontinuas): 91\n",
      "len(frasesComDescontinuas) unicas: 51\n",
      "--Teste:-- C:\\Users\\lisat\\OneDrive\\jupyter notebook\\spanclassification\\preProcessamento\\corpus\\test\n",
      "numMaxTokensPorFrase: 146\n",
      "numMaxTokensPorEntidade: 18\n",
      "len(sentences): 506\n",
      "len(Descontinuas): 44\n",
      "len(frasesComDescontinuas) unicas: 17\n"
     ]
    }
   ],
   "source": [
    "print('--Treinamento:--', pastasCorpus[1])\n",
    "dic_sentencesTrain, frasesComDescontinuasTrain = getDicSentencesGabarito(pastasCorpus[1])\n",
    "print('len(sentences):', len(dic_sentencesTrain))\n",
    "print('len(Descontinuas):',len(frasesComDescontinuasTrain))\n",
    "print('len(frasesComDescontinuas) unicas:',len(set(frasesComDescontinuasTrain)))\n",
    "\n",
    "\n",
    "print('--Teste:--', pastasCorpus[0])\n",
    "dic_sentencesTest, frasesComDescontinuasTest = getDicSentencesGabarito(pastasCorpus[0])\n",
    "print('len(sentences):', len(dic_sentencesTest))\n",
    "print('len(Descontinuas):',len(frasesComDescontinuasTest))\n",
    "print('len(frasesComDescontinuas) unicas:',len(set(frasesComDescontinuasTest)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['MMII', 0, 695],\n",
       "  ['sem', 1, 700],\n",
       "  ['edema', 2, 704],\n",
       "  [',', 3, 709],\n",
       "  ['panturrilhas', 4, 711],\n",
       "  ['livres', 5, 724],\n",
       "  ['.', 6, 730]],\n",
       " [['MMII', [0], 'Anatomia'],\n",
       "  ['MMII edema', [0, 2], 'Problema'],\n",
       "  ['panturrilhas', [4], 'Anatomia']]]"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_sentencesTest[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tamanhoTrain 1319\n",
      "tamanhoDev 417\n",
      "len(dic_sentencesTrain): 1319\n",
      "len(dic_sentencesDev): 416\n",
      "[[['Paciente', 0, 151], ['relata', 1, 160], ['apenas', 2, 167], ['um', 3, 174], ['episodio', 4, 177], ['no', 5, 186], ['momento', 6, 189], ['de', 7, 197], ['gripe', 8, 200], ['.', 9, 205]], [['gripe', [8], 'Problema']]]\n",
      "[[['HAS', 0, 207], [',', 1, 210], ['ICC', 2, 212], [',', 3, 215], ['nega', 4, 217], ['DM', 5, 222], ['.', 6, 224]], [['HAS', [0], 'Problema'], ['ICC', [2], 'Problema'], ['DM', [5], 'Problema']]]\n"
     ]
    }
   ],
   "source": [
    "save_obj('dic_sentencesTrainDevGabarito',dic_sentencesTrain)\n",
    "porc=0.76\n",
    "tamanhoTotal = len(dic_sentencesTrain)\n",
    "tamanhoTrain = int(tamanhoTotal*porc)\n",
    "print('tamanhoTrain', tamanhoTrain)\n",
    "tamanhoDev = tamanhoTotal - tamanhoTrain\n",
    "print('tamanhoDev', tamanhoDev)\n",
    "dic_sentencesDev_temp = {k: dic_sentencesTrain[k] for k in list(dic_sentencesTrain)[tamanhoTrain:-1]}\n",
    "dic_sentencesTrain = {k: dic_sentencesTrain[k] for k in list(dic_sentencesTrain)[:tamanhoTrain]}\n",
    "num=0\n",
    "dic_sentencesDev = {}\n",
    "for key, value in dic_sentencesDev_temp.items():\n",
    "    dic_sentencesDev[num] = value\n",
    "    num=num+1\n",
    "\n",
    "print('len(dic_sentencesTrain):', len(dic_sentencesTrain))\n",
    "print('len(dic_sentencesDev):', len(dic_sentencesDev))\n",
    "print(dic_sentencesTrain[tamanhoTrain-1])\n",
    "print(dic_sentencesDev[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj('dic_sentencesTrainGabarito',dic_sentencesTrain)\n",
    "save_obj('dic_sentencesDevGabarito',dic_sentencesDev)\n",
    "save_obj('dic_sentencesTestGabarito',dic_sentencesTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "def checkConsecutive(l):\n",
    "    return sorted(l) == list(range(min(l), max(l)+1))\n",
    "      \n",
    "# Driver Code\n",
    "lst = [2, 3, 1, 4, 7]\n",
    "print(checkConsecutive(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Dispneia', 0, 43],\n",
       "  ['importante', 1, 52],\n",
       "  ['aos', 2, 63],\n",
       "  ['esforços', 3, 67],\n",
       "  ['+', 4, 76],\n",
       "  ['dor', 5, 78],\n",
       "  ['tipo', 6, 82],\n",
       "  ['peso', 7, 87],\n",
       "  ['no', 8, 92],\n",
       "  ['peito', 9, 95],\n",
       "  ['no', 10, 101],\n",
       "  ['esforço', 11, 104],\n",
       "  ['.', 12, 111]],\n",
       " [['Dispneia importante aos esforços', [0, 1, 2, 3], 'Problema'],\n",
       "  ['peito', [9], 'Anatomia'],\n",
       "  ['dor tipo peso no peito no esforço', [5, 6, 7, 8, 9, 10, 11], 'Problema']]]"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_sentencesTrain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "descontinuas em train:\n",
      "['dispneia mdoeardos - leves esforço', [3, 10, 11, 12, 13], 'Problema']\n",
      "['ATC CD', [1, 5], 'Tratamento']\n",
      "['MMII PULSOS REDUZIDOS BILAT', [46, 48, 49, 50], 'Problema']\n",
      "['MMII EDEMA', [46, 53], 'Problema']\n",
      "['MMII EMPASTAMENTO', [46, 55], 'Problema']\n",
      "['abdome massas', [37, 46], 'Problema']\n",
      "['MMII edema', [51, 53], 'Problema']\n",
      "['pulmonar crepitação em região interscapular', [1, 3, 4, 5, 6], 'Problema']\n",
      "['cardiovascular Bulhas não audíveis', [1, 3, 4, 5], 'Problema']\n",
      "['RM COM 2 SAFENAS E 1 MAMARIA CIRURGIA', [0, 1, 2, 3, 4, 5, 6, 8], 'Tratamento']\n",
      "['DOR TIPO QUEIMAÇÃO EM HEMITORAX ESQUERDO SEM RELÇAO COM EXERCICIIOS FISICOS', [1, 2, 3, 4, 5, 6, 15, 16, 17, 18, 19], 'Problema']\n",
      "['VENTRICULO DIREITO DIMENSÕES AUMENTADAS', [0, 1, 3, 4], 'Problema']\n",
      "['ATRIO DIREITO', [0, 3], 'Anatomia']\n",
      "['ATRIO DIREITO', [0, 3], 'Anatomia']\n",
      "['ATRIO DIREITO', [0, 3], 'Anatomia']\n",
      "['DOR INFRAMAMARIA MESMO EM REPOUSO , TIPO FISGADA , CONTINUA , SEM RELAÇÃO COM ATIVIDADE', [7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22], 'Problema']\n",
      "['VENTRICULO DIREITO DIMENSÕES AUMENTADAS', [0, 1, 3, 4], 'Problema']\n",
      "['ATRIO DIREITO', [0, 3], 'Anatomia']\n",
      "['angina aos mínimos esforços', [4, 12, 13, 14], 'Problema']\n",
      "['atrio esquerdo', [4, 7], 'Anatomia']\n",
      "['Dispnéia progressiva , aos moderados esforços', [0, 5, 6, 7, 8, 9], 'Problema']\n",
      "['MMII EDEMA', [0, 2], 'Problema']\n",
      "['MMII EDEMA', [0, 2], 'Problema']\n",
      "['MMII EDEMA', [0, 2], 'Problema']\n",
      "['PANTURRILHAS EDEMAS', [0, 4], 'Problema']\n",
      "['torax creptação', [6, 10], 'Problema']\n",
      "['tontura melhora no decorrer do dia', [10, 13, 14, 15, 16, 17], 'Problema']\n",
      "['mmii edema', [0, 2], 'Problema']\n",
      "['CPP MV + bilateral , REDUZIDO EM BASES', [0, 2, 3, 4, 5, 6, 7, 8], 'Problema']\n",
      "['ABD massas palpáveis', [0, 9, 10], 'Problema']\n",
      "['MMII edema 2 + / 4 + ', [0, 2, 3, 4, 5, 6, 7], 'Problema']\n",
      "['PALPITAÇÃO TRÊS CRISES', [4, 7, 8], 'Problema']\n",
      "['PALPITAÇÃO AOS MODERADOS ESFORÇOS 3 EPISODIOS POR DIA', [1, 2, 3, 4, 11, 12, 13, 14], 'Problema']\n",
      "['Ext edema', [0, 7], 'Problema']\n",
      "['câncer prostata', [16, 19], 'Problema']\n",
      "['dor em ponto específico paraesternal sem relação com esforço físico', [4, 5, 6, 7, 8, 10, 11, 12, 13, 14], 'Problema']\n",
      "['VALVA MITRAL VEGETAÇÃO', [11, 12, 17], 'Problema']\n",
      "['VALVA MITRAL REGURGITAÇÃO MODERADA A ACENTUADA', [11, 12, 20, 21, 22, 23], 'Problema']\n",
      "['CORONARIA LESOES', [31, 33], 'Problema']\n",
      "['CORONARIA IRREGULARIDADES PARIETAIS', [31, 36, 37], 'Problema']\n",
      "['MMII edema', [23, 25], 'Problema']\n",
      "['CANSAÇO QUANDO CAMINHA EM TORNO DE UMA QUADRA DOR EMPATURRILHA BILATERAL', [2, 3, 4, 5, 6, 7, 8, 9, 13, 14, 15], 'Problema']\n",
      "['VMi folhetos espessados', [0, 2, 3], 'Problema']\n",
      "['VMi refluxo discreto', [0, 8, 9], 'Problema']\n",
      "['VAo cúspides calcificadas', [18, 20, 21], 'Problema']\n",
      "['VAo dupla lesão', [18, 24, 25], 'Problema']\n",
      "['Vtri folhetos espessados', [4, 6, 7], 'Problema']\n",
      "['VE hipertrofiado', [0, 2], 'Problema']\n",
      "['VE alt de relaxamento', [0, 12, 13, 14], 'Problema']\n",
      "['AD aumentado', [0, 2], 'Problema']\n",
      "['dor torácica precordial em queimação sem relação com esforço físico', [7, 8, 9, 10, 11, 17, 18, 19, 20, 21], 'Problema']\n",
      "['dor torácica retroesternal e em MSE , em queimação , ', [3, 4, 9, 10, 11, 12, 13, 14, 15, 16], 'Problema']\n",
      "['CPP CREPITANTES EM BASE DIREITA', [0, 6, 7, 8, 9], 'Problema']\n",
      "['MMII EDEMA', [0, 3], 'Problema']\n",
      "['CAROTIDA ESTENOSE 20 - 30 % NA BIFURCAÇÃO CAROTÍDEA', [9, 11, 12, 13, 14, 15, 16, 17, 18], 'Problema']\n",
      "['MMII EDEMA', [0, 2], 'Problema']\n",
      "['protese biologica espessamentoi de seus folhetos', [34, 35, 37, 38, 39, 40], 'Problema']\n",
      "['protese biologica rgurgitação moderada a importante', [34, 35, 45, 46, 47, 48], 'Problema']\n",
      "['VAo cuspides calcificadas', [0, 2, 3], 'Problema']\n",
      "['VAo mobilidade comprometida', [0, 5, 6], 'Problema']\n",
      "['VTri refluxo disreto', [0, 5, 6], 'Problema']\n",
      "['PROTESE AO BIOL DLAO DISCRETA', [0, 1, 2, 9, 10], 'Problema']\n",
      "['edema de membros inferiores piora no final do dia', [1, 2, 3, 4, 7, 8, 9, 10, 11], 'Problema']\n",
      "['VE função sistólica global diminuída de grau moderado', [8, 13, 14, 15, 16, 17, 18, 19], 'Problema']\n",
      "['síncope mais com o calor', [2, 6, 7, 8, 9], 'Problema']\n",
      "['mmii edema', [0, 2], 'Problema']\n",
      "num: 66\n"
     ]
    }
   ],
   "source": [
    "print('descontinuas em train:')\n",
    "num=0\n",
    "for i in range(len(dic_sentencesTrain)):\n",
    "    tokens = dic_sentencesTrain[i][0]\n",
    "    ents = dic_sentencesTrain[i][1]\n",
    "    indiceEnts=[]\n",
    "    for ent in ents:\n",
    "        is_consecutive = checkConsecutive(ent[1])\n",
    "        if not is_consecutive:\n",
    "            print(ent)\n",
    "            num=num+1\n",
    "print('num:', num)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "descontinuas em dev:\n",
      "['MMII edema', [0, 3], 'Problema']\n",
      "['MMII empastamento', [0, 6], 'Problema']\n",
      "['DOR TORÁCICA EM APERTO EM REPOUSO', [10, 11, 12, 13, 23, 24], 'Problema']\n",
      "['MMII EDEMA', [0, 3], 'Problema']\n",
      "['Abdome massas palpáveis', [0, 12, 13], 'Problema']\n",
      "['MMII edema', [0, 3], 'Problema']\n",
      "['membros edemas', [0, 4], 'Problema']\n",
      "['membors edemas', [0, 4], 'Problema']\n",
      "['membors edemas', [0, 4], 'Problema']\n",
      "['membros edenas', [0, 5], 'Problema']\n",
      "['membors edemas', [0, 4], 'Problema']\n",
      "['CX TROCA VALVAR MITRAL BIOPROTESE N 31', [6, 8, 9, 10, 11, 12, 13], 'Tratamento']\n",
      "['MMII edema', [0, 3], 'Problema']\n",
      "['MMII EDEMA BILATERAL + + / 4 + ', [0, 2, 3, 4, 5, 6, 7, 8], 'Problema']\n",
      "['cefaléia em regiao parietal bilateral melhora com dipirona', [1, 2, 3, 4, 5, 7, 8, 9], 'Problema']\n",
      "['palpitações inicio súbito', [4, 6, 7], 'Problema']\n",
      "['MMII edema', [0, 2], 'Problema']\n",
      "num: 17\n"
     ]
    }
   ],
   "source": [
    "print('descontinuas em dev:')\n",
    "num=0\n",
    "for i in range(len(dic_sentencesDev)):\n",
    "    tokens = dic_sentencesDev[i][0]\n",
    "    ents = dic_sentencesDev[i][1]\n",
    "    indiceEnts=[]\n",
    "    for ent in ents:\n",
    "        is_consecutive = checkConsecutive(ent[1])\n",
    "        if not is_consecutive:\n",
    "            print(ent)\n",
    "            num=num+1\n",
    "print('num:', num)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "descontinuas em test:\n",
      "['MMII edema', [0, 2], 'Problema']\n",
      "['abdome massas', [102, 111], 'Problema']\n",
      "['MMII edema', [114, 116], 'Problema']\n",
      "['MMII EDEMA', [0, 2], 'Problema']\n",
      "['dor toracica quando epigastralgia melhoram com paracetamol . ', [10, 11, 12, 13, 15, 16, 17, 18], 'Problema']\n",
      "['VMi calcificada', [22, 28], 'Problema']\n",
      "['VMi refluxo discreto', [22, 30, 31], 'Problema']\n",
      "['VAo relfuxo discreto', [0, 7, 8], 'Problema']\n",
      "['VTri refluxo discreto', [0, 2, 3], 'Problema']\n",
      "['PULMONAR ALTERAÇÕES', [1, 3], 'Problema']\n",
      "['DISPNEIA AOS GRANDES ESFORCOS MELHORA COM REPOUSO', [1, 2, 3, 4, 7, 8, 9], 'Problema']\n",
      "['dor de cabeça melhora com analgésico comum', [8, 9, 10, 12, 13, 14, 15], 'Problema']\n",
      "['VMi folhetos espessados', [0, 2, 3], 'Problema']\n",
      "['VMi refluxo discreto', [0, 8, 9], 'Problema']\n",
      "['VAo cúspides calcificadas', [18, 20, 21], 'Problema']\n",
      "['VAo dupla lesão', [18, 24, 25], 'Problema']\n",
      "['Vtri folhetos espessados', [4, 6, 7], 'Problema']\n",
      "['Vtri insuficiente', [4, 12], 'Problema']\n",
      "['VE hipertrofiado', [0, 2], 'Problema']\n",
      "['VE alt de relaxamento', [0, 12, 13, 14], 'Problema']\n",
      "['AD aumentado', [0, 2], 'Problema']\n",
      "['AO CUSPIDES CALCIFICADAS', [0, 2, 3], 'Problema']\n",
      "['AO DUPLA LESAO', [0, 6, 7], 'Problema']\n",
      "['MMII EDEM', [0, 3], 'Problema']\n",
      "['CAROTIDA ESTENOSE 20 - 30 % NA BIFURCAÇÃO CAROTÍDEA', [9, 11, 12, 13, 14, 15, 16, 17, 18], 'Problema']\n",
      "['DOR NA FO AOS MOVIMENTOS E AO TOSSIR', [5, 6, 7, 9, 10, 11, 12, 13], 'Problema']\n",
      "['FO SINAIS FLOGÍSTICOS', [2, 7, 8], 'Problema']\n",
      "['CPP RA', [0, 7], 'Problema']\n",
      "['FO SECREÇÃO', [0, 8], 'Problema']\n",
      "['MMII EDEMA', [0, 3], 'Problema']\n",
      "['MMII EMPASTAMENTO', [0, 5], 'Problema']\n",
      "['MMII EDEMA', [0, 2], 'Problema']\n",
      "['síncope mais com o calor', [4, 8, 9, 10, 11], 'Problema']\n",
      "['sintomas durante atividades domesticas', [4, 12, 13, 14], 'Problema']\n",
      "['mmii edema', [0, 2], 'Problema']\n",
      "['MMII pedioso poplíteo femoral', [1, 2, 3, 5], 'Anatomia']\n",
      "['DOR PRECORDIAL LEVE A MODERADA DO TIPO QUEIMAÇÃO SEM IRRADIAÇÃO', [1, 2, 3, 4, 5, 6, 7, 8, 10, 11], 'Problema']\n",
      "['DOR TORÁCICA PRECORDIAL TIPO QUEIMAÇÃO IRRADIAÇÃO', [3, 4, 5, 6, 7, 17], 'Problema']\n",
      "['ABDOME MASSAS PALPAVEIS', [0, 10, 11], 'Problema']\n",
      "['MMII EDEMA', [0, 2], 'Problema']\n",
      "['MMII EMPASTAMENTO', [0, 4], 'Problema']\n",
      "['ABDOME DOLOROSO A PALPAÇÃO', [3, 5, 6, 7], 'Problema']\n",
      "['Decréscimo FC', [0, 3], 'Problema']\n",
      "num: 43\n"
     ]
    }
   ],
   "source": [
    "print('descontinuas em test:')\n",
    "num=0\n",
    "for i in range(len(dic_sentencesTest)):\n",
    "    tokens = dic_sentencesTest[i][0]\n",
    "    ents = dic_sentencesTest[i][1]\n",
    "    indiceEnts=[]\n",
    "    for ent in ents:\n",
    "        is_consecutive = checkConsecutive(ent[1])\n",
    "        if not is_consecutive:\n",
    "            print(ent)\n",
    "            num=num+1\n",
    "print('num:', num)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arq treinamento - 1o. nivel\n",
    "\n",
    "Regras:\n",
    "\n",
    "Entidade descontinua: se tem E como ligação, treina como se fosse uma unica entidade.. se não tem, treina separado, mas usando a label da descontinua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor i in range(len(dic_sentencesTest)):\\n    tokens = dic_sentencesTest[i][0]\\n    ents = dic_sentencesTest[i][1]\\n    indiceEnts=[]\\n    for ent in ents:\\n        is_consecutive = checkConsecutive(ent[1])\\n        if not is_consecutive:\\n            if ' e ' in ent[0].lower():\\n                print(i)\\n                print(ent)\\n                \\n\\n\\n\""
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for i in range(len(dic_sentencesTest)):\n",
    "    tokens = dic_sentencesTest[i][0]\n",
    "    ents = dic_sentencesTest[i][1]\n",
    "    indiceEnts=[]\n",
    "    for ent in ents:\n",
    "        is_consecutive = checkConsecutive(ent[1])\n",
    "        if not is_consecutive:\n",
    "            if ' e ' in ent[0].lower():\n",
    "                print(i)\n",
    "                print(ent)\n",
    "                \n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['MV', 0, 391],\n",
       "  ['normodist', 1, 394],\n",
       "  ['sem', 2, 404],\n",
       "  ['ruidos', 3, 408],\n",
       "  ['adv', 4, 415],\n",
       "  ['.', 5, 418]],\n",
       " [['ruidos adv', [3, 4], 'Problema']]]"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_sentencesTrain[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gravando em  data-ner\n",
      "num_entidade_train 13801\n",
      "num_entidade_dev 4009\n",
      "num_entidade_test: 5453\n",
      "num_entidade_total: 23263\n"
     ]
    }
   ],
   "source": [
    "# gerar arquivo treinamento\n",
    "path='data-ner'\n",
    "f_train = open(path+r'\\nested_train.conll', 'w', encoding='utf-8')\n",
    "print('Gravando em ', path)\n",
    "num_entidade_total=0\n",
    "num_entidade_train=0\n",
    "num_entidade_dev=0\n",
    "num_entidade_test=0\n",
    "\n",
    "\n",
    "for i in range(len(dic_sentencesTrain)):\n",
    "    tokens = dic_sentencesTrain[i][0]\n",
    "    ents = dic_sentencesTrain[i][1]\n",
    "    indiceEnts=[]\n",
    "    for token in tokens:\n",
    "        #print('token:', token)\n",
    "        indiceToken = token[1]\n",
    "        tag='O'\n",
    "        for ent in ents:\n",
    "            if indiceToken in ent[1]:\n",
    "                tag = ent[2]\n",
    "                #break\n",
    "        tokenGravar = token[0].replace(' ','')\n",
    "        tokenGravar = tokenGravar.strip()\n",
    "        f_train.write(tokenGravar+' '+tag+'\\n')\n",
    "        num_entidade_train=num_entidade_train+1\n",
    "    f_train.write('\\n')\n",
    "        \n",
    "f_train.close()\n",
    "\n",
    "f_dev = open(path+r'\\nested_dev.conll', 'w', encoding='utf-8')\n",
    "for i in range(len(dic_sentencesDev)):\n",
    "    tokens = dic_sentencesDev[i][0]\n",
    "    ents = dic_sentencesDev[i][1]\n",
    "    indiceEnts=[]\n",
    "    for token in tokens:\n",
    "        #print('token:', token)\n",
    "        indiceToken = token[1]\n",
    "        tag='O'\n",
    "        for ent in ents:\n",
    "            if indiceToken in ent[1]:\n",
    "                tag = ent[2]\n",
    "                #break\n",
    "        tokenGravar = token[0].replace(' ','')\n",
    "        tokenGravar = tokenGravar.strip()\n",
    "        f_dev.write(tokenGravar+' '+tag+'\\n')\n",
    "        num_entidade_dev=num_entidade_dev+1\n",
    "    f_dev.write('\\n')\n",
    "f_dev.close()\n",
    "\n",
    "\n",
    "f_test = open(path+r'\\nested_test.conll', 'w', encoding='utf-8')\n",
    "for i in range(len(dic_sentencesTest)):\n",
    "    tokens = dic_sentencesTest[i][0]\n",
    "    ents = dic_sentencesTest[i][1]\n",
    "    indiceEnts=[]\n",
    "    for token in tokens:\n",
    "        #print('token:', token)\n",
    "        indiceToken = token[1]\n",
    "        tag='O'\n",
    "        for ent in ents:\n",
    "            if indiceToken in ent[1]:\n",
    "                tag = ent[2]\n",
    "                #break\n",
    "        tokenGravar = token[0].replace(' ','')\n",
    "        tokenGravar = tokenGravar.strip()\n",
    "        f_test.write(tokenGravar+' '+tag+'\\n')\n",
    "        num_entidade_test=num_entidade_test+1\n",
    "    f_test.write('\\n')\n",
    "f_test.close()\n",
    "\n",
    "print('num_entidade_train', num_entidade_train)\n",
    "print('num_entidade_dev', num_entidade_dev)\n",
    "print('num_entidade_test:', num_entidade_test)\n",
    "num_entidade_total=num_entidade_train+num_entidade_dev+num_entidade_test\n",
    "print('num_entidade_total:', num_entidade_total)\n",
    "\n",
    "save_obj('dic_sentencesTrain',dic_sentencesTrain)\n",
    "save_obj('dic_sentencesDev',dic_sentencesDev)\n",
    "save_obj('dic_sentencesTest',dic_sentencesTest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerar arquivo para NER segundo nivel\n",
    "\n",
    "Formato NER e também formato special Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Discreto', 0, 308],\n",
       "  ['edema', 1, 317],\n",
       "  ['mmii', 2, 323],\n",
       "  ['pricn', 3, 328],\n",
       "  ['a', 4, 334],\n",
       "  ['esquerda', 5, 336],\n",
       "  ['.', 6, 344]],\n",
       " [['mmii', [2], 'Anatomia'],\n",
       "  ['Discreto edema mmii pricn a esquerda', [0, 1, 2, 3, 4, 5], 'Problema']]]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_sentencesTrain[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token: ['Dispneia', 0, 43]\n",
      "temTag: 0\n",
      "token: ['importante', 1, 52]\n",
      "temTag: 0\n",
      "token: ['aos', 2, 63]\n",
      "temTag: 0\n",
      "token: ['esforços', 3, 67]\n",
      "temTag: 0\n",
      "token: ['+', 4, 76]\n",
      "temTag: 0\n",
      "token: ['dor', 5, 78]\n",
      "temTag: 0\n",
      "token: ['tipo', 6, 82]\n",
      "temTag: 0\n",
      "token: ['peso', 7, 87]\n",
      "temTag: 0\n",
      "token: ['no', 8, 92]\n",
      "temTag: 0\n",
      "token: ['peito', 9, 95]\n",
      "temTag: 1\n",
      "token: ['no', 10, 101]\n",
      "temTag: 0\n",
      "token: ['esforço', 11, 104]\n",
      "temTag: 0\n",
      "token: ['.', 12, 111]\n",
      "temTag: 0\n",
      "['Dispneia O\\n', 'importante O\\n', 'aos O\\n', 'esforços O\\n', '+ O\\n', 'dor O\\n', 'tipo O\\n', 'peso O\\n', 'no O\\n', 'peito Anatomia\\n', 'no O\\n', 'esforço O\\n', '. O\\n']\n",
      "token: ['Obeso', 0, 113]\n",
      "temTag: 0\n",
      "token: [',', 1, 118]\n",
      "temTag: 0\n",
      "token: ['has', 2, 120]\n",
      "temTag: 0\n",
      "token: [',', 3, 123]\n",
      "temTag: 0\n",
      "token: ['icc', 4, 125]\n",
      "temTag: 0\n",
      "token: ['.', 5, 128]\n",
      "temTag: 0\n",
      "token: ['c', 0, 130]\n",
      "temTag: 0\n",
      "token: ['#', 1, 132]\n",
      "temTag: 0\n",
      "token: ['cintilografia', 2, 134]\n",
      "temTag: 0\n",
      "token: ['miocardica', 3, 148]\n",
      "temTag: 1\n",
      "token: ['para', 4, 159]\n",
      "temTag: 0\n",
      "token: ['avaliar', 5, 164]\n",
      "temTag: 0\n",
      "token: ['angina', 6, 172]\n",
      "temTag: 0\n",
      "token: ['.', 7, 178]\n",
      "temTag: 0\n",
      "['c O\\n', '# O\\n', 'cintilografia O\\n', 'miocardica Anatomia\\n', 'para O\\n', 'avaliar O\\n', 'angina O\\n', '. O\\n']\n",
      "token: ['Plastia', 0, 43]\n",
      "temTag: 0\n",
      "token: ['Mitral', 1, 51]\n",
      "temTag: 1\n",
      "token: ['(', 2, 58]\n",
      "temTag: 0\n",
      "token: ['Insuficiencia', 3, 60]\n",
      "temTag: 0\n",
      "token: [')', 4, 74]\n",
      "temTag: 0\n",
      "token: [',', 5, 75]\n",
      "temTag: 0\n",
      "token: ['CRM', 6, 77]\n",
      "temTag: 0\n",
      "token: ['Saf', 7, 81]\n",
      "temTag: 0\n",
      "token: ['-', 8, 84]\n",
      "temTag: 0\n",
      "token: ['2Mg', 9, 85]\n",
      "temTag: 0\n",
      "token: ['e', 10, 89]\n",
      "temTag: 0\n",
      "token: ['e', 11, 89]\n",
      "temTag: 0\n",
      "token: ['Saf', 12, 93]\n",
      "temTag: 0\n",
      "token: ['-', 13, 96]\n",
      "temTag: 0\n",
      "token: ['3MG', 14, 97]\n",
      "temTag: 0\n",
      "token: [')', 15, 101]\n",
      "temTag: 0\n",
      "token: ['.', 16, 102]\n",
      "temTag: 0\n",
      "['Plastia O\\n', 'Mitral Anatomia\\n', '( O\\n', 'Insuficiencia O\\n', ') O\\n', ', O\\n', 'CRM O\\n', 'Saf O\\n', '- O\\n', '2Mg O\\n', 'e O\\n', 'e O\\n', 'Saf O\\n', '- O\\n', '3MG O\\n', ') O\\n', '. O\\n']\n",
      "token: ['(', 0, 103]\n",
      "temTag: 0\n",
      "token: ['09', 1, 104]\n",
      "temTag: 0\n",
      "token: ['/', 2, 106]\n",
      "temTag: 0\n",
      "token: ['03', 3, 107]\n",
      "temTag: 0\n",
      "token: ['/', 4, 109]\n",
      "temTag: 0\n",
      "token: ['16', 5, 110]\n",
      "temTag: 0\n",
      "token: [')', 6, 112]\n",
      "temTag: 0\n",
      "token: ['.', 7, 113]\n",
      "temTag: 0\n",
      "token: ['Uso', 0, 115]\n",
      "temTag: 0\n",
      "token: [':', 1, 118]\n",
      "temTag: 0\n",
      "token: ['AAS', 2, 120]\n",
      "temTag: 0\n",
      "token: ['100', 3, 124]\n",
      "temTag: 0\n",
      "token: ['-', 4, 129]\n",
      "temTag: 0\n",
      "token: ['1xd', 5, 130]\n",
      "temTag: 0\n",
      "token: [';', 6, 133]\n",
      "temTag: 0\n",
      "token: ['Metoprolol', 7, 135]\n",
      "temTag: 0\n",
      "token: ['25', 8, 147]\n",
      "temTag: 0\n",
      "token: ['-', 9, 150]\n",
      "temTag: 0\n",
      "token: ['1xd', 10, 151]\n",
      "temTag: 0\n",
      "token: [';', 11, 154]\n",
      "temTag: 0\n",
      "token: ['FSM', 12, 156]\n",
      "temTag: 0\n",
      "token: ['-', 13, 160]\n",
      "temTag: 0\n",
      "token: ['1xd', 14, 161]\n",
      "temTag: 0\n",
      "token: [';', 15, 165]\n",
      "temTag: 0\n",
      "token: ['Levotiroxina', 16, 167]\n",
      "temTag: 0\n",
      "token: ['175', 17, 180]\n",
      "temTag: 0\n",
      "token: ['-', 18, 185]\n",
      "temTag: 0\n",
      "token: ['1xd', 19, 186]\n",
      "temTag: 0\n",
      "token: [';', 20, 189]\n",
      "temTag: 0\n",
      "token: ['Sinva', 21, 191]\n",
      "temTag: 0\n",
      "token: ['40', 22, 197]\n",
      "temTag: 0\n",
      "token: ['-', 23, 200]\n",
      "temTag: 0\n",
      "token: ['1xd', 24, 201]\n",
      "temTag: 0\n",
      "token: [';', 25, 204]\n",
      "temTag: 0\n",
      "token: ['Fluoxetina', 26, 206]\n",
      "temTag: 0\n",
      "token: ['20', 27, 217]\n",
      "temTag: 0\n",
      "token: ['-', 28, 219]\n",
      "temTag: 0\n",
      "token: ['1xd', 29, 220]\n",
      "temTag: 0\n",
      "token: ['.', 30, 223]\n",
      "temTag: 0\n",
      "token: ['Refere', 0, 226]\n",
      "temTag: 0\n",
      "token: ['melhora', 1, 233]\n",
      "temTag: 0\n",
      "token: ['da', 2, 241]\n",
      "temTag: 0\n",
      "token: ['dispneia', 3, 244]\n",
      "temTag: 0\n",
      "token: ['depois', 4, 253]\n",
      "temTag: 0\n",
      "token: ['da', 5, 260]\n",
      "temTag: 0\n",
      "token: ['cx', 6, 263]\n",
      "temTag: 0\n",
      "token: ['porem', 7, 266]\n",
      "temTag: 0\n",
      "token: ['mantem', 8, 272]\n",
      "temTag: 0\n",
      "token: ['aos', 9, 279]\n",
      "temTag: 0\n",
      "token: ['mdoeardos', 10, 283]\n",
      "temTag: 0\n",
      "token: ['-', 11, 292]\n",
      "temTag: 0\n",
      "token: ['leves', 12, 293]\n",
      "temTag: 0\n",
      "token: ['esforço', 13, 299]\n",
      "temTag: 0\n",
      "token: ['.', 14, 306]\n",
      "temTag: 0\n",
      "token: ['Discreto', 0, 308]\n",
      "temTag: 0\n",
      "token: ['edema', 1, 317]\n",
      "temTag: 0\n",
      "token: ['mmii', 2, 323]\n",
      "temTag: 1\n",
      "token: ['pricn', 3, 328]\n",
      "temTag: 0\n",
      "token: ['a', 4, 334]\n",
      "temTag: 0\n",
      "token: ['esquerda', 5, 336]\n",
      "temTag: 0\n",
      "token: ['.', 6, 344]\n",
      "temTag: 0\n",
      "['Discreto O\\n', 'edema O\\n', 'mmii Anatomia\\n', 'pricn O\\n', 'a O\\n', 'esquerda O\\n', '. O\\n']\n",
      "token: ['Nega', 0, 345]\n",
      "temTag: 0\n",
      "token: ['palpitação', 1, 350]\n",
      "temTag: 0\n",
      "token: ['.', 2, 360]\n",
      "temTag: 0\n",
      "token: ['Dor', 0, 361]\n",
      "temTag: 0\n",
      "token: ['a', 1, 365]\n",
      "temTag: 0\n",
      "token: ['movimentação', 2, 367]\n",
      "temTag: 0\n",
      "token: ['do', 3, 380]\n",
      "temTag: 0\n",
      "token: ['torax', 4, 383]\n",
      "temTag: 1\n",
      "token: ['.', 5, 388]\n",
      "temTag: 0\n",
      "['Dor O\\n', 'a O\\n', 'movimentação O\\n', 'do O\\n', 'torax Anatomia\\n', '. O\\n']\n",
      "token: ['MV', 0, 391]\n",
      "temTag: 0\n",
      "token: ['normodist', 1, 394]\n",
      "temTag: 0\n",
      "token: ['sem', 2, 404]\n",
      "temTag: 0\n",
      "token: ['ruidos', 3, 408]\n",
      "temTag: 0\n",
      "token: ['adv', 4, 415]\n",
      "temTag: 0\n",
      "token: ['.', 5, 418]\n",
      "temTag: 0\n",
      "token: ['RCR', 0, 421]\n",
      "temTag: 0\n",
      "token: [',', 1, 425]\n",
      "temTag: 0\n",
      "token: ['2T', 2, 426]\n",
      "temTag: 0\n",
      "token: [',', 3, 429]\n",
      "temTag: 0\n",
      "token: ['BNf', 4, 431]\n",
      "temTag: 0\n",
      "token: ['sem', 5, 435]\n",
      "temTag: 0\n",
      "token: ['sopros', 6, 439]\n",
      "temTag: 0\n",
      "token: ['.', 7, 445]\n",
      "temTag: 0\n",
      "token: ['PA', 0, 447]\n",
      "temTag: 0\n",
      "token: ['120x70mmHg', 1, 451]\n",
      "temTag: 0\n",
      "token: ['P', 2, 463]\n",
      "temTag: 0\n",
      "token: ['81', 3, 465]\n",
      "temTag: 0\n",
      "token: ['spm', 4, 468]\n",
      "temTag: 0\n",
      "token: [',', 5, 472]\n",
      "temTag: 0\n",
      "token: ['Fr', 6, 474]\n",
      "temTag: 0\n",
      "token: ['12', 7, 477]\n",
      "temTag: 0\n",
      "token: ['rpm', 8, 480]\n",
      "temTag: 0\n",
      "token: ['.', 9, 483]\n",
      "temTag: 0\n",
      "token: ['CD', 0, 487]\n",
      "temTag: 0\n",
      "token: ['>', 1, 489]\n",
      "temTag: 0\n",
      "token: [':', 2, 490]\n",
      "temTag: 0\n",
      "token: ['mantida', 3, 492]\n",
      "temTag: 0\n",
      "token: ['.', 4, 499]\n",
      "temTag: 0\n",
      "token: ['Ecocardio', 0, 501]\n",
      "temTag: 0\n",
      "token: ['e', 1, 511]\n",
      "temTag: 0\n",
      "token: ['lab', 2, 513]\n",
      "temTag: 0\n",
      "token: ['.', 3, 516]\n",
      "temTag: 0\n",
      "token: ['S', 0, 43]\n",
      "temTag: 0\n",
      "token: ['#', 1, 45]\n",
      "temTag: 0\n",
      "token: ['angioplastia', 2, 47]\n",
      "temTag: 0\n",
      "token: [',', 3, 59]\n",
      "temTag: 0\n",
      "token: ['implante', 4, 61]\n",
      "temTag: 0\n",
      "token: ['de', 5, 70]\n",
      "temTag: 0\n",
      "token: ['marcapasso', 6, 73]\n",
      "temTag: 0\n",
      "token: ['(', 7, 84]\n",
      "temTag: 0\n",
      "token: ['pacte', 8, 85]\n",
      "temTag: 0\n",
      "token: ['assintomatico', 9, 91]\n",
      "temTag: 0\n",
      "token: [',', 10, 104]\n",
      "temTag: 0\n",
      "token: ['ecg', 11, 106]\n",
      "temTag: 0\n",
      "token: ['de', 12, 110]\n",
      "temTag: 0\n",
      "token: ['repouso', 13, 113]\n",
      "temTag: 0\n",
      "token: ['com', 14, 121]\n",
      "temTag: 0\n",
      "token: ['bav', 15, 125]\n",
      "temTag: 0\n",
      "token: ['de', 16, 129]\n",
      "temTag: 0\n",
      "token: ['2', 17, 132]\n",
      "temTag: 0\n",
      "token: ['grau', 18, 134]\n",
      "temTag: 0\n",
      "token: ['mobitz', 19, 139]\n",
      "temTag: 0\n",
      "token: ['2', 20, 146]\n",
      "temTag: 0\n",
      "token: [')', 21, 147]\n",
      "temTag: 0\n",
      "token: [',', 22, 148]\n",
      "temTag: 0\n",
      "token: ['cat', 23, 150]\n",
      "temTag: 0\n",
      "token: ['em', 24, 154]\n",
      "temTag: 0\n",
      "token: ['agosto', 25, 157]\n",
      "temTag: 0\n",
      "token: ['desse', 26, 164]\n",
      "temTag: 0\n",
      "token: ['ano', 27, 170]\n",
      "temTag: 0\n",
      "token: ['.', 28, 173]\n",
      "temTag: 0\n",
      "token: ['Permaneceu', 0, 175]\n",
      "temTag: 0\n",
      "token: ['em', 1, 186]\n",
      "temTag: 0\n",
      "token: ['UTI', 2, 189]\n",
      "temTag: 0\n",
      "token: ['por', 3, 193]\n",
      "temTag: 0\n",
      "token: ['10', 4, 197]\n",
      "temTag: 0\n",
      "token: ['dias', 5, 200]\n",
      "temTag: 0\n",
      "token: ['.', 6, 204]\n",
      "temTag: 0\n",
      "token: ['DM', 0, 206]\n",
      "temTag: 0\n",
      "token: [',', 1, 208]\n",
      "temTag: 0\n",
      "token: ['HAS', 2, 210]\n",
      "temTag: 0\n",
      "token: [',', 3, 213]\n",
      "temTag: 0\n",
      "token: ['DAC', 4, 215]\n",
      "temTag: 0\n",
      "token: ['(', 5, 219]\n",
      "temTag: 0\n",
      "token: ['rvm', 6, 220]\n",
      "temTag: 0\n",
      "token: ['há', 7, 224]\n",
      "temTag: 0\n",
      "token: ['4', 8, 227]\n",
      "temTag: 0\n",
      "token: ['anos', 9, 229]\n",
      "temTag: 0\n",
      "token: [')', 10, 233]\n",
      "temTag: 0\n",
      "token: [',', 11, 234]\n",
      "temTag: 0\n",
      "token: ['dislipidemia', 12, 236]\n",
      "temTag: 0\n",
      "token: ['.', 13, 248]\n",
      "temTag: 0\n",
      "token: ['Em', 0, 250]\n",
      "temTag: 0\n",
      "token: ['uso', 1, 253]\n",
      "temTag: 0\n",
      "token: ['de', 2, 257]\n",
      "temTag: 0\n",
      "token: ['insulina', 3, 260]\n",
      "temTag: 0\n",
      "token: ['nph', 4, 269]\n",
      "temTag: 0\n",
      "token: [',', 5, 272]\n",
      "temTag: 0\n",
      "token: ['metformina', 6, 274]\n",
      "temTag: 0\n",
      "token: [',', 7, 284]\n",
      "temTag: 0\n",
      "token: ['sinvastatina', 8, 286]\n",
      "temTag: 0\n",
      "token: ['40mg', 9, 299]\n",
      "temTag: 0\n",
      "token: ['2', 10, 304]\n",
      "temTag: 0\n",
      "token: ['cp', 11, 306]\n",
      "temTag: 0\n",
      "token: [',', 12, 308]\n",
      "temTag: 0\n",
      "token: ['carvedilol', 13, 310]\n",
      "temTag: 0\n",
      "token: ['6', 14, 321]\n",
      "temTag: 0\n",
      "token: [',', 15, 322]\n",
      "temTag: 0\n",
      "token: ['25mg', 16, 323]\n",
      "temTag: 0\n",
      "token: [',', 17, 327]\n",
      "temTag: 0\n",
      "token: ['losartana', 18, 329]\n",
      "temTag: 0\n",
      "token: ['50mg', 19, 339]\n",
      "temTag: 0\n",
      "token: ['2', 20, 344]\n",
      "temTag: 0\n",
      "token: ['x', 21, 346]\n",
      "temTag: 0\n",
      "token: ['dia', 22, 348]\n",
      "temTag: 0\n",
      "token: [',', 23, 351]\n",
      "temTag: 0\n",
      "token: ['AAS', 24, 353]\n",
      "temTag: 0\n",
      "token: ['100mg', 25, 357]\n",
      "temTag: 0\n",
      "token: [',', 26, 362]\n",
      "temTag: 0\n",
      "token: ['clopidogrel', 27, 364]\n",
      "temTag: 0\n",
      "token: ['75mg', 28, 376]\n",
      "temTag: 0\n",
      "token: ['.', 29, 380]\n",
      "temTag: 0\n",
      "token: ['Tabagista', 0, 382]\n",
      "temTag: 0\n",
      "token: ['há', 1, 392]\n",
      "temTag: 0\n",
      "token: ['mais', 2, 395]\n",
      "temTag: 0\n",
      "token: ['de', 3, 400]\n",
      "temTag: 0\n",
      "token: ['40', 4, 403]\n",
      "temTag: 0\n",
      "token: ['anos', 5, 406]\n",
      "temTag: 0\n",
      "token: ['.', 6, 410]\n",
      "temTag: 0\n",
      "token: ['Hoje', 0, 412]\n",
      "temTag: 0\n",
      "token: ['fuma', 1, 417]\n",
      "temTag: 0\n",
      "token: ['8', 2, 422]\n",
      "temTag: 0\n",
      "token: ['cigarros', 3, 424]\n",
      "temTag: 0\n",
      "token: ['por', 4, 433]\n",
      "temTag: 0\n",
      "token: ['dia', 5, 437]\n",
      "temTag: 0\n",
      "token: ['.', 6, 440]\n",
      "temTag: 0\n",
      "token: ['Antes', 0, 442]\n",
      "temTag: 0\n",
      "token: ['2', 1, 448]\n",
      "temTag: 0\n",
      "token: ['carteiras', 2, 450]\n",
      "temTag: 0\n",
      "token: ['/', 3, 459]\n",
      "temTag: 0\n",
      "token: ['dia', 4, 460]\n",
      "temTag: 0\n",
      "token: ['.', 5, 463]\n",
      "temTag: 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(dic_sentencesTrain)):\n",
    "        tokens = dic_sentencesTrain[i][0]\n",
    "        ents = dic_sentencesTrain[i][1]\n",
    "        indiceEnts=[]\n",
    "        linha = []\n",
    "        temTagFrase=0\n",
    "        for token in tokens:\n",
    "            temTag=0\n",
    "            print('token:', token)\n",
    "            indiceToken = token[1]\n",
    "            tag='O'\n",
    "            tamanhoEntidade=0\n",
    "            for ent in ents:\n",
    "                #print('ent:', ent)\n",
    "                if indiceToken in ent[1] and tamanhoEntidade==0:\n",
    "                    tamanhoEntidade = len(ent[1])\n",
    "                    tag = ent[2]\n",
    "                    #print('caiu primeira vez, continuando, tag:', tag)\n",
    "                    continue\n",
    "                if indiceToken in ent[1] and tamanhoEntidade>0:\n",
    "                    if len(ent[1]) < tamanhoEntidade: \n",
    "                        tag = ent[2]\n",
    "                    temTag=1\n",
    "                    temTagFrase=1\n",
    "                    #print('tag:', tag)\n",
    "                    #continue\n",
    "            if temTag==0:\n",
    "                tag='O'\n",
    "            print('temTag:', temTag)\n",
    "                \n",
    "            tokenGravar = token[0].replace(' ','')\n",
    "            tokenGravar = tokenGravar.strip()\n",
    "            linha.append(tokenGravar+' '+tag+'\\n')\n",
    "            \n",
    "        if temTagFrase==1:\n",
    "            print(linha)\n",
    "            linha=[]\n",
    "            temTag=0\n",
    "        if i>20:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Antes O\\n', '2 O\\n', 'carteiras O\\n', '/ O\\n', 'dia O\\n', '. O\\n']"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gravarArquivosNerSegundoNivel(dic_sentences, tipo):\n",
    "    # gerar arquivo treinamento\n",
    "    f_entidade = open(r'segundo_nivel/nested_'+tipo+'.conll', 'w', encoding='utf-8')\n",
    "\n",
    "    num_entidade_total=0\n",
    "    num_entidade=0\n",
    "\n",
    "    print('\\nGravando arquivo de {}'.format(tipo))\n",
    "\n",
    "    for i in range(len(dic_sentences)):\n",
    "        tokens = dic_sentences[i][0]\n",
    "        ents = dic_sentences[i][1]\n",
    "        indiceEnts=[]\n",
    "        linha = []\n",
    "        temTagFrase=0\n",
    "        for token in tokens:\n",
    "            temTag=0\n",
    "            #print('token:', token)\n",
    "            indiceToken = token[1]\n",
    "            tag='O'\n",
    "            tamanhoEntidade=0\n",
    "            for ent in ents:\n",
    "                #print('ent:', ent)\n",
    "                if indiceToken in ent[1] and tamanhoEntidade==0:\n",
    "                    tamanhoEntidade = len(ent[1])\n",
    "                    tag = ent[2]\n",
    "                    #print('caiu primeira vez, continuando, tag:', tag)\n",
    "                    continue\n",
    "                if indiceToken in ent[1] and tamanhoEntidade>0:\n",
    "                    if len(ent[1]) < tamanhoEntidade: \n",
    "                        tag = ent[2]\n",
    "                    temTag=1\n",
    "                    temTagFrase=1\n",
    "                    #print('tag:', tag)\n",
    "                    continue\n",
    "            if temTag==0:\n",
    "                tag='O'\n",
    "\n",
    "            tokenGravar = token[0].replace(' ','')\n",
    "            tokenGravar = tokenGravar.strip()\n",
    "            linha.append(tokenGravar+' '+tag+'\\n')\n",
    "            \n",
    "        if temTagFrase==1:\n",
    "            for l in linha:\n",
    "                f_entidade.write(l)\n",
    "            f_entidade.write('\\n')\n",
    "            linha=[]\n",
    "            temTag=0\n",
    "            \n",
    "    f_entidade.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gravando arquivo de train\n",
      "\n",
      "Gravando arquivo de dev\n",
      "\n",
      "Gravando arquivo de test\n"
     ]
    }
   ],
   "source": [
    "gravarArquivosNerSegundoNivel(dic_sentencesTrain, 'train')\n",
    "gravarArquivosNerSegundoNivel(dic_sentencesDev, 'dev')\n",
    "gravarArquivosNerSegundoNivel(dic_sentencesTest, 'test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gravar arquivo para multilabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gravarArquivosMultilabel(dic_sentences, tipo):\n",
    "    # gerar arquivo treinamento\n",
    "    f_entidade = open(r'multilabel/nested_'+tipo+'.conll', 'w', encoding='utf-8')\n",
    "\n",
    "    num_entidade_total=0\n",
    "\n",
    "    print('\\nGravando arquivo multilabel de {}'.format(tipo))\n",
    "\n",
    "    entidades = ['Problema','Teste','Tratamento','Anatomia']\n",
    "    for i in range(len(dic_sentences)):\n",
    "        tokens = dic_sentences[i][0]\n",
    "        ents = dic_sentences[i][1]\n",
    "        #print('tokens:', tokens)\n",
    "        #print('ents:', ents)\n",
    "        for token in tokens:\n",
    "            listaTokensLabels = list()\n",
    "            #print('token:', token)\n",
    "            indiceToken = token[1]\n",
    "            #print('indiceToken:', indiceToken)\n",
    "            labels = list()\n",
    "            for ent in ents:\n",
    "                #print('ent:', ent)\n",
    "                #print('indiceToken:', indiceToken)\n",
    "                if indiceToken in ent[1]:\n",
    "                    for entidade in entidades:\n",
    "                        tag = ent[2]\n",
    "                        if tag==entidade:\n",
    "                            labels.append(tag)\n",
    "                            num_entidade_total=num_entidade_total+1\n",
    "                        else:\n",
    "                            labels.append('O')\n",
    "            if len(labels)==0:\n",
    "                labels = ['O','O','O','O']\n",
    "            tokenGravar = token[0].replace(' ','')\n",
    "            tokenGravar = tokenGravar.strip()\n",
    "            f_entidade.write(tokenGravar+'\\t'+'\\t'.join(labels)+'\\n')\n",
    "            #print(tokenGravar+'\\t'+'\\t'.join(labels))\n",
    "\n",
    "        \n",
    "        f_entidade.write('\\n')\n",
    "\n",
    "    f_entidade.close()\n",
    "\n",
    "    print('num_entidade_total:', num_entidade_total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gravando arquivo multilabel de train\n",
      "num_entidade_total: 4830\n",
      "\n",
      "Gravando arquivo multilabel de dev\n",
      "num_entidade_total: 1226\n",
      "\n",
      "Gravando arquivo multilabel de test\n",
      "num_entidade_total: 1803\n"
     ]
    }
   ],
   "source": [
    "gravarArquivosMultilabel(dic_sentencesTrain, 'train')\n",
    "gravarArquivosMultilabel(dic_sentencesDev, 'dev')\n",
    "gravarArquivosMultilabel(dic_sentencesTest, 'test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parte 2- Gerar arquivo treinamento para SpanClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def areConsecutive(arr):\n",
    "    # Sort the array\n",
    "    arr.sort()\n",
    "    n = len(arr)\n",
    "    # checking the adjacent elements\n",
    "    for i in range (1,n):\n",
    "        if(arr[i]!=arr[i-1]+1):\n",
    "            return False;\n",
    "             \n",
    "    return True;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_sentencesTest = load_obj('dic_sentencesTest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCombinacaoEntidades(dic_predictions, filtro_postagger, dicPosTagger, taxaDownsampling):\n",
    "    num=0\n",
    "    erro_corpus=0\n",
    "    num_frases_sem_entidade=0\n",
    "    lista_erro_corpus=list()\n",
    "    combinacaoEntidadesAll = list()\n",
    "    combinacaoEntidades = list()\n",
    "    pulando_termos_postagger = list()\n",
    "    if filtro_postagger:\n",
    "        print('Com filtro-postagger')\n",
    "    else:\n",
    "        print('Sem filtro-postagger')\n",
    "    if taxaDownsampling>0:\n",
    "        print('Com taxa de Downsampling de ', taxaDownsampling)\n",
    "    else:\n",
    "        print('Sem taxa de Downsampling')\n",
    "    for key, value in dic_predictions.items():\n",
    "        num=num+1\n",
    "        combinacaoEntidades = list()\n",
    "        tokens=value[0].copy()\n",
    "        so_tokens = [t[0] for t in tokens]\n",
    "        entidades=value[1].copy()\n",
    "        num_positivas=0\n",
    "        for entidade in entidades:\n",
    "            erros_entidade = list()\n",
    "            texto_entidade=entidade[0].strip()\n",
    "            indices = entidade[1]\n",
    "            tipo_entidade = entidade[2]\n",
    "            frase = so_tokens.copy()\n",
    "            inicio=indices[0]\n",
    "            fim=indices[-1]\n",
    "            frase.insert(inicio, '<e1>')\n",
    "            frase.insert(fim+2, '</e1>')\n",
    "            if texto_entidade=='-' or texto_entidade=='=' or texto_entidade=='+' or texto_entidade==':' or texto_entidade==',' or texto_entidade==\"'\" or texto_entidade=='\"' or texto_entidade=='.' or texto_entidade==';' or texto_entidade=='/' or texto_entidade=='(' or texto_entidade==')' or texto_entidade=='[' or texto_entidade==']':\n",
    "                pass\n",
    "            texto_entidade_comparar=texto_entidade.replace('/','').replace(')','').replace('(','').replace(']','').replace('[','').replace(',','').replace('.','').replace(';','').replace('-','').replace('+','').replace(\"'\",'')\n",
    "            texto_entidade_comparar = replaceWhiteSpaces(texto_entidade_comparar)\n",
    "            texto_frase_comparar = ' '.join(frase[inicio+1:fim+2]).strip().replace('/','').replace(')','').replace('(','').replace(']','').replace('[','').replace(',','').replace('.','').replace(';','').replace('-','').replace('+','').replace(\"'\",'')\n",
    "            texto_frase_comparar = replaceWhiteSpaces(texto_frase_comparar)\n",
    "            texto_entidade_comparar = texto_entidade_comparar.lower()\n",
    "            texto_frase_comparar = texto_frase_comparar.lower()\n",
    "            if (texto_entidade_comparar == texto_frase_comparar):\n",
    "                num_positivas=num_positivas+1\n",
    "                combinacaoEntidades.append([' '.join(frase).strip(), tipo_entidade]) # apendando entidades reais\n",
    "            else:\n",
    "                print('erro, key:', key)\n",
    "                erro_corpus=erro_corpus+1\n",
    "                erros_entidade.append(indices)\n",
    "                lista_erro_corpus.append([' '.join(frase).strip(), tipo_entidade, ' '.join(so_tokens), entidade])\n",
    "\n",
    "        for entidade in entidades:\n",
    "                indices = entidade[1]\n",
    "                #print('indices:', indices)\n",
    "                if indices in erros_entidade:\n",
    "                    continue\n",
    "                inicio=indices[0]\n",
    "                fim=indices[-1]\n",
    "                # agora, fazer a combinacao entre eles.. todas a seguir serão do tipo 'O'           \n",
    "                for indice in indices:\n",
    "                    for i in range(indice, fim+1):\n",
    "                        # ver se nao tem antes\n",
    "                        frase = so_tokens.copy()\n",
    "                        #termo = frase[indice:i+2]\n",
    "                        termo = frase[indice:i+1]\n",
    "                        #print('--termo--:', termo)\n",
    "                        frase.insert(indice, '<e1>')\n",
    "                        frase.insert(i+2, '</e1>')\n",
    "                        frase_string=' '.join(frase).strip()\n",
    "                        #print('frase_string:', frase_string)\n",
    "                        devePular = 0\n",
    "                        if '. </e1>' in frase_string or ', </e1>' in frase_string  or '; </e1>' in frase_string or '- </e1>' in frase_string  or ': </e1>' in frase_string  or '= </e1>' in frase_string  or '/ </e1>' in frase_string  or '( </e1>' in frase_string  or ') </e1>' in frase_string  or '[ </e1>' in frase_string  or '] </e1>' in frase_string  or ': </e1>' in frase_string or 'and </e1>' in frase_string or 'or </e1>' in frase_string:\n",
    "                            devePular=1\n",
    "                        if '<e1> .' in frase_string or '<e1> ,' in frase_string  or '<e1> ;' in frase_string or '<e1> -' in frase_string  or '<e1> :' in frase_string  or '<e1> =' in frase_string  or '<e1> /' in frase_string  or '<e1> (' in frase_string  or '<e1> )' in frase_string  or '<e1> [' in frase_string  or '<e1> ]' in frase_string  or '<e1> :' in frase_string  or '<e1> and' in frase_string  or '<e1> or' in frase_string:\n",
    "                            devePular=1\n",
    "                        if re.search(\"<e1> [0-9]* </e1>\", frase_string):\n",
    "                            devePular=1\n",
    "                        if filtro_postagger==True:\n",
    "                            pos_tagger_termo = tipoPostaggerTokens(termo, dicPosTagger)\n",
    "                            if pos_tagger_termo not in lista_postaggers_entidades:\n",
    "                                pulando_termos_postagger.append([termo, pos_tagger_termo])\n",
    "                                devePular=1\n",
    "                \n",
    "                        tem_frase = 0\n",
    "                        for frase in combinacaoEntidades:\n",
    "                            if frase[0] == frase_string:\n",
    "                                tem_frase=''\n",
    "                                break\n",
    "                        if tem_frase==0 and devePular==0:\n",
    "                            combinacaoEntidades.append([frase_string, 'O'])\n",
    "        # shuffle no combinacaoEntidades\n",
    "        # taxaDownsampling, ex 2 para o dobro, 1 para mesma quantidade\n",
    "        if len(combinacaoEntidades)>0:\n",
    "            if taxaDownsampling>0:\n",
    "                combinacaoEntidades = combinacaoEntidades[:(num_positivas*taxaDownsampling)+num_positivas]\n",
    "            random.shuffle(combinacaoEntidades)\n",
    "            combinacaoEntidadesAll.append([' '.join(so_tokens).strip(), combinacaoEntidades])\n",
    "        else:\n",
    "            num_frases_sem_entidade = num_frases_sem_entidade+1\n",
    "            combinacaoEntidadesAll.append([])\n",
    "        combinacaoEntidades = list()\n",
    "        if (num % 1000) ==0:\n",
    "            print('key:', key)\n",
    "\n",
    "    print('erro_corpus:', erro_corpus)\n",
    "    print('num_frases_sem_entidade:', num_frases_sem_entidade)\n",
    "    print('len(combinacaoEntidadesAll:)',len(combinacaoEntidadesAll))\n",
    "    print('len(pulando_termos_postagger):', len(pulando_termos_postagger))\n",
    "    \n",
    "    return combinacaoEntidadesAll, pulando_termos_postagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels sempre tem que começar com zero, senao da erro no treinamento\n",
    "# RuntimeError: CUDA error: device-side assert triggered\n",
    "def getCombinacaoEntidadesSoPositivos(dic_predictions):\n",
    "    num=0\n",
    "    erro_corpus=0\n",
    "    num_frases_sem_entidade=0\n",
    "    lista_erro_corpus=list()\n",
    "    combinacaoEntidadesAll = list()\n",
    "    combinacaoEntidades = list()\n",
    "    print('Só positivos')\n",
    "    for key, value in dic_predictions.items():\n",
    "        num=num+1\n",
    "        combinacaoEntidades = list()\n",
    "        tokens=value[0].copy()\n",
    "        so_tokens = [t[0] for t in tokens]\n",
    "        entidades=value[1].copy()\n",
    "        num_positivas=0\n",
    "        for entidade in entidades:\n",
    "            erros_entidade = list()\n",
    "            texto_entidade=entidade[0].strip()\n",
    "            indices = entidade[1]\n",
    "            tipo_entidade = entidade[2]\n",
    "            #print('tipo_entidade:', tipo_entidade)\n",
    "            frase = so_tokens.copy()\n",
    "            inicio=indices[0]\n",
    "            fim=indices[-1]\n",
    "            frase.insert(inicio, '<e1>')\n",
    "            frase.insert(fim+2, '</e1>')\n",
    "            if texto_entidade=='-' or texto_entidade=='=' or texto_entidade=='+' or texto_entidade==':' or texto_entidade==',' or texto_entidade==\"'\" or texto_entidade=='\"' or texto_entidade=='.' or texto_entidade==';' or texto_entidade=='/' or texto_entidade=='(' or texto_entidade==')' or texto_entidade=='[' or texto_entidade==']':\n",
    "                pass\n",
    "            texto_entidade_comparar=texto_entidade.replace('/','').replace(')','').replace('(','').replace(']','').replace('[','').replace(',','').replace('.','').replace(';','').replace('-','').replace('+','').replace(\"'\",'')\n",
    "            texto_entidade_comparar = replaceWhiteSpaces(texto_entidade_comparar)\n",
    "            texto_frase_comparar = ' '.join(frase[inicio+1:fim+2]).strip().replace('/','').replace(')','').replace('(','').replace(']','').replace('[','').replace(',','').replace('.','').replace(';','').replace('-','').replace('+','').replace(\"'\",'')\n",
    "            texto_frase_comparar = replaceWhiteSpaces(texto_frase_comparar)\n",
    "            texto_entidade_comparar = texto_entidade_comparar.lower()\n",
    "            texto_frase_comparar = texto_frase_comparar.lower()\n",
    "            if (texto_entidade_comparar == texto_frase_comparar):\n",
    "                num_positivas=num_positivas+1\n",
    "                combinacaoEntidades.append([' '.join(frase).strip(), tipo_entidade]) # apendando entidades reais\n",
    "            else:\n",
    "                print('erro, key:', key)\n",
    "                erro_corpus=erro_corpus+1\n",
    "                erros_entidade.append(indices)\n",
    "                lista_erro_corpus.append([' '.join(frase).strip(), tipo_entidade, ' '.join(so_tokens), entidade])\n",
    "\n",
    "        # shuffle no combinacaoEntidades\n",
    "        if len(combinacaoEntidades)>0:\n",
    "            random.shuffle(combinacaoEntidades)\n",
    "            combinacaoEntidadesAll.append([' '.join(so_tokens).strip(), combinacaoEntidades])\n",
    "        else:\n",
    "            num_frases_sem_entidade = num_frases_sem_entidade+1\n",
    "            combinacaoEntidadesAll.append([])\n",
    "        combinacaoEntidades = list()\n",
    "  \n",
    "    print('erro_corpus:', erro_corpus)\n",
    "    print('num_frases_sem_entidade:', num_frases_sem_entidade)\n",
    "    print('len(combinacaoEntidadesAll:)',len(combinacaoEntidadesAll))\n",
    "    \n",
    "    return combinacaoEntidadesAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Train--\n",
      "Só positivos\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 237\n",
      "len(combinacaoEntidadesAll:) 1319\n",
      "\n",
      "--Dev--\n",
      "Só positivos\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 89\n",
      "len(combinacaoEntidadesAll:) 416\n",
      "\n",
      "--Test--\n",
      "Só positivos\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 107\n",
      "len(combinacaoEntidadesAll:) 506\n"
     ]
    }
   ],
   "source": [
    "print('--Train--')\n",
    "combinacaoEntidadesTrainPos= getCombinacaoEntidadesSoPositivos(dic_sentencesTrain)\n",
    "print('\\n--Dev--')\n",
    "combinacaoEntidadesDevPos = getCombinacaoEntidadesSoPositivos(dic_sentencesDev)\n",
    "print('\\n--Test--')\n",
    "combinacaoEntidadesTestPos = getCombinacaoEntidadesSoPositivos(dic_sentencesTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Train--\n",
      "Sem filtro-postagger\n",
      "Sem taxa de Downsampling\n",
      "key: 999\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 237\n",
      "len(combinacaoEntidadesAll:) 1319\n",
      "len(pulando_termos_postagger): 0\n",
      "\n",
      "--Dev--\n",
      "Sem filtro-postagger\n",
      "Sem taxa de Downsampling\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 89\n",
      "len(combinacaoEntidadesAll:) 416\n",
      "len(pulando_termos_postagger): 0\n",
      "\n",
      "--Test--\n",
      "Sem filtro-postagger\n",
      "Sem taxa de Downsampling\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 107\n",
      "len(combinacaoEntidadesAll:) 506\n",
      "len(pulando_termos_postagger): 0\n"
     ]
    }
   ],
   "source": [
    "print('--Train--')\n",
    "combinacaoEntidadesTrain, _ = getCombinacaoEntidades(dic_sentencesTrain, False, '', 0)\n",
    "print('\\n--Dev--')\n",
    "combinacaoEntidadesDev, _ = getCombinacaoEntidades(dic_sentencesDev, False, '', 0)\n",
    "print('\\n--Test--')\n",
    "combinacaoEntidadesTest, _ = getCombinacaoEntidades(dic_sentencesTest, False, '', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aumento moderado de átrio esquerdo .',\n",
       " [['aumento <e1> moderado de átrio </e1> esquerdo .', 'O'],\n",
       "  ['aumento moderado <e1> de </e1> átrio esquerdo .', 'O'],\n",
       "  ['<e1> aumento moderado de átrio esquerdo </e1> .', 'Problema'],\n",
       "  ['<e1> aumento moderado de </e1> átrio esquerdo .', 'O'],\n",
       "  ['aumento moderado <e1> de átrio esquerdo </e1> .', 'O'],\n",
       "  ['aumento <e1> moderado de </e1> átrio esquerdo .', 'O'],\n",
       "  ['<e1> aumento moderado </e1> de átrio esquerdo .', 'O'],\n",
       "  ['aumento moderado de átrio <e1> esquerdo </e1> .', 'O'],\n",
       "  ['aumento moderado de <e1> átrio esquerdo </e1> .', 'Anatomia'],\n",
       "  ['<e1> aumento moderado de átrio </e1> esquerdo .', 'O'],\n",
       "  ['aumento <e1> moderado </e1> de átrio esquerdo .', 'O'],\n",
       "  ['<e1> aumento </e1> moderado de átrio esquerdo .', 'O'],\n",
       "  ['aumento <e1> moderado de átrio esquerdo </e1> .', 'O'],\n",
       "  ['aumento moderado <e1> de átrio </e1> esquerdo .', 'O'],\n",
       "  ['aumento moderado de <e1> átrio </e1> esquerdo .', 'O']]]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinacaoEntidadesTest[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['CONTRAÇÃO', 0, 507],\n",
       "  ['SEGMENTAR', 1, 517],\n",
       "  ['DO', 2, 527],\n",
       "  ['VE', 3, 530],\n",
       "  ['ALTERADA', 4, 533],\n",
       "  ['.', 5, 541]],\n",
       " [['CONTRAÇÃO SEGMENTAR DO VE ALTERADA', [0, 1, 2, 3, 4], 'Problema'],\n",
       "  ['VE', [3], 'Anatomia']]]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# o erro é que o ponto faz com que quebre a frase, mas a entidade continua...\n",
    "dic_sentencesTrain[829]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DSLP em uso de sinvastatina , marevan 1 cp / dia seg - sab para no alvo sic .',\n",
       " [['DSLP em uso de <e1> sinvastatina </e1> , marevan 1 cp / dia seg - sab para no alvo sic .',\n",
       "   'Tratamento'],\n",
       "  ['DSLP em uso de sinvastatina , <e1> marevan </e1> 1 cp / dia seg - sab para no alvo sic .',\n",
       "   'Tratamento'],\n",
       "  ['<e1> DSLP </e1> em uso de sinvastatina , marevan 1 cp / dia seg - sab para no alvo sic .',\n",
       "   'Problema']]]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinacaoEntidadesTest[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CONTRAÇÃO SEGMENTAR DO VE ALTERADA .',\n",
       " [['CONTRAÇÃO SEGMENTAR <e1> DO </e1> VE ALTERADA .', 'O'],\n",
       "  ['CONTRAÇÃO SEGMENTAR DO VE <e1> ALTERADA </e1> .', 'O'],\n",
       "  ['<e1> CONTRAÇÃO </e1> SEGMENTAR DO VE ALTERADA .', 'O'],\n",
       "  ['CONTRAÇÃO <e1> SEGMENTAR DO VE </e1> ALTERADA .', 'O'],\n",
       "  ['CONTRAÇÃO SEGMENTAR DO <e1> VE ALTERADA </e1> .', 'O'],\n",
       "  ['<e1> CONTRAÇÃO SEGMENTAR DO VE ALTERADA </e1> .', 'Problema'],\n",
       "  ['CONTRAÇÃO <e1> SEGMENTAR DO </e1> VE ALTERADA .', 'O'],\n",
       "  ['<e1> CONTRAÇÃO SEGMENTAR DO </e1> VE ALTERADA .', 'O'],\n",
       "  ['<e1> CONTRAÇÃO SEGMENTAR </e1> DO VE ALTERADA .', 'O'],\n",
       "  ['CONTRAÇÃO SEGMENTAR <e1> DO VE </e1> ALTERADA .', 'O'],\n",
       "  ['CONTRAÇÃO <e1> SEGMENTAR DO VE ALTERADA </e1> .', 'O'],\n",
       "  ['CONTRAÇÃO SEGMENTAR DO <e1> VE </e1> ALTERADA .', 'Anatomia'],\n",
       "  ['CONTRAÇÃO <e1> SEGMENTAR </e1> DO VE ALTERADA .', 'O'],\n",
       "  ['<e1> CONTRAÇÃO SEGMENTAR DO VE </e1> ALTERADA .', 'O'],\n",
       "  ['CONTRAÇÃO SEGMENTAR <e1> DO VE ALTERADA </e1> .', 'O']]]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinacaoEntidadesTrain[829]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HAS , ICC , nega DM .',\n",
       " [['HAS , <e1> ICC </e1> , nega DM .', 'Problema'],\n",
       "  ['<e1> HAS </e1> , ICC , nega DM .', 'Problema'],\n",
       "  ['HAS , ICC , nega <e1> DM </e1> .', 'Problema']]]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinacaoEntidadesDev[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gravarArquivosTreinamento(path, combinacaoEntidadesTrain, combinacaoEntidadesDev, combinacaoEntidadesTest):\n",
    "\n",
    "    # já ir gravando arquivos treinamento, test e dev...\n",
    "    # pra fazer um teste sem descontinuas\n",
    "    numTotalEntidades=0\n",
    "    numTotalEntidadesTrain=0\n",
    "    numTotalEntidadesDev=0\n",
    "    numTotalEntidadesTest=0\n",
    "\n",
    "    existeDir = os.path.exists(path)\n",
    "    if not existeDir:\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    f_train = open(path+r'\\span.train', 'w', encoding='utf-8')\n",
    "\n",
    "    for i, combinacaoEntidades in enumerate(combinacaoEntidadesTrain):\n",
    "        if len(combinacaoEntidades)>0:\n",
    "            frase = combinacaoEntidades[0]\n",
    "            frases_entidade = combinacaoEntidades[1]\n",
    "            f_train.write(frase+'\\n')\n",
    "            for frase_entidade in frases_entidade:\n",
    "                f_train.write(frase_entidade[1]+'\\t'+frase_entidade[0]+'\\n')\n",
    "                numTotalEntidades=numTotalEntidades+1\n",
    "                numTotalEntidadesTrain=numTotalEntidadesTrain+1\n",
    "\n",
    "    f_train.close()\n",
    "\n",
    "    f_dev = open(path+r'\\span.dev', 'w', encoding='utf-8')\n",
    "\n",
    "    for i, combinacaoEntidades in enumerate(combinacaoEntidadesDev):\n",
    "        if len(combinacaoEntidades)>0:\n",
    "            frase = combinacaoEntidades[0]\n",
    "            frases_entidade = combinacaoEntidades[1]\n",
    "            f_dev.write(frase+'\\n')\n",
    "            for frase_entidade in frases_entidade:\n",
    "                f_dev.write(frase_entidade[1]+'\\t'+frase_entidade[0]+'\\n')\n",
    "                numTotalEntidades=numTotalEntidades+1\n",
    "                numTotalEntidadesDev=numTotalEntidadesDev+1\n",
    "\n",
    "    f_dev.close()\n",
    "\n",
    "    f_test = open(path+r'\\span.test', 'w', encoding='utf-8')\n",
    "    for i, combinacaoEntidades in enumerate(combinacaoEntidadesTest):\n",
    "        #print(dicSentences[i])\n",
    "        if len(combinacaoEntidades)>0:\n",
    "            frase = combinacaoEntidades[0]\n",
    "            frases_entidade = combinacaoEntidades[1]\n",
    "            f_test.write(frase+'\\n')\n",
    "            for frase_entidade in frases_entidade:\n",
    "                f_test.write(frase_entidade[1]+'\\t'+frase_entidade[0]+'\\n')\n",
    "                numTotalEntidades=numTotalEntidades+1\n",
    "                numTotalEntidadesTest=numTotalEntidadesTest+1\n",
    "\n",
    "    f_test.close()\n",
    "\n",
    "    print('numTotalEntidades:', numTotalEntidades)\n",
    "    print('numTotalEntidadesTrain:', numTotalEntidadesTrain)\n",
    "    print('numTotalEntidadesDev:', numTotalEntidadesDev)\n",
    "    print('numTotalEntidadesTest:', numTotalEntidadesTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numTotalEntidades: 14133\n",
      "numTotalEntidadesTrain: 9079\n",
      "numTotalEntidadesDev: 2001\n",
      "numTotalEntidadesTest: 3053\n"
     ]
    }
   ],
   "source": [
    "gravarArquivosTreinamento('sem_filtro',combinacaoEntidadesTrain, combinacaoEntidadesDev, combinacaoEntidadesTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numTotalEntidades: 4204\n",
      "numTotalEntidadesTrain: 2510\n",
      "numTotalEntidadesDev: 703\n",
      "numTotalEntidadesTest: 991\n"
     ]
    }
   ],
   "source": [
    "gravarArquivosTreinamento('so_positivos',combinacaoEntidadesTrainPos, combinacaoEntidadesDevPos, combinacaoEntidadesTestPos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gravar arquivo para sentence pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCombinacaoEntidadesSentence(dic_predictions):\n",
    "    #labels = {0:'O', 1:'Problema', 2:'Tratamento', 3:'Teste', 4:'Anatomia'}\n",
    "    labels = {'O':0, 'Problema':1, 'Tratamento':2, 'Teste':3, 'Anatomia':4}\n",
    "    num=0\n",
    "    erro_corpus=0\n",
    "    num_frases_sem_entidade=0\n",
    "    lista_erro_corpus=list()\n",
    "    combinacaoEntidades = list()\n",
    "    print('Sentence Pairs - Só positivos')\n",
    "    for key, value in dic_predictions.items():\n",
    "        num=num+1\n",
    "        tokens=value[0].copy()\n",
    "        so_tokens = [t[0] for t in tokens]\n",
    "        entidades=value[1].copy()\n",
    "        num_positivas=0\n",
    "        for entidade in entidades:\n",
    "            erros_entidade = list()\n",
    "            texto_entidade=entidade[0].strip()\n",
    "            indices = entidade[1]\n",
    "            tipo_entidade = entidade[2]\n",
    "            frase = so_tokens.copy()\n",
    "            inicio=indices[0]\n",
    "            fim=indices[-1]\n",
    "            #entidade_frase=frase[inicio:fim+1] # texto_entidade\n",
    "            entidade_frase=texto_entidade\n",
    "            #print('entidade_frase:', entidade_frase)\n",
    "            #print('frase:', frase)\n",
    "            #print('texto_entidade:', texto_entidade)\n",
    "            if texto_entidade=='-' or texto_entidade=='=' or texto_entidade=='+' or texto_entidade==':' or texto_entidade==',' or texto_entidade==\"'\" or texto_entidade=='\"' or texto_entidade=='.' or texto_entidade==';' or texto_entidade=='/' or texto_entidade=='(' or texto_entidade==')' or texto_entidade=='[' or texto_entidade==']':\n",
    "                pass\n",
    "            texto_entidade_comparar=texto_entidade.replace('/','').replace(')','').replace('(','').replace(']','').replace('[','').replace(',','').replace('.','').replace(';','').replace('-','').replace('+','').replace(\"'\",'')\n",
    "            texto_entidade_comparar = replaceWhiteSpaces(texto_entidade_comparar)\n",
    "            texto_frase_comparar = ' '.join(frase[inicio:fim+1]).strip().replace('/','').replace(')','').replace('(','').replace(']','').replace('[','').replace(',','').replace('.','').replace(';','').replace('-','').replace('+','').replace(\"'\",'')\n",
    "            texto_frase_comparar = replaceWhiteSpaces(texto_frase_comparar)\n",
    "            texto_entidade_comparar = texto_entidade_comparar.lower()\n",
    "            texto_frase_comparar = texto_frase_comparar.lower()\n",
    "            #print('texto_entidade_comparar:', texto_entidade_comparar)\n",
    "            #print('texto_frase_comparar:', texto_frase_comparar)\n",
    "            if (texto_entidade_comparar == texto_frase_comparar):\n",
    "                num_positivas=num_positivas+1\n",
    "                combinacaoEntidades.append([entidade_frase, ' '.join(frase).strip(), labels[tipo_entidade]]) # apendando entidades reais\n",
    "            else:\n",
    "                print('erro, key:', key)\n",
    "                erro_corpus=erro_corpus+1\n",
    "                erros_entidade.append(indices)\n",
    "                lista_erro_corpus.append([' '.join(frase).strip(), tipo_entidade, ' '.join(so_tokens), entidade])\n",
    "\n",
    "        # shuffle no combinacaoEntidades\n",
    "        #if len(combinacaoEntidades)>0:\n",
    "        #    random.shuffle(combinacaoEntidades)\n",
    "        #    combinacaoEntidadesAll.append(combinacaoEntidades)\n",
    "        #else:\n",
    "        #    num_frases_sem_entidade = num_frases_sem_entidade+1\n",
    "            #combinacaoEntidadesAll.append([])\n",
    "        #combinacaoEntidades = list()\n",
    "        \n",
    "    random.shuffle(combinacaoEntidades)\n",
    "  \n",
    "    print('erro_corpus:', erro_corpus)\n",
    "    print('num_frases_sem_entidade:', num_frases_sem_entidade)\n",
    "    print('len(combinacaoEntidades:)',len(combinacaoEntidades))\n",
    "    \n",
    "    return combinacaoEntidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 4), match='12mg'>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(\"^[0-9]*mg\", '12mg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCombinacaoEntidadesSentence(dic_predictions, filtro_postagger, dicPosTagger, taxaDownsampling):\n",
    "    #labels = {0:'O', 1:'Problema', 2:'Tratamento', 3:'Teste', 4:'Anatomia'}\n",
    "    labels = {'O':0, 'Problema':1, 'Tratamento':2, 'Teste':3, 'Anatomia':4}\n",
    "    num=0\n",
    "    erro_corpus=0\n",
    "    num_frases_sem_entidade=0\n",
    "    lista_erro_corpus=list()\n",
    "    combinacaoEntidadesPos = list()\n",
    "    combinacaoEntidadesNeg = list()\n",
    "    combinacaoEntidades = list()\n",
    "    pulando_termos_postagger = list()\n",
    "    if filtro_postagger:\n",
    "        print('Sentence Pairs - Com filtro-postagger')\n",
    "    else:\n",
    "        print('Sentence Pairs - Sem filtro-postagger')\n",
    "    if taxaDownsampling>0:\n",
    "        print('Sentence Pairs - Com taxa de Downsampling de ', taxaDownsampling)\n",
    "    else:\n",
    "        print('Sentence Pairs - Sem taxa de Downsampling')\n",
    "\n",
    "    for key, value in dic_predictions.items():\n",
    "        num=num+1\n",
    "        tokens=value[0].copy()\n",
    "        so_tokens = [t[0] for t in tokens]\n",
    "        entidades=value[1].copy()\n",
    "        num_positivas=0\n",
    "        for entidade in entidades:\n",
    "            erros_entidade = list()\n",
    "            texto_entidade=entidade[0].strip()\n",
    "            indices = entidade[1]\n",
    "            tipo_entidade = entidade[2]\n",
    "            frase = so_tokens.copy()\n",
    "            inicio=indices[0]\n",
    "            fim=indices[-1]\n",
    "            #entidade_frase=frase[inicio:fim+1] # texto_entidade\n",
    "            entidade_frase=texto_entidade\n",
    "            #print('entidade_frase:', entidade_frase)\n",
    "            #print('frase:', frase)\n",
    "            #print('texto_entidade:', texto_entidade)\n",
    "            if texto_entidade=='-' or texto_entidade=='=' or texto_entidade=='+' or texto_entidade==':' or texto_entidade==',' or texto_entidade==\"'\" or texto_entidade=='\"' or texto_entidade=='.' or texto_entidade==';' or texto_entidade=='/' or texto_entidade=='(' or texto_entidade==')' or texto_entidade=='[' or texto_entidade==']':\n",
    "                pass\n",
    "            texto_entidade_comparar=texto_entidade.replace('/','').replace(')','').replace('(','').replace(']','').replace('[','').replace(',','').replace('.','').replace(';','').replace('-','').replace('+','').replace(\"'\",'')\n",
    "            texto_entidade_comparar = replaceWhiteSpaces(texto_entidade_comparar)\n",
    "            texto_frase_comparar = ' '.join(frase[inicio:fim+1]).strip().replace('/','').replace(')','').replace('(','').replace(']','').replace('[','').replace(',','').replace('.','').replace(';','').replace('-','').replace('+','').replace(\"'\",'')\n",
    "            texto_frase_comparar = replaceWhiteSpaces(texto_frase_comparar)\n",
    "            texto_entidade_comparar = texto_entidade_comparar.lower()\n",
    "            texto_frase_comparar = texto_frase_comparar.lower()\n",
    "            #print('texto_entidade_comparar:', texto_entidade_comparar)\n",
    "            #print('texto_frase_comparar:', texto_frase_comparar)\n",
    "            if (texto_entidade_comparar == texto_frase_comparar):\n",
    "                num_positivas=num_positivas+1\n",
    "                combinacaoEntidadesPos.append([entidade_frase, ' '.join(frase).strip(), labels[tipo_entidade]]) # apendando entidades reais\n",
    "            else:\n",
    "                print('erro, key:', key)\n",
    "                erro_corpus=erro_corpus+1\n",
    "                erros_entidade.append(indices)\n",
    "                lista_erro_corpus.append([' '.join(frase).strip(), tipo_entidade, ' '.join(so_tokens), entidade])\n",
    "        # agora, os negativos\n",
    "        for entidade in entidades:\n",
    "                indices = entidade[1]\n",
    "                #print('indices:', indices)\n",
    "                if indices in erros_entidade:\n",
    "                    continue\n",
    "                inicio=indices[0]\n",
    "                fim=indices[-1]\n",
    "                # agora, fazer a combinacao entre eles.. todas a seguir serão do tipo 'O'           \n",
    "                for indice in indices:\n",
    "                    for i in range(indice, fim+1):\n",
    "                        # ver se nao tem antes\n",
    "                        frase = so_tokens.copy()\n",
    "                        termo = frase[indice:i+1]\n",
    "                        #termo=entidade[0].strip()\n",
    "                        #print('--termo--:', termo)\n",
    "                        #frase.insert(indice, '<e1>')\n",
    "                        #frase.insert(i+2, '</e1>')\n",
    "                        #frase_string=' '.join(frase).strip()\n",
    "                        #frase_string=texto_entidade\n",
    "                        #frase_string=termo[0]\n",
    "                        frase_string=' '.join(termo).strip()\n",
    "                        #print('frase_string:', frase_string)\n",
    "                        devePular = 0\n",
    "                        if '.' in frase_string[-1:] or ',' in frase_string[-1:]  or ';' in frase_string[-1:] or '-' in frase_string[-1:]  or ':' in frase_string[-1:]  or '=' in frase_string[-1:]  or '/' in frase_string[-1:]  or '(' in frase_string[-1:]  or ')' in frase_string[-1:]  or '[' in frase_string[-1:]  or ']' in frase_string[-1:]  or ':' in frase_string[-1:]:\n",
    "                            devePular=1\n",
    "                        if '.' in frase_string[:1] or ',' in frase_string[:1]  or ';' in frase_string[:1] or '-' in frase_string[:1]  or ':' in frase_string[:1]  or '=' in frase_string[:1] or '/' in frase_string[:1]  or '(' in frase_string[:1]  or ')' in frase_string[:1] or '[' in frase_string[:1]  or ']' in frase_string[:1]  or ':' in frase_string[:1]:\n",
    "                            devePular=1\n",
    "                        if re.search(\"^[0-9]*mg\", frase_string):\n",
    "                            devePular=1\n",
    "                            \n",
    "                        if filtro_postagger==True:\n",
    "                            pos_tagger_termo = tipoPostaggerTokens(termo, dicPosTagger)\n",
    "                            if pos_tagger_termo not in lista_postaggers_entidades:\n",
    "                                pulando_termos_postagger.append([termo, pos_tagger_termo])\n",
    "                                devePular=1\n",
    "                \n",
    "                        tem_frase = 0\n",
    "                        for frase_l in combinacaoEntidadesPos:\n",
    "                            if frase_l[0] == frase_string:\n",
    "                                tem_frase='1'\n",
    "                                break\n",
    "                        if tem_frase==0 and devePular==0:\n",
    "                        #print('tem_frase:', tem_frase)\n",
    "                        #if tem_frase==0:\n",
    "                            #print('aaaaaaaaaaaa, frase_string:', frase_string)\n",
    "                            combinacaoEntidadesNeg.append([frase_string, ' '.join(frase).strip(), labels['O']])\n",
    "                        #combinacaoEntidadesNeg.append([frase_string, ' '.join(frase).strip(), labels['O']])\n",
    "                        \n",
    "        # shuffle no combinacaoEntidades\n",
    "        #if len(combinacaoEntidades)>0:\n",
    "        #    random.shuffle(combinacaoEntidades)\n",
    "        #    combinacaoEntidadesAll.append(combinacaoEntidades)\n",
    "        #else:\n",
    "        #    num_frases_sem_entidade = num_frases_sem_entidade+1\n",
    "            #combinacaoEntidadesAll.append([])\n",
    "        #combinacaoEntidades = list()\n",
    "        \n",
    "        # shuffle no combinacaoEntidades\n",
    "        # taxaDownsampling, ex 2 para o dobro, 1 para mesma quantidade\n",
    "        if len(combinacaoEntidadesPos)>0:\n",
    "            if taxaDownsampling>0:\n",
    "                combinacaoEntidadesNeg = combinacaoEntidadesNeg[:(num_positivas*taxaDownsampling)]\n",
    "            random.shuffle(combinacaoEntidadesNeg)\n",
    "        else:\n",
    "            num_frases_sem_entidade = num_frases_sem_entidade+1\n",
    "        if (num % 1000) ==0:\n",
    "            print('key:', key)\n",
    "\n",
    "        #print('combinacaoEntidadesNeg:',combinacaoEntidadesNeg)\n",
    "        combinacaoEntidades = combinacaoEntidades+combinacaoEntidadesPos+combinacaoEntidadesNeg\n",
    "        combinacaoEntidadesPos=list()\n",
    "        combinacaoEntidadesNeg=list()\n",
    "  \n",
    "    print('erro_corpus:', erro_corpus)\n",
    "    print('num_frases_sem_entidade:', num_frases_sem_entidade)\n",
    "    print('len(combinacaoEntidades:)',len(combinacaoEntidades))\n",
    "    \n",
    "    return combinacaoEntidades\n",
    "\n",
    "#combinacaoEntidadesTestSentence = getCombinacaoEntidadesSentence(dic_sentencesTest, True, dicPosTagger, 1)\n",
    "#combinacaoEntidadesTrain, pulando_termos_postaggerTrain = getCombinacaoEntidades(dic_sentencesTrain, True, dicPosTagger, 0)\n",
    "#combinacaoEntidadesTestSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCombinacaoEntidadesSentencePos(dic_predictions):\n",
    "    labels = {'Problema':0, 'Tratamento':1, 'Teste':2, 'Anatomia':3}\n",
    "    num=0\n",
    "    erro_corpus=0\n",
    "    num_frases_sem_entidade=0\n",
    "    lista_erro_corpus=list()\n",
    "    combinacaoEntidadesPos = list()\n",
    "    combinacaoEntidades = list()\n",
    "    pulando_termos_postagger = list()\n",
    "    print('Sentence Pairs - So positivos')\n",
    "\n",
    "    for key, value in dic_predictions.items():\n",
    "        num=num+1\n",
    "        tokens=value[0].copy()\n",
    "        so_tokens = [t[0] for t in tokens]\n",
    "        entidades=value[1].copy()\n",
    "        num_positivas=0\n",
    "        for entidade in entidades:\n",
    "            erros_entidade = list()\n",
    "            texto_entidade=entidade[0].strip()\n",
    "            indices = entidade[1]\n",
    "            tipo_entidade = entidade[2]\n",
    "            frase = so_tokens.copy()\n",
    "            inicio=indices[0]\n",
    "            fim=indices[-1]\n",
    "            #entidade_frase=frase[inicio:fim+1] # texto_entidade\n",
    "            entidade_frase=texto_entidade\n",
    "            #print('entidade_frase:', entidade_frase)\n",
    "            #print('frase:', frase)\n",
    "            #print('texto_entidade:', texto_entidade)\n",
    "            if texto_entidade=='-' or texto_entidade=='=' or texto_entidade=='+' or texto_entidade==':' or texto_entidade==',' or texto_entidade==\"'\" or texto_entidade=='\"' or texto_entidade=='.' or texto_entidade==';' or texto_entidade=='/' or texto_entidade=='(' or texto_entidade==')' or texto_entidade=='[' or texto_entidade==']':\n",
    "                pass\n",
    "            texto_entidade_comparar=texto_entidade.replace('/','').replace(')','').replace('(','').replace(']','').replace('[','').replace(',','').replace('.','').replace(';','').replace('-','').replace('+','').replace(\"'\",'')\n",
    "            texto_entidade_comparar = replaceWhiteSpaces(texto_entidade_comparar)\n",
    "            texto_frase_comparar = ' '.join(frase[inicio:fim+1]).strip().replace('/','').replace(')','').replace('(','').replace(']','').replace('[','').replace(',','').replace('.','').replace(';','').replace('-','').replace('+','').replace(\"'\",'')\n",
    "            texto_frase_comparar = replaceWhiteSpaces(texto_frase_comparar)\n",
    "            texto_entidade_comparar = texto_entidade_comparar.lower()\n",
    "            texto_frase_comparar = texto_frase_comparar.lower()\n",
    "            #print('texto_entidade_comparar:', texto_entidade_comparar)\n",
    "            #print('texto_frase_comparar:', texto_frase_comparar)\n",
    "            if (texto_entidade_comparar == texto_frase_comparar):\n",
    "                num_positivas=num_positivas+1\n",
    "                combinacaoEntidadesPos.append([entidade_frase, ' '.join(frase).strip(), labels[tipo_entidade]]) # apendando entidades reais\n",
    "            else:\n",
    "                print('erro, key:', key)\n",
    "                erro_corpus=erro_corpus+1\n",
    "                erros_entidade.append(indices)\n",
    "                lista_erro_corpus.append([' '.join(frase).strip(), tipo_entidade, ' '.join(so_tokens), entidade])\n",
    "\n",
    "        if len(combinacaoEntidadesPos)>0:\n",
    "            random.shuffle(combinacaoEntidadesPos)\n",
    "        else:\n",
    "            num_frases_sem_entidade = num_frases_sem_entidade+1\n",
    "        if (num % 1000) ==0:\n",
    "            print('key:', key)\n",
    "\n",
    "        #print('combinacaoEntidadesNeg:',combinacaoEntidadesNeg)\n",
    "        combinacaoEntidades = combinacaoEntidades+combinacaoEntidadesPos\n",
    "        combinacaoEntidadesPos=list()\n",
    "  \n",
    "    print('erro_corpus:', erro_corpus)\n",
    "    print('num_frases_sem_entidade:', num_frases_sem_entidade)\n",
    "    print('len(combinacaoEntidades:)',len(combinacaoEntidades))\n",
    "    \n",
    "    return combinacaoEntidades\n",
    "\n",
    "#combinacaoEntidadesTestSentence = getCombinacaoEntidadesSentencePos(dic_sentencesTest)\n",
    "#combinacaoEntidadesTrain, pulando_termos_postaggerTrain = getCombinacaoEntidades(dic_sentencesTrain, True, dicPosTagger, 0)\n",
    "#combinacaoEntidadesTestSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Train--\n",
      "Sentence Pairs - Sem filtro-postagger\n",
      "Sentence Pairs - Sem taxa de Downsampling\n",
      "key: 999\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 237\n",
      "len(combinacaoEntidades:) 9696\n",
      "\n",
      "--Dev--\n",
      "Sentence Pairs - Sem filtro-postagger\n",
      "Sentence Pairs - Sem taxa de Downsampling\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 89\n",
      "len(combinacaoEntidades:) 2159\n",
      "\n",
      "--Test--\n",
      "Sentence Pairs - Sem filtro-postagger\n",
      "Sentence Pairs - Sem taxa de Downsampling\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 107\n",
      "len(combinacaoEntidades:) 3233\n"
     ]
    }
   ],
   "source": [
    "print('--Train--')\n",
    "combinacaoEntidadesTrainSentence= getCombinacaoEntidadesSentence(dic_sentencesTrain, False, '', 0)\n",
    "print('\\n--Dev--')\n",
    "combinacaoEntidadesDevSentence = getCombinacaoEntidadesSentence(dic_sentencesDev, False, '', 0)\n",
    "print('\\n--Test--')\n",
    "combinacaoEntidadesTestSentence = getCombinacaoEntidadesSentence(dic_sentencesTest, False, '', 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gravarArquivosTreinamentoSentence(path, combinacaoEntidadesTrain, combinacaoEntidadesDev, combinacaoEntidadesTest):\n",
    "\n",
    "    # já ir gravando arquivos treinamento, test e dev...\n",
    "    # pra fazer um teste sem descontinuas\n",
    "    numTotalEntidades=0\n",
    "    numTotalEntidadesTrain=0\n",
    "    numTotalEntidadesDev=0\n",
    "    numTotalEntidadesTest=0\n",
    "\n",
    "    existeDir = os.path.exists(path)\n",
    "    if not existeDir:\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    f_train = open(path+r'\\sentence_pairs.train', 'w', encoding='utf-8')\n",
    "\n",
    "    for i, combinacaoEntidades in enumerate(combinacaoEntidadesTrain):\n",
    "        if len(combinacaoEntidades)>0:\n",
    "            termo = combinacaoEntidades[0]\n",
    "            frase = combinacaoEntidades[1]\n",
    "            label = str(combinacaoEntidades[2])\n",
    "            f_train.write(termo+'\\t'+frase+'\\t'+label+'\\n')\n",
    "            numTotalEntidades=numTotalEntidades+1\n",
    "            numTotalEntidadesTrain=numTotalEntidadesTrain+1\n",
    "    f_train.close()\n",
    "\n",
    "    f_dev = open(path+r'\\sentence_pairs.dev', 'w', encoding='utf-8')\n",
    "\n",
    "    for i, combinacaoEntidades in enumerate(combinacaoEntidadesDev):\n",
    "        if len(combinacaoEntidades)>0:\n",
    "            termo = combinacaoEntidades[0]\n",
    "            frase = combinacaoEntidades[1]\n",
    "            label = str(combinacaoEntidades[2])\n",
    "            f_dev.write(termo+'\\t'+frase+'\\t'+label+'\\n')\n",
    "            numTotalEntidades=numTotalEntidades+1\n",
    "            numTotalEntidadesDev=numTotalEntidadesDev+1\n",
    "    f_dev.close()\n",
    "\n",
    "    f_test = open(path+r'\\sentence_pairs.test', 'w', encoding='utf-8')\n",
    "    for i, combinacaoEntidades in enumerate(combinacaoEntidadesTest):\n",
    "        #print(dicSentences[i])\n",
    "        if len(combinacaoEntidades)>0:\n",
    "            termo = combinacaoEntidades[0]\n",
    "            frase = combinacaoEntidades[1]\n",
    "            label = str(combinacaoEntidades[2])\n",
    "            f_test.write(termo+'\\t'+frase+'\\t'+label+'\\n')\n",
    "            numTotalEntidades=numTotalEntidades+1\n",
    "            numTotalEntidadesTest=numTotalEntidadesTest+1\n",
    "    f_test.close()\n",
    "\n",
    "    print('numTotalEntidades:', numTotalEntidades)\n",
    "    print('numTotalEntidadesTrain:', numTotalEntidadesTrain)\n",
    "    print('numTotalEntidadesDev:', numTotalEntidadesDev)\n",
    "    print('numTotalEntidadesTest:', numTotalEntidadesTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numTotalEntidades: 15088\n",
      "numTotalEntidadesTrain: 9696\n",
      "numTotalEntidadesDev: 2159\n",
      "numTotalEntidadesTest: 3233\n"
     ]
    }
   ],
   "source": [
    "gravarArquivosTreinamentoSentence('sentence-pairs-all',combinacaoEntidadesTrainSentence, combinacaoEntidadesDevSentence, combinacaoEntidadesTestSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Train--\n",
      "Sentence Pairs - So positivos\n",
      "key: 999\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 237\n",
      "len(combinacaoEntidades:) 2510\n",
      "\n",
      "--Dev--\n",
      "Sentence Pairs - So positivos\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 89\n",
      "len(combinacaoEntidades:) 703\n",
      "\n",
      "--Test--\n",
      "Sentence Pairs - So positivos\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 107\n",
      "len(combinacaoEntidades:) 991\n",
      "numTotalEntidades: 4204\n",
      "numTotalEntidadesTrain: 2510\n",
      "numTotalEntidadesDev: 703\n",
      "numTotalEntidadesTest: 991\n"
     ]
    }
   ],
   "source": [
    "print('--Train--')\n",
    "combinacaoEntidadesTrainSentencePos= getCombinacaoEntidadesSentencePos(dic_sentencesTrain)\n",
    "print('\\n--Dev--')\n",
    "combinacaoEntidadesDevSentencePos = getCombinacaoEntidadesSentencePos(dic_sentencesDev)\n",
    "print('\\n--Test--')\n",
    "combinacaoEntidadesTestSentencePos = getCombinacaoEntidadesSentencePos(dic_sentencesTest)\n",
    "\n",
    "gravarArquivosTreinamentoSentence('sentence-pairs-positivos',combinacaoEntidadesTrainSentencePos, combinacaoEntidadesDevSentencePos, combinacaoEntidadesTestSentencePos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Train--\n",
      "Sentence Pairs - Com filtro-postagger\n",
      "Sentence Pairs - Com taxa de Downsampling de  1\n",
      "key: 999\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 237\n",
      "len(combinacaoEntidades:) 4062\n",
      "\n",
      "--Dev--\n",
      "Sentence Pairs - Com filtro-postagger\n",
      "Sentence Pairs - Com taxa de Downsampling de  1\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 89\n",
      "len(combinacaoEntidades:) 1048\n",
      "\n",
      "--Test--\n",
      "Sentence Pairs - Com filtro-postagger\n",
      "Sentence Pairs - Com taxa de Downsampling de  1\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 107\n",
      "len(combinacaoEntidades:) 1555\n",
      "numTotalEntidades: 6665\n",
      "numTotalEntidadesTrain: 4062\n",
      "numTotalEntidadesDev: 1048\n",
      "numTotalEntidadesTest: 1555\n"
     ]
    }
   ],
   "source": [
    "print('--Train--')\n",
    "combinacaoEntidadesTrainSentence= getCombinacaoEntidadesSentence(dic_sentencesTrain, True, dicPosTagger, 1)\n",
    "print('\\n--Dev--')\n",
    "combinacaoEntidadesDevSentence = getCombinacaoEntidadesSentence(dic_sentencesDev, True, dicPosTagger, 1)\n",
    "print('\\n--Test--')\n",
    "combinacaoEntidadesTestSentence = getCombinacaoEntidadesSentence(dic_sentencesTest, True, dicPosTagger, 1)\n",
    "gravarArquivosTreinamentoSentence('sentence-pairs-downsampling',combinacaoEntidadesTrainSentence, combinacaoEntidadesDevSentence, combinacaoEntidadesTestSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Train--\n",
      "Sentence Pairs - Com filtro-postagger\n",
      "Sentence Pairs - Com taxa de Downsampling de  1\n",
      "key: 999\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 237\n",
      "len(combinacaoEntidades:) 5037\n",
      "\n",
      "--Dev--\n",
      "Sentence Pairs - Com filtro-postagger\n",
      "Sentence Pairs - Com taxa de Downsampling de  1\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 89\n",
      "len(combinacaoEntidades:) 1264\n",
      "\n",
      "--Test--\n",
      "Sentence Pairs - Com filtro-postagger\n",
      "Sentence Pairs - Com taxa de Downsampling de  1\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 107\n",
      "len(combinacaoEntidades:) 1917\n"
     ]
    }
   ],
   "source": [
    "#OLD\n",
    "print('--Train--')\n",
    "combinacaoEntidadesTrainSentence= getCombinacaoEntidadesSentence(dic_sentencesTrain, True, dicPosTagger, 1)\n",
    "print('\\n--Dev--')\n",
    "combinacaoEntidadesDevSentence = getCombinacaoEntidadesSentence(dic_sentencesDev, True, dicPosTagger, 1)\n",
    "print('\\n--Test--')\n",
    "combinacaoEntidadesTestSentence = getCombinacaoEntidadesSentence(dic_sentencesTest, True, dicPosTagger, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numTotalEntidades: 8218\n",
      "numTotalEntidadesTrain: 5037\n",
      "numTotalEntidadesDev: 1264\n",
      "numTotalEntidadesTest: 1917\n"
     ]
    }
   ],
   "source": [
    "#gravarArquivosTreinamentoSentence('sentence-pairs-filtro',combinacaoEntidadesTrainSentence, combinacaoEntidadesDevSentence, combinacaoEntidadesTestSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Train--\n",
      "Sentence Pairs - Com filtro-postagger\n",
      "Sentence Pairs - Sem taxa de Downsampling\n",
      "key: 999\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 237\n",
      "len(combinacaoEntidades:) 6753\n",
      "\n",
      "--Dev--\n",
      "Sentence Pairs - Com filtro-postagger\n",
      "Sentence Pairs - Sem taxa de Downsampling\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 89\n",
      "len(combinacaoEntidades:) 1635\n",
      "\n",
      "--Test--\n",
      "Sentence Pairs - Com filtro-postagger\n",
      "Sentence Pairs - Sem taxa de Downsampling\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 107\n",
      "len(combinacaoEntidades:) 2478\n",
      "numTotalEntidades: 10866\n",
      "numTotalEntidadesTrain: 6753\n",
      "numTotalEntidadesDev: 1635\n",
      "numTotalEntidadesTest: 2478\n"
     ]
    }
   ],
   "source": [
    "print('--Train--')\n",
    "combinacaoEntidadesTrainSentence= getCombinacaoEntidadesSentence(dic_sentencesTrain, True, dicPosTagger, 0)\n",
    "print('\\n--Dev--')\n",
    "combinacaoEntidadesDevSentence = getCombinacaoEntidadesSentence(dic_sentencesDev, True, dicPosTagger, 0)\n",
    "print('\\n--Test--')\n",
    "combinacaoEntidadesTestSentence = getCombinacaoEntidadesSentence(dic_sentencesTest, True, dicPosTagger, 0)\n",
    "gravarArquivosTreinamentoSentence('sentence-pairs-filtro',combinacaoEntidadesTrainSentence, combinacaoEntidadesDevSentence, combinacaoEntidadesTestSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Train--\n",
      "Sentence Pairs - Com filtro-postagger\n",
      "Sentence Pairs - Sem taxa de Downsampling\n",
      "key: 999\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 237\n",
      "len(combinacaoEntidades:) 6753\n",
      "\n",
      "--Dev--\n",
      "Sentence Pairs - Com filtro-postagger\n",
      "Sentence Pairs - Sem taxa de Downsampling\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 89\n",
      "len(combinacaoEntidades:) 1635\n",
      "\n",
      "--Test--\n",
      "Sentence Pairs - Com filtro-postagger\n",
      "Sentence Pairs - Sem taxa de Downsampling\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 107\n",
      "len(combinacaoEntidades:) 2478\n",
      "numTotalEntidades: 10866\n",
      "numTotalEntidadesTrain: 6753\n",
      "numTotalEntidadesDev: 1635\n",
      "numTotalEntidadesTest: 2478\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "print('--Train--')\n",
    "combinacaoEntidadesTrainSentence= getCombinacaoEntidadesSentence(dic_sentencesTrain, True, dicPosTagger, 0)\n",
    "print('\\n--Dev--')\n",
    "combinacaoEntidadesDevSentence = getCombinacaoEntidadesSentence(dic_sentencesDev, True, dicPosTagger, 0)\n",
    "print('\\n--Test--')\n",
    "combinacaoEntidadesTestSentence = getCombinacaoEntidadesSentence(dic_sentencesTest, True, dicPosTagger, 0)\n",
    "gravarArquivosTreinamentoSentence('sentence-pairs-downsampling',combinacaoEntidadesTrainSentence, combinacaoEntidadesDevSentence, combinacaoEntidadesTestSentence)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinacaoEntidadesTestSentence[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parte 2.2 - tentar tirar verbos, CC, etc do O para nao ficar com mto FP\n",
    "### depois, com nosso pos tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3484"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"pucpr-br/postagger-bio-portuguese\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('pucpr-br/postagger-bio-portuguese')\n",
    "\n",
    "nlp_token_class = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy='max')\n",
    "\n",
    "def getDicPosTagger(dic_sentencesTrainDev):\n",
    "    dicPostagger = load_obj('dic_postagger')\n",
    "    allFrases = load_obj('allFrases')\n",
    "    if dicPostagger==None or allFrases==None:\n",
    "        dicPostagger = {}\n",
    "        allFrases=[]\n",
    "        for key, value in dic_sentencesTrainDev.items():\n",
    "            tokens=value[0]\n",
    "            frase = [t[0] for t in tokens]\n",
    "            frase = ' '.join(frase)\n",
    "            allFrases.append(frase)\n",
    "            #print(frase)\n",
    "\n",
    "        #print(allFrases)\n",
    "        doc = nlp_token_class(allFrases)\n",
    "        #print(doc)\n",
    "        for frase in doc:\n",
    "            for d in frase:\n",
    "                #print(d)\n",
    "                pos = d['entity_group']\n",
    "                #print(pos)\n",
    "                token=d['word']\n",
    "                if pos=='PREP+ART':\n",
    "                    pos='ART'\n",
    "                if pos=='NPROP':\n",
    "                    pos='N'\n",
    "                if 'ADV' in pos: # ADV-KS, ADV-KS-REL\t\n",
    "                    pos='ADV'\n",
    "                if 'PRON' in pos: # PRO-KS\t, PRO-KS-REL, PROPESS, PROPSUB\n",
    "                    pos='PRON'\n",
    "                if pos=='VAUX'or pos=='PCP': #participio\n",
    "                    pos='V'\n",
    "                dicPostagger[token] = pos    \n",
    "\n",
    "        save_obj('dicPostagger', dicPostagger)\n",
    "        save_obj('allFrases', allFrases)\n",
    "    return dicPostagger, allFrases\n",
    "\n",
    "dic_sentencesTrainDev = load_obj('dic_sentencesTrainDev')\n",
    "\n",
    "dicPosTagger, _ = getDicPosTagger(dic_sentencesTrainDev)\n",
    "#dicPosTagger, _ = getDicPosTagger(dic_sentencesTest)\n",
    "len(dicPosTagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-N-PREP-ART-N-ADV-PU'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tipoPostaggerTokens(entidade_token, dicPostagger):\n",
    "    postagger = ''\n",
    "    for p in entidade_token:\n",
    "        #print('p:', p)\n",
    "        if p.lower() in dicPostagger.keys():\n",
    "            postagger = postagger + '-' + dicPostagger.get(p.lower())\n",
    "        else:\n",
    "            #print('nao tem:', p)\n",
    "            # se nao tem, considera N\n",
    "            postagger = postagger + '-' + 'N'\n",
    "    return postagger\n",
    "tipoPostaggerTokens(['ola','como','aos','você','hoje', '?'], dicPosTagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_sentencesTrainDev = load_obj('dic_sentencesTrainDev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-N-ADJ-ART-N',\n",
       " '-N-N-N-ART-N-ART-N',\n",
       " '-N',\n",
       " '-N-ADJ',\n",
       " '-ADJ',\n",
       " '-V',\n",
       " '-N-NUM',\n",
       " '-N-PU-ADJ-N',\n",
       " '-ADJ-N-N-ADJ-PREP-N',\n",
       " '-N-PREP-N-ART-N']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getListaPostaggerEntidades(dic_sentencesTrainDev, dicPosTagger):\n",
    "    lista_postaggers_entidades = []\n",
    "    for key, value in dic_sentencesTrainDev.items():\n",
    "        entidades = value[1]\n",
    "        for entidade in entidades:\n",
    "            #print(entidade[0])\n",
    "            pos_tagger=tipoPostaggerTokens(entidade[0].split(), dicPosTagger)\n",
    "            if pos_tagger not in lista_postaggers_entidades:\n",
    "                lista_postaggers_entidades.append(pos_tagger)\n",
    "    return lista_postaggers_entidades\n",
    "\n",
    "lista_postaggers_entidades = getListaPostaggerEntidades(dic_sentencesTrainDev, dicPosTagger)\n",
    "lista_postaggers_entidades[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-N-ADJ-ADJ-N-N-N',\n",
       " '-N-ADJ-N-N-N',\n",
       " '-ADJ-N-N-N',\n",
       " '-N-ADV-N-N-PU-NUM-N',\n",
       " '-ADJ-N-PREP-N',\n",
       " '-N-PREP-N-N-ADV',\n",
       " '-V-PDEN-PREP-ADJ-N',\n",
       " '-N-N-PREP-N-PU-N-PU-N-PU-N-PREP-N',\n",
       " '-ADJ-N-PREP-N-ADJ']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_postaggers_entidades[-10:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print('-KC' in lista_postaggers_entidades) # e\n",
    "print('-PREP+ART' in lista_postaggers_entidades) \n",
    "print('-PREP' in lista_postaggers_entidades)\n",
    "print('-ART' in lista_postaggers_entidades)\n",
    "print('-V' in lista_postaggers_entidades)\n",
    "print('-ADJ' in lista_postaggers_entidades)\n",
    "print('-PU' in lista_postaggers_entidades)\n",
    "print('-PCP' in lista_postaggers_entidades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Train--\n",
      "Com filtro-postagger\n",
      "Sem taxa de Downsampling\n",
      "key: 999\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 237\n",
      "len(combinacaoEntidadesAll:) 1319\n",
      "len(pulando_termos_postagger): 3925\n",
      "\n",
      "--Dev--\n",
      "Com filtro-postagger\n",
      "Sem taxa de Downsampling\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 89\n",
      "len(combinacaoEntidadesAll:) 416\n",
      "len(pulando_termos_postagger): 745\n",
      "\n",
      "--Test--\n",
      "Com filtro-postagger\n",
      "Sem taxa de Downsampling\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 107\n",
      "len(combinacaoEntidadesAll:) 506\n",
      "len(pulando_termos_postagger): 1183\n"
     ]
    }
   ],
   "source": [
    "print('--Train--')\n",
    "combinacaoEntidadesTrain, pulando_termos_postaggerTrain = getCombinacaoEntidades(dic_sentencesTrain, True, dicPosTagger, 0)\n",
    "print('\\n--Dev--')\n",
    "combinacaoEntidadesDev, pulando_termos_postaggerDev= getCombinacaoEntidades(dic_sentencesDev, True, dicPosTagger, 0)\n",
    "print('\\n--Test--')\n",
    "combinacaoEntidadesTest, pulando_termos_postaggerTest= getCombinacaoEntidades(dic_sentencesTest, True, dicPosTagger, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aumento moderado de átrio esquerdo .',\n",
       " [['aumento moderado <e1> de átrio esquerdo </e1> .', 'O'],\n",
       "  ['aumento <e1> moderado </e1> de átrio esquerdo .', 'O'],\n",
       "  ['aumento moderado <e1> de </e1> átrio esquerdo .', 'O'],\n",
       "  ['aumento <e1> moderado de </e1> átrio esquerdo .', 'O'],\n",
       "  ['<e1> aumento moderado de átrio esquerdo </e1> .', 'Problema'],\n",
       "  ['aumento moderado de <e1> átrio esquerdo </e1> .', 'Anatomia'],\n",
       "  ['aumento moderado de átrio <e1> esquerdo </e1> .', 'O'],\n",
       "  ['aumento <e1> moderado de átrio esquerdo </e1> .', 'O'],\n",
       "  ['aumento moderado <e1> de átrio </e1> esquerdo .', 'O'],\n",
       "  ['aumento <e1> moderado de átrio </e1> esquerdo .', 'O'],\n",
       "  ['<e1> aumento </e1> moderado de átrio esquerdo .', 'O'],\n",
       "  ['aumento moderado de <e1> átrio </e1> esquerdo .', 'O']]]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinacaoEntidadesTest[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['25'], '-NUM'],\n",
       " [['ventrículo', 'esquerdo', 'com'], '-N-ADJ-PREP'],\n",
       " [['ventrículo', 'esquerdo', 'com', 'hipertrofia', 'concentrica', 'de'],\n",
       "  '-N-ADJ-PREP-N-N-PREP'],\n",
       " [['ventrículo',\n",
       "   'esquerdo',\n",
       "   'com',\n",
       "   'hipertrofia',\n",
       "   'concentrica',\n",
       "   'de',\n",
       "   'grau'],\n",
       "  '-N-ADJ-PREP-N-N-PREP-N'],\n",
       " [['ventrículo',\n",
       "   'esquerdo',\n",
       "   'com',\n",
       "   'hipertrofia',\n",
       "   'concentrica',\n",
       "   'de',\n",
       "   'grau',\n",
       "   'discreto'],\n",
       "  '-N-ADJ-PREP-N-N-PREP-N-ADJ'],\n",
       " [['esquerdo', 'com'], '-ADJ-PREP'],\n",
       " [['esquerdo', 'com', 'hipertrofia'], '-ADJ-PREP-N'],\n",
       " [['esquerdo', 'com', 'hipertrofia', 'concentrica'], '-ADJ-PREP-N-N'],\n",
       " [['esquerdo', 'com', 'hipertrofia', 'concentrica', 'de'],\n",
       "  '-ADJ-PREP-N-N-PREP'],\n",
       " [['esquerdo', 'com', 'hipertrofia', 'concentrica', 'de', 'grau'],\n",
       "  '-ADJ-PREP-N-N-PREP-N'],\n",
       " [['esquerdo', 'com', 'hipertrofia', 'concentrica', 'de', 'grau', 'discreto'],\n",
       "  '-ADJ-PREP-N-N-PREP-N-ADJ'],\n",
       " [['com', 'hipertrofia', 'concentrica', 'de'], '-PREP-N-N-PREP'],\n",
       " [['com', 'hipertrofia', 'concentrica', 'de', 'grau'], '-PREP-N-N-PREP-N'],\n",
       " [['com', 'hipertrofia', 'concentrica', 'de', 'grau', 'discreto'],\n",
       "  '-PREP-N-N-PREP-N-ADJ'],\n",
       " [['hipertrofia', 'concentrica', 'de'], '-N-N-PREP'],\n",
       " [['hipertrofia', 'concentrica', 'de', 'grau', 'discreto'], '-N-N-PREP-N-ADJ'],\n",
       " [['aumento', 'moderado'], '-V-N'],\n",
       " [['aumento', 'moderado', 'de'], '-V-N-PREP'],\n",
       " [['aumento', 'moderado', 'de', 'átrio'], '-V-N-PREP-N'],\n",
       " [['aumento', 'moderado', 'de', 'átrio', 'esquerdo'], '-V-N-PREP-N-ADJ'],\n",
       " [['calcificação', 'mitral', 'e'], '-N-ADJ-KC'],\n",
       " [['calcificação', 'mitral', 'e', 'aórtica'], '-N-ADJ-KC-N'],\n",
       " [['calcificação', 'mitral', 'e', 'aórtica', 'com'], '-N-ADJ-KC-N-PREP'],\n",
       " [['calcificação', 'mitral', 'e', 'aórtica', 'com', 'refluxo'],\n",
       "  '-N-ADJ-KC-N-PREP-N'],\n",
       " [['calcificação', 'mitral', 'e', 'aórtica', 'com', 'refluxo', 'leve'],\n",
       "  '-N-ADJ-KC-N-PREP-N-ADJ'],\n",
       " [['mitral', 'e'], '-ADJ-KC'],\n",
       " [['mitral', 'e', 'aórtica'], '-ADJ-KC-N'],\n",
       " [['mitral', 'e', 'aórtica', 'com'], '-ADJ-KC-N-PREP'],\n",
       " [['mitral', 'e', 'aórtica', 'com', 'refluxo'], '-ADJ-KC-N-PREP-N'],\n",
       " [['mitral', 'e', 'aórtica', 'com', 'refluxo', 'leve'],\n",
       "  '-ADJ-KC-N-PREP-N-ADJ'],\n",
       " [['e'], '-KC'],\n",
       " [['e', 'aórtica'], '-KC-N'],\n",
       " [['e', 'aórtica', 'com'], '-KC-N-PREP'],\n",
       " [['e', 'aórtica', 'com', 'refluxo'], '-KC-N-PREP-N'],\n",
       " [['e', 'aórtica', 'com', 'refluxo', 'leve'], '-KC-N-PREP-N-ADJ'],\n",
       " [['nos', 'MMII'], '-ART-N'],\n",
       " [['comprometimento', 'difuso', 'do'], '-N-N-ART'],\n",
       " [['comprometimento', 'difuso', 'do', 'VE', 'grau'], '-N-N-ART-N-N'],\n",
       " [['comprometimento', 'difuso', 'do', 'VE', 'grau', 'moderado'],\n",
       "  '-N-N-ART-N-N-N'],\n",
       " [['difuso', 'do', 'VE', 'grau', 'moderado'], '-N-ART-N-N-N'],\n",
       " [['do', 'VE'], '-ART-N'],\n",
       " [['do', 'VE', 'grau'], '-ART-N-N'],\n",
       " [['do', 'VE', 'grau', 'moderado'], '-ART-N-N-N'],\n",
       " [['fraqueza', 'intermitente', 'em'], '-N-N-PREP'],\n",
       " [['queixas', 'urinarias', 'e'], '-N-N-KC'],\n",
       " [['queixas', 'urinarias', 'e', 'gastrointestinais'], '-N-N-KC-N'],\n",
       " [['urinarias', 'e', 'gastrointestinais'], '-N-KC-N'],\n",
       " [['e'], '-KC'],\n",
       " [['e', 'gastrointestinais'], '-KC-N'],\n",
       " [['/'], '-PU']]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pulando_termos_postaggerTest[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Dispneia', 'importante', 'aos'], '-N-ADJ-ART'],\n",
       " [['importante', 'aos'], '-ADJ-ART'],\n",
       " [['importante', 'aos', 'esforços'], '-ADJ-ART-N'],\n",
       " [['aos', 'esforços'], '-ART-N'],\n",
       " [['dor', 'tipo', 'peso', 'no'], '-N-N-N-ART'],\n",
       " [['dor', 'tipo', 'peso', 'no', 'peito'], '-N-N-N-ART-N'],\n",
       " [['dor', 'tipo', 'peso', 'no', 'peito', 'no'], '-N-N-N-ART-N-ART'],\n",
       " [['tipo', 'peso', 'no'], '-N-N-ART'],\n",
       " [['tipo', 'peso', 'no', 'peito', 'no'], '-N-N-ART-N-ART'],\n",
       " [['tipo', 'peso', 'no', 'peito', 'no', 'esforço'], '-N-N-ART-N-ART-N'],\n",
       " [['peso', 'no', 'peito', 'no'], '-N-ART-N-ART'],\n",
       " [['no', 'peito'], '-ART-N'],\n",
       " [['no', 'peito', 'no'], '-ART-N-ART'],\n",
       " [['no', 'peito', 'no', 'esforço'], '-ART-N-ART-N'],\n",
       " [['no', 'esforço'], '-ART-N'],\n",
       " [['100'], '-NUM'],\n",
       " [['25'], '-NUM'],\n",
       " [['175'], '-NUM'],\n",
       " [['40'], '-NUM'],\n",
       " [['20'], '-NUM']]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pulando_termos_postaggerTrain[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numTotalEntidades: 10626\n",
      "numTotalEntidadesTrain: 6578\n",
      "numTotalEntidadesDev: 1611\n",
      "numTotalEntidadesTest: 2437\n"
     ]
    }
   ],
   "source": [
    "gravarArquivosTreinamento('com_filtro',combinacaoEntidadesTrain, combinacaoEntidadesDev, combinacaoEntidadesTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numTotalEntidades: 8965\n",
      "numTotalEntidadesTrain: 5533\n",
      "numTotalEntidadesDev: 1389\n",
      "numTotalEntidadesTest: 2043\n"
     ]
    }
   ],
   "source": [
    "#OLD\n",
    "#gravarArquivosTreinamento('com_filtro',combinacaoEntidadesTrain, combinacaoEntidadesDev, combinacaoEntidadesTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Train--\n",
      "Com filtro-postagger\n",
      "Com taxa de Downsampling de  1\n",
      "key: 999\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 237\n",
      "len(combinacaoEntidadesAll:) 1319\n",
      "len(pulando_termos_postagger): 3925\n",
      "\n",
      "--Dev--\n",
      "Com filtro-postagger\n",
      "Com taxa de Downsampling de  1\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 89\n",
      "len(combinacaoEntidadesAll:) 416\n",
      "len(pulando_termos_postagger): 745\n",
      "\n",
      "--Test--\n",
      "Com filtro-postagger\n",
      "Com taxa de Downsampling de  1\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 107\n",
      "len(combinacaoEntidadesAll:) 506\n",
      "len(pulando_termos_postagger): 1183\n",
      "numTotalEntidades: 6664\n",
      "numTotalEntidadesTrain: 4055\n",
      "numTotalEntidadesDev: 1050\n",
      "numTotalEntidadesTest: 1559\n"
     ]
    }
   ],
   "source": [
    "# com downsampling\n",
    "print('--Train--')\n",
    "combinacaoEntidadesTrain, pulando_termos_postaggerTrain = getCombinacaoEntidades(dic_sentencesTrain, True, dicPosTagger, 1)\n",
    "print('\\n--Dev--')\n",
    "combinacaoEntidadesDev, pulando_termos_postaggerDev= getCombinacaoEntidades(dic_sentencesDev, True, dicPosTagger, 1)\n",
    "print('\\n--Test--')\n",
    "combinacaoEntidadesTest, pulando_termos_postaggerTest= getCombinacaoEntidades(dic_sentencesTest, True, dicPosTagger, 1)\n",
    "\n",
    "gravarArquivosTreinamento('com-filtro-downsampling',combinacaoEntidadesTrain, combinacaoEntidadesDev, combinacaoEntidadesTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agora, tratar descontinuas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1568"
      ]
     },
     "execution_count": 837,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combinacaoEntidadesAll_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16614"
      ]
     },
     "execution_count": 838,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combinacaoEntidadesAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numTreinamento: 13291\n",
      "numTeste: 1661\n",
      "numTotalEntidades: 165571\n",
      "numTotalEntidadesTrain: 131376\n",
      "numTotalEntidadesDev: 16986\n",
      "numTotalEntidadesTest: 17209\n"
     ]
    }
   ],
   "source": [
    "# já ir gravando arquivos treinamento, test e dev...\n",
    "# pra fazer um teste sem descontinuas\n",
    "numTreinamento = len(combinacaoEntidadesAll)*0.8\n",
    "numTreinamento = int(numTreinamento)\n",
    "numTeste = (len(combinacaoEntidadesAll) - numTreinamento)/2\n",
    "numTeste = int(numTeste)\n",
    "numTotalEntidades=0\n",
    "numTotalEntidadesTrain=0\n",
    "numTotalEntidadesDev=0\n",
    "numTotalEntidadesTest=0\n",
    "\n",
    "print('numTreinamento:', numTreinamento)\n",
    "print('numTeste:', numTeste)\n",
    "\n",
    "f_train = open('genia.train', 'w')\n",
    "f_dev = open('genia.dev', 'w')\n",
    "f_test = open('genia.test', 'w')\n",
    "f_dev_entidades = open('genia_entidades.dev', 'w')\n",
    "f_test_entidades = open('genia_entidades.test', 'w')\n",
    "\n",
    "for i, combinacaoEntidades in enumerate(combinacaoEntidadesAll):\n",
    "    #print(dicSentences[i])\n",
    "    frase = combinacaoEntidades[0]\n",
    "    frases_entidade = combinacaoEntidades[1]\n",
    "    if i<=numTreinamento:\n",
    "        f_train.write(frase+'\\n')\n",
    "        for frase_entidade in frases_entidade:\n",
    "            f_train.write(frase_entidade[1]+'\\t'+frase_entidade[0]+'\\n')\n",
    "            numTotalEntidades=numTotalEntidades+1\n",
    "            numTotalEntidadesTrain=numTotalEntidadesTrain+1\n",
    "    elif i <= numTreinamento + numTeste:\n",
    "        f_dev.write(frase+'\\n')\n",
    "        for frase_entidade in frases_entidade:\n",
    "            f_dev.write(frase_entidade[1]+'\\t'+frase_entidade[0]+'\\n')\n",
    "            f_dev_entidades.write(frase_entidade[1]+'\\t'+frase_entidade[0]+'\\n')\n",
    "            numTotalEntidades=numTotalEntidades+1\n",
    "            numTotalEntidadesDev=numTotalEntidadesDev+1\n",
    "    else:\n",
    "        f_test.write(frase+'\\n')\n",
    "        for frase_entidade in frases_entidade:\n",
    "            f_test.write(frase_entidade[1]+'\\t'+frase_entidade[0]+'\\n')\n",
    "            f_test_entidades.write(frase_entidade[1]+'\\t'+frase_entidade[0]+'\\n')\n",
    "            numTotalEntidades=numTotalEntidades+1\n",
    "            numTotalEntidadesTest=numTotalEntidadesTest+1\n",
    "\n",
    "f_train.close()\n",
    "f_test.close()\n",
    "f_dev.close()\n",
    "f_test_entidades.close()\n",
    "f_dev_entidades.close()\n",
    "\n",
    "print('numTotalEntidades:', numTotalEntidades)\n",
    "print('numTotalEntidadesTrain:', numTotalEntidadesTrain)\n",
    "print('numTotalEntidadesDev:', numTotalEntidadesDev)\n",
    "print('numTotalEntidadesTest:', numTotalEntidadesTest)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
