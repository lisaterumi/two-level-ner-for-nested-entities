{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerando arquivos para NER-NestedClinBr\n",
    "\n",
    "Deixar sempre o arquivo de test para fazer o teste completo (com o pipeline todo)\n",
    "1o. -> passar pelo modelo de NER normal (binario), pegar entidades nivel externo\n",
    "2o. -> novo modelo de NER para entidades internas\n",
    "\n",
    "Descontinuas: tentar RBERT - relação entre entidades... pensar em como fazer.. pra nao ficar pesado... se estiver desbalanceado, dar menor peso para label O no crossentropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pickle\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTiposEntidade():\n",
    "    return ['Problema','Teste','Tratamento','Anatomia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceWhiteSpaces(str):\n",
    "    return re.sub('\\s{2,}',' ',str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(name, obj):\n",
    "    existeDir = os.path.exists('obj')\n",
    "    if not existeDir:\n",
    "        os.makedirs('obj')\n",
    "    with open('obj/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_obj(name):\n",
    "    existeDir = os.path.exists('obj')\n",
    "    if not existeDir:\n",
    "        os.makedirs('obj')\n",
    "    try:\n",
    "        with open('obj/' + name + '.pkl', 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    except:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pastasCorpus = [r'C:\\Users\\lisat\\OneDrive\\jupyter notebook\\spanclassification\\preProcessamento\\corpus\\test',r'C:\\Users\\lisat\\OneDrive\\jupyter notebook\\spanclassification\\preProcessamento\\corpus\\train']\n",
    "#pastasCorpus = [r'corpus\\test']\n",
    "#pastasCorpus = [r'corpus\\TESTE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make the World a 2y4Better Place2y0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "s = 'Make the World a 2.4Better Place2.0'\n",
    "pattern = r'([0-9])\\.([0-9])'\n",
    "replacement = r'\\1y\\2'\n",
    "html = re.sub(pattern, replacement, s)\n",
    "\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# com todas entidades corretamente (descontinuas)\n",
    "def getDicSentencesGabarito(pastaCorpus):\n",
    "    #devePrintar=True\n",
    "    devePrintar=False\n",
    "    dic_sentences = {}\n",
    "    numMaxTokensPorFrase=0 # numero tokens da maior frase\n",
    "    numMaxTokensPorEntidade=0 # numero tokens da maior entidade\n",
    "    num=0\n",
    "    listaEntidades=[]\n",
    "    frasesComDescontinuas=[]\n",
    "    for filename in os.listdir(pastaCorpus):\n",
    "        f = os.path.join(pastaCorpus, filename)\n",
    "        if os.path.isfile(f):\n",
    "            fileNameSemExtensao=os.path.splitext(filename)[0]\n",
    "            if devePrintar:\n",
    "                print('\\n\\n--fileName (sem extensao):--', fileNameSemExtensao)\n",
    "                pass\n",
    "            extension = os.path.splitext(filename)[1][1:]\n",
    "            if extension=='ann':\n",
    "                # tokens da frase\n",
    "                fileTxt = open(os.path.join(pastaCorpus, fileNameSemExtensao)+'.txt', \"r\", encoding='utf-8')\n",
    "                linha=fileTxt.readlines()\n",
    "                fileTxt.close()\n",
    "                if devePrintar:\n",
    "                    print('linha:', linha)\n",
    "                    pass\n",
    "                frases=[]\n",
    "                allFrasesString=''\n",
    "                numL=0\n",
    "                for l in linha:\n",
    "                    numL=numL+1\n",
    "                    allFrasesString = allFrasesString+l\n",
    "                    if numL==1: # descarta primeira frase, que é data de criação do doc\n",
    "                        #print('descartando frase:', l)\n",
    "                        continue\n",
    "                    if l.strip() and l.strip()!='\\n':\n",
    "                        #print('l:', l)\n",
    "                        pattern = r'([0-9])\\.([0-9])'\n",
    "                        replacement = r'\\1==\\2'\n",
    "                        novoL = re.sub(pattern, replacement, l.strip())\n",
    "                        l2 = novoL.split('.') # quebrando frases\n",
    "                        for l3 in l2:\n",
    "                            if l3.strip() and l3.strip()!='\\n':\n",
    "                                novaFrase = l3.replace('\\n','').replace('==','.').strip()+'.'\n",
    "                                frases.append(novaFrase)\n",
    "                #print('frases:', frases)\n",
    "                # para cada frase\n",
    "                # para tokenizar nesses tokens\n",
    "                #print('allFrasesString:', allFrasesString)\n",
    "                frasesTokens={}\n",
    "                #numCaracteresTotal=42 # primeira frase ignorada\n",
    "                numIndiceAnterior=0\n",
    "                for frase in frases:\n",
    "                    tokens=[]\n",
    "                    frase2 = frase.strip().replace('/',' / ').replace(')',' ) ').replace('(',' ( ').replace(']',' ] ').replace('[',' [ ').replace(',',' , ').replace('.',' . ').replace(';',' ; ').replace('-',' - ').replace('+',' + ').replace(\"'\",\" ' \").replace(\" = \",\" = \").replace(\"#\",\" # \").replace(\" $ \",\" $ \").replace(\" ! \",\" ! \").replace(\"?\",\" ? \").replace(\"%\",\" % \").replace(\":\",\" : \").replace(\">\",\" > \").replace(\"<\",\" < \")\n",
    "                    frase2 = replaceWhiteSpaces(frase2)\n",
    "                    frase2 = frase2.split()\n",
    "                    for numtoken, token in enumerate(frase2):\n",
    "                        if devePrintar:\n",
    "                            print('token:', token)\n",
    "                            print('numIndiceAnterior:', numIndiceAnterior)\n",
    "                        if token!='.':\n",
    "                            numCaracteresTotal = allFrasesString.find(token, numIndiceAnterior, len(allFrasesString))\n",
    "                        else:\n",
    "                            numCaracteresTotal=numIndiceAnterior+1\n",
    "                        #tokens.append([token,numtoken])\n",
    "                        tokens.append([token,numtoken, numCaracteresTotal])\n",
    "                        numIndiceAnterior = numCaracteresTotal+len(token)-1\n",
    "                        if numMaxTokensPorFrase<len(tokens):\n",
    "                            numMaxTokensPorFrase = len(tokens)\n",
    "                    frasesTokens[frase]=tokens\n",
    "                    #linhaTokens=linha.copy()    \n",
    "                if devePrintar:\n",
    "                    print('frasesTokens:', frasesTokens)\n",
    "                # agora, as entidades\n",
    "                fileAnn = open(f, \"r\", encoding='utf-8')\n",
    "                linha=fileAnn.readlines()\n",
    "                fileAnn.close()\n",
    "\n",
    "                dicEntidades={}\n",
    "                dicEntidadesDescontinuas={}\n",
    "                listaEntidadesDescontinuas={}\n",
    "                numIndiceDesc=0\n",
    "                for entidade_linha in linha:\n",
    "                    if ';' not in entidade_linha: # nao é descontinua\n",
    "                        entidade = entidade_linha.split('\\t')\n",
    "                        tipo_entidade = entidade[1]\n",
    "                        inicio, fim = tipo_entidade.split()[1:3]\n",
    "                        tipo_entidade = tipo_entidade.split()[0]\n",
    "                        termos_entidade = entidade[2].replace('\\n','')\n",
    "                        termos_entidade = termos_entidade.strip().replace('/',' / ').replace(')',' ) ').replace('(',' ( ').replace(']',' ] ').replace('[',' [ ').replace(',',' , ').replace('.',' . ').replace(';',' ; ').replace('-',' - ').replace('+',' + ').replace(\"'\",\" ' \").replace(\" = \",\" = \").replace(\"#\",\" # \").replace(\" $ \",\" $ \").replace(\" ! \",\" ! \").replace(\"?\",\" ? \").replace(\"%\",\" % \").replace(\":\",\" : \").replace(\">\",\" > \").replace(\"<\",\" < \")\n",
    "                        dicEntidades[(int(inicio), int(fim))]=[tipo_entidade, replaceWhiteSpaces(termos_entidade)]\n",
    "                    else:\n",
    "                        frasesComDescontinuas.append(num)\n",
    "                        #print('descontinua, linha: {}, num: {}'.format(linha, num))\n",
    "                        #print('descontinua, file: {}, num: {}'.format(filename, num))\n",
    "                        \n",
    "                        # grava com os indices corretoss\n",
    "                        entidade = entidade_linha.split('\\t')\n",
    "                        # ex T10\tProblema 244 252;279 306\tdispneia aos mdoeardos-leves esforço\n",
    "                        #Problema 244 252;279 306\n",
    "                        entidade_temp=entidade[1].split(';')\n",
    "                        entidade1=entidade_temp[0]\n",
    "                        tipo_entidade = entidade1\n",
    "                        inicio1, fim1 = tipo_entidade.split()[1:3]\n",
    "                        tipo_entidade_string = tipo_entidade.split()[0]\n",
    "                        # mandar só os termos referentes...\n",
    "                        tamTermo1=int(fim1)-int(inicio1)\n",
    "                        termos_entidade = entidade[2].replace('\\n','')\n",
    "                        termos_entidade=termos_entidade[:tamTermo1]\n",
    "                        termos_entidade = termos_entidade.strip().replace('/',' / ').replace(')',' ) ').replace('(',' ( ').replace(']',' ] ').replace('[',' [ ').replace(',',' , ').replace('.',' . ').replace(';',' ; ').replace('-',' - ').replace('+',' + ').replace(\"'\",\" ' \").replace(\" = \",\" = \").replace(\"#\",\" # \").replace(\" $ \",\" $ \").replace(\" ! \",\" ! \").replace(\"?\",\" ? \").replace(\"%\",\" % \").replace(\":\",\" : \").replace(\">\",\" > \").replace(\"<\",\" < \")\n",
    "                        #print('aaaaaaaaaaaaaaaaa')\n",
    "                        \n",
    "                        dicEntidadesDescontinuas[(int(inicio1), int(fim1))]=[tipo_entidade_string, replaceWhiteSpaces(termos_entidade)]\n",
    "                        #print(\"(int(inicio1), int(fim1)):\", (int(inicio1), int(fim1)))\n",
    "                        #print(\"tipo_entidade_string, replaceWhiteSpaces(termos_entidade):\", tipo_entidade_string, replaceWhiteSpaces(termos_entidade))\n",
    "                        \n",
    "                        entidade2=entidade_temp[1]\n",
    "                        inicio2, fim2 = entidade2.split()[0:2]\n",
    "                        termos_entidade = entidade[2].replace('\\n','')\n",
    "                        termos_entidade=termos_entidade[tamTermo1:len(termos_entidade)]\n",
    "                        termos_entidade = termos_entidade.strip().replace('/',' / ').replace(')',' ) ').replace('(',' ( ').replace(']',' ] ').replace('[',' [ ').replace(',',' , ').replace('.',' . ').replace(';',' ; ').replace('-',' - ').replace('+',' + ').replace(\"'\",\" ' \").replace(\" = \",\" = \").replace(\"#\",\" # \").replace(\" $ \",\" $ \").replace(\" ! \",\" ! \").replace(\"?\",\" ? \").replace(\"%\",\" % \").replace(\":\",\" : \").replace(\">\",\" > \").replace(\"<\",\" < \")\n",
    "                        dicEntidadesDescontinuas[(int(inicio2), int(fim2))]=[tipo_entidade_string, replaceWhiteSpaces(termos_entidade)]\n",
    "                        #print(\"(int(inicio2), int(fim2)):\", (int(inicio2), int(fim2)))\n",
    "                        #print(\"tipo_entidade_string, replaceWhiteSpaces(termos_entidade):\", tipo_entidade_string, replaceWhiteSpaces(termos_entidade))\n",
    "                        listaEntidadesDescontinuas[numIndiceDesc] = dicEntidadesDescontinuas\n",
    "                        dicEntidadesDescontinuas={}\n",
    "                        numIndiceDesc=numIndiceDesc+1\n",
    "                        \n",
    "                        \n",
    "                #print('--listaEntidadesDescontinuas:--', listaEntidadesDescontinuas)\n",
    "                        \n",
    "                #print('frasesTokens:', frasesTokens)\n",
    "                indicesDic = sorted(dicEntidades.keys(), key = lambda item: item[0])\n",
    "                #indicesDicDescontinuas = sorted(dicEntidadesDescontinuas.keys(), key = lambda item: item[0])\n",
    "                listaIndicesJaUsados = []\n",
    "                #list_students.sort(key = lambda x: x[1])   #index 1 means second element\n",
    "                #print('indicesDicDescontinuas:', indicesDicDescontinuas)\n",
    "                #print('dicEntidades:', dicEntidades)\n",
    "                \n",
    "                for key, value in frasesTokens.items():\n",
    "                    #print('key:', key)\n",
    "                    #print('value:', value)\n",
    "                    for i in indicesDic:\n",
    "                        tipo_entidade,termos_entidade = dicEntidades[i]\n",
    "                        ##key (i) = indices old a comparar\n",
    "                        #print('i:', i)\n",
    "                        for token in value:\n",
    "                            #print('token:', token)\n",
    "                            if i[0]==token[2]:\n",
    "                                novo_inicio, novo_fim = [token[1],token[1]+len(termos_entidade.split())]\n",
    "                                novos_indices=[]\n",
    "                                for k in range(novo_inicio,novo_fim):\n",
    "                                    novos_indices.append(k)\n",
    "                                listaEntidades.append([termos_entidade, novos_indices, tipo_entidade])\n",
    "                                if len(termos_entidade.split()) > numMaxTokensPorEntidade:\n",
    "                                    numMaxTokensPorEntidade = len(termos_entidade.split())\n",
    "                            else:\n",
    "                                #print('else, i[0]:',i[0])\n",
    "                                pass\n",
    "                    # agora, descontinuas\n",
    "                    for keyD, valueD in listaEntidadesDescontinuas.items():\n",
    "                        #print('valueD>', valueD)\n",
    "                        listTemp= list(valueD)\n",
    "                        indice1=listTemp[0]\n",
    "                        indice2=listTemp[1]\n",
    "                        tipo_entidade1,termos_entidade1 = valueD[indice1]\n",
    "                        tipo_entidade2,termos_entidade2 = valueD[indice2]\n",
    "                        #print('indice1:{},tipo_entidade1:{},termos_entidade1:{},indice2:{},tipo_entidade2:{},termos_entidade2:{}'.format(indice1, tipo_entidade1,termos_entidade1,indice2, tipo_entidade2,termos_entidade2))\n",
    "                        achou=0\n",
    "                        token1=''\n",
    "                        for token in value:\n",
    "                            #print('token:', token)\n",
    "                            if indice1[0]==token[2]: # indice igual na primeira palavra\n",
    "                                achou=1\n",
    "                                token1 = token\n",
    "                            if achou==1:\n",
    "                                if indice2[0]==token[2]: # segunda palavra bate tb\n",
    "                                    novo_inicio1, novo_fim1 = [token1[1],token1[1]+len(termos_entidade1.split())]\n",
    "                                    novo_inicio2, novo_fim2 = [token[1],token[1]+len(termos_entidade2.split())]\n",
    "                                    novos_indices=[]\n",
    "                                    for k in range(novo_inicio1,novo_fim1):\n",
    "                                        novos_indices.append(k)\n",
    "                                    for k in range(novo_inicio2,novo_fim2):\n",
    "                                        novos_indices.append(k)\n",
    "                                    listaEntidades.append([termos_entidade1+' '+termos_entidade2, novos_indices, tipo_entidade1])\n",
    "                                    #print('[termos_entidade1+' '+termos_entidade2, novos_indices, tipo_entidade1]', [termos_entidade1+' '+termos_entidade2, novos_indices, tipo_entidade1])\n",
    "                        \n",
    "                    if len(value)>0:\n",
    "                        #print('incluindo:', key)\n",
    "                        dic_sentences[num]=[value, listaEntidades]\n",
    "                        listaEntidades=[]\n",
    "                        num=num+1 \n",
    "\n",
    "        #print(num)\n",
    "        #if num>318:\n",
    "        #    break\n",
    "\n",
    "        #if num>5:\n",
    "        #    break\n",
    "    print('numMaxTokensPorFrase:', numMaxTokensPorFrase)\n",
    "    print('numMaxTokensPorEntidade:', numMaxTokensPorEntidade)\n",
    "    \n",
    "    return dic_sentences, frasesComDescontinuas\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numMaxTokensPorFrase: 146\n",
      "numMaxTokensPorEntidade: 18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[['MMII', 0, 695],\n",
       "  ['sem', 1, 700],\n",
       "  ['edema', 2, 704],\n",
       "  [',', 3, 709],\n",
       "  ['panturrilhas', 4, 711],\n",
       "  ['livres', 5, 724],\n",
       "  ['.', 6, 730]],\n",
       " [['MMII', [0], 'Anatomia'],\n",
       "  ['panturrilhas', [4], 'Anatomia'],\n",
       "  ['MMII edema', [0, 2], 'Problema']]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_sentencesTest, frasesComDescontinuasTest = getDicSentencesGabarito(pastasCorpus[0])\n",
    "\n",
    "dic_sentencesTest[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numMaxTokensPorFrase: 192\n",
      "numMaxTokensPorEntidade: 27\n"
     ]
    }
   ],
   "source": [
    "dic_sentencesTrain, frasesComDescontinuasTrain = getDicSentencesGabarito(pastasCorpus[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "# frases com entidades descontinuas\n",
    "print(len(set(frasesComDescontinuasTrain)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "print(len(set(frasesComDescontinuasTest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Treinamento:-- corpus\\train\n",
      "numMaxTokensPorFrase: 192\n",
      "len(sentences): 1736\n",
      "len(Descontinuas): 91\n",
      "len(frasesComDescontinuas): 51\n",
      "--Teste:-- corpus\\test\n",
      "numMaxTokensPorFrase: 146\n",
      "len(sentences): 506\n",
      "len(Descontinuas): 44\n",
      "len(frasesComDescontinuas): 17\n"
     ]
    }
   ],
   "source": [
    "print('--Treinamento:--', pastasCorpus[1])\n",
    "dic_sentencesTrain, frasesComDescontinuasTrain = getDicSentences(pastasCorpus[1])\n",
    "print('len(sentences):', len(dic_sentencesTrain))\n",
    "print('len(Descontinuas):',len(frasesComDescontinuasTrain))\n",
    "print('len(frasesComDescontinuas):',len(set(frasesComDescontinuasTrain)))\n",
    "\n",
    "\n",
    "print('--Teste:--', pastasCorpus[0])\n",
    "dic_sentencesTest, frasesComDescontinuasTest = getDicSentences(pastasCorpus[0])\n",
    "print('len(sentences):', len(dic_sentencesTest))\n",
    "print('len(Descontinuas):',len(frasesComDescontinuasTest))\n",
    "print('len(frasesComDescontinuas):',len(set(frasesComDescontinuasTest)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tamanhoTrain 1319\n",
      "tamanhoDev 417\n",
      "len(dic_sentencesTrain): 1319\n",
      "len(dic_sentencesDev): 416\n",
      "[[['Paciente', 0, 151], ['relata', 1, 160], ['apenas', 2, 167], ['um', 3, 174], ['episodio', 4, 177], ['no', 5, 186], ['momento', 6, 189], ['de', 7, 197], ['gripe', 8, 200], ['.', 9, 205]], [['gripe', [8], 'Problema']]]\n",
      "[[['HAS', 0, 207], [',', 1, 210], ['ICC', 2, 212], [',', 3, 215], ['nega', 4, 217], ['DM', 5, 222], ['.', 6, 224]], [['HAS', [0], 'Problema'], ['ICC', [2], 'Problema'], ['DM', [5], 'Problema']]]\n"
     ]
    }
   ],
   "source": [
    "save_obj('dic_sentencesTrainDev',dic_sentencesTrain)\n",
    "porc=0.76\n",
    "tamanhoTotal = len(dic_sentencesTrain)\n",
    "tamanhoTrain = int(tamanhoTotal*porc)\n",
    "print('tamanhoTrain', tamanhoTrain)\n",
    "tamanhoDev = tamanhoTotal - tamanhoTrain\n",
    "print('tamanhoDev', tamanhoDev)\n",
    "dic_sentencesDev_temp = {k: dic_sentencesTrain[k] for k in list(dic_sentencesTrain)[tamanhoTrain:-1]}\n",
    "dic_sentencesTrain = {k: dic_sentencesTrain[k] for k in list(dic_sentencesTrain)[:tamanhoTrain]}\n",
    "num=0\n",
    "dic_sentencesDev = {}\n",
    "for key, value in dic_sentencesDev_temp.items():\n",
    "    dic_sentencesDev[num] = value\n",
    "    num=num+1\n",
    "\n",
    "print('len(dic_sentencesTrain):', len(dic_sentencesTrain))\n",
    "print('len(dic_sentencesDev):', len(dic_sentencesDev))\n",
    "print(dic_sentencesTrain[tamanhoTrain-1])\n",
    "print(dic_sentencesDev[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['HAS', 0, 207],\n",
       "  [',', 1, 210],\n",
       "  ['ICC', 2, 212],\n",
       "  [',', 3, 215],\n",
       "  ['nega', 4, 217],\n",
       "  ['DM', 5, 222],\n",
       "  ['.', 6, 224]],\n",
       " [['HAS', [0], 'Problema'], ['ICC', [2], 'Problema'], ['DM', [5], 'Problema']]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_sentencesDev[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['carvedilol', 0, 226],\n",
       "  ['1', 1, 237],\n",
       "  ['cp', 2, 239],\n",
       "  ['12', 3, 242],\n",
       "  ['/', 4, 244],\n",
       "  ['12', 5, 245],\n",
       "  [',', 6, 247],\n",
       "  ['furosemida', 7, 249],\n",
       "  ['20mg', 8, 260],\n",
       "  ['2', 9, 265],\n",
       "  ['cp', 10, 267],\n",
       "  ['de', 11, 270],\n",
       "  ['12', 12, 273],\n",
       "  ['/', 13, 275],\n",
       "  ['12', 14, 276],\n",
       "  [',', 15, 278],\n",
       "  ['sinvastatina', 16, 280],\n",
       "  ['1cp', 17, 293],\n",
       "  ['a', 18, 297],\n",
       "  ['noite', 19, 299],\n",
       "  [',', 20, 304],\n",
       "  ['AAS', 21, 306],\n",
       "  ['100mg', 22, 310],\n",
       "  ['apos', 23, 316],\n",
       "  ['almoço', 24, 321],\n",
       "  ['e', 25, 328],\n",
       "  ['Omeprazol', 26, 330],\n",
       "  ['20mg', 27, 340],\n",
       "  ['1', 28, 345],\n",
       "  ['xdia', 29, 347],\n",
       "  ['.', 30, 351]],\n",
       " [['carvedilol', [0], 'Tratamento'],\n",
       "  ['furosemida 20mg', [7, 8], 'Tratamento'],\n",
       "  ['sinvastatina', [16], 'Tratamento'],\n",
       "  ['AAS 100mg', [21, 22], 'Tratamento'],\n",
       "  ['Omeprazol 20mg', [26, 27], 'Tratamento']]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_sentencesDev[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj('dic_sentencesTrain',dic_sentencesTrain)\n",
    "save_obj('dic_sentencesDev',dic_sentencesDev)\n",
    "save_obj('dic_sentencesTest',dic_sentencesTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gravando em  data-ner\n",
      "num_entidade_train 13801\n",
      "num_entidade_dev 4009\n",
      "num_entidade_test: 5453\n",
      "num_entidade_total: 23263\n"
     ]
    }
   ],
   "source": [
    "# gerar arquivo treinamento\n",
    "path='data-ner'\n",
    "f_train = open(path+r'\\nested_train.conll', 'w', encoding='utf-8')\n",
    "print('Gravando em ', path)\n",
    "num_entidade_total=0\n",
    "num_entidade_train=0\n",
    "num_entidade_dev=0\n",
    "num_entidade_test=0\n",
    "\n",
    "\n",
    "for i in range(len(dic_sentencesTrain)):\n",
    "    tokens = dic_sentencesTrain[i][0]\n",
    "    ents = dic_sentencesTrain[i][1]\n",
    "    indiceEnts=[]\n",
    "    for token in tokens:\n",
    "        #print('token:', token)\n",
    "        indiceToken = token[1]\n",
    "        tag='O'\n",
    "        for ent in ents:\n",
    "            if indiceToken in ent[1]:\n",
    "                tag = ent[2]\n",
    "                break\n",
    "        tokenGravar = token[0].replace(' ','')\n",
    "        tokenGravar = tokenGravar.strip()\n",
    "        f_train.write(tokenGravar+' '+tag+'\\n')\n",
    "        num_entidade_train=num_entidade_train+1\n",
    "    f_train.write('\\n')\n",
    "        \n",
    "f_train.close()\n",
    "\n",
    "f_dev = open(path+r'\\nested_dev.conll', 'w', encoding='utf-8')\n",
    "for i in range(len(dic_sentencesDev)):\n",
    "    tokens = dic_sentencesDev[i][0]\n",
    "    ents = dic_sentencesDev[i][1]\n",
    "    indiceEnts=[]\n",
    "    for token in tokens:\n",
    "        #print('token:', token)\n",
    "        indiceToken = token[1]\n",
    "        tag='O'\n",
    "        for ent in ents:\n",
    "            if indiceToken in ent[1]:\n",
    "                tag = ent[2]\n",
    "                break\n",
    "        tokenGravar = token[0].replace(' ','')\n",
    "        tokenGravar = tokenGravar.strip()\n",
    "        f_dev.write(tokenGravar+' '+tag+'\\n')\n",
    "        num_entidade_dev=num_entidade_dev+1\n",
    "    f_dev.write('\\n')\n",
    "f_dev.close()\n",
    "\n",
    "\n",
    "f_test = open(path+r'\\nested_test.conll', 'w', encoding='utf-8')\n",
    "for i in range(len(dic_sentencesTest)):\n",
    "    tokens = dic_sentencesTest[i][0]\n",
    "    ents = dic_sentencesTest[i][1]\n",
    "    indiceEnts=[]\n",
    "    for token in tokens:\n",
    "        #print('token:', token)\n",
    "        indiceToken = token[1]\n",
    "        tag='O'\n",
    "        for ent in ents:\n",
    "            if indiceToken in ent[1]:\n",
    "                tag = ent[2]\n",
    "                break\n",
    "        tokenGravar = token[0].replace(' ','')\n",
    "        tokenGravar = tokenGravar.strip()\n",
    "        f_test.write(tokenGravar+' '+tag+'\\n')\n",
    "        num_entidade_test=num_entidade_test+1\n",
    "    f_test.write('\\n')\n",
    "f_test.close()\n",
    "\n",
    "print('num_entidade_train', num_entidade_train)\n",
    "print('num_entidade_dev', num_entidade_dev)\n",
    "print('num_entidade_test:', num_entidade_test)\n",
    "num_entidade_total=num_entidade_train+num_entidade_dev+num_entidade_test\n",
    "print('num_entidade_total:', num_entidade_total)\n",
    "\n",
    "save_obj('dic_sentencesTrain',dic_sentencesTrain)\n",
    "save_obj('dic_sentencesDev',dic_sentencesDev)\n",
    "save_obj('dic_sentencesTest',dic_sentencesTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Dispneia', 0, 43],\n",
       "  ['importante', 1, 52],\n",
       "  ['aos', 2, 63],\n",
       "  ['esforços', 3, 67],\n",
       "  ['+', 4, 76],\n",
       "  ['dor', 5, 78],\n",
       "  ['tipo', 6, 82],\n",
       "  ['peso', 7, 87],\n",
       "  ['no', 8, 92],\n",
       "  ['peito', 9, 95],\n",
       "  ['no', 10, 101],\n",
       "  ['esforço', 11, 104],\n",
       "  ['.', 12, 111]],\n",
       " [['Dispneia importante aos esforços', [0, 1, 2, 3], 'Problema'],\n",
       "  ['dor tipo peso no peito no esforço', [5, 6, 7, 8, 9, 10, 11], 'Problema'],\n",
       "  ['peito', [9], 'Anatomia']]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_sentencesTrain[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerar modelos binarios para baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gravarArquivosBinarios(entidade, dic_sentences, tipo):\n",
    "    # gerar arquivo treinamento\n",
    "    f_entidade = open(r'binarios/nested_'+tipo+'_'+entidade+'.conll', 'w', encoding='utf-8')\n",
    "\n",
    "    num_entidade_total=0\n",
    "    num_entidade=0\n",
    "\n",
    "    # TODO - refazer.. qdo vem entidade isolada, nao está gravando...\n",
    "    print('\\nGravando arquivo de {} para entidade {}'.format(tipo, entidade))\n",
    "\n",
    "    for i in range(len(dic_sentences)):\n",
    "        tokens = dic_sentences[i][0]\n",
    "        ents = dic_sentences[i][1]\n",
    "        indiceEnts=[]\n",
    "        for token in tokens:\n",
    "            #print('token:', token)\n",
    "            indiceToken = token[1]\n",
    "            tag='O'\n",
    "            for ent in ents:\n",
    "                if indiceToken in ent[1] and ent[2]==entidade:\n",
    "                    tag = ent[2]\n",
    "                    num_entidade=num_entidade+1\n",
    "                    break\n",
    "            if tag != entidade:\n",
    "                tag='O'\n",
    "            tokenGravar = token[0].replace(' ','')\n",
    "            tokenGravar = tokenGravar.strip()\n",
    "            f_entidade.write(tokenGravar+' '+tag+'\\n')\n",
    "            num_entidade_total=num_entidade_total+1\n",
    "        f_entidade.write('\\n')\n",
    "\n",
    "    f_entidade.close()\n",
    "\n",
    "    print('num_entidade:', num_entidade)\n",
    "    print('num_entidade_total:', num_entidade_total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gravando arquivo de train para entidade Problema\n",
      "num_entidade: 2297\n",
      "num_entidade_total: 13801\n",
      "\n",
      "Gravando arquivo de dev para entidade Problema\n",
      "num_entidade: 538\n",
      "num_entidade_total: 4009\n",
      "\n",
      "Gravando arquivo de test para entidade Problema\n",
      "num_entidade: 795\n",
      "num_entidade_total: 5453\n",
      "\n",
      "Gravando arquivo de train para entidade Teste\n",
      "num_entidade: 754\n",
      "num_entidade_total: 13801\n",
      "\n",
      "Gravando arquivo de dev para entidade Teste\n",
      "num_entidade: 172\n",
      "num_entidade_total: 4009\n",
      "\n",
      "Gravando arquivo de test para entidade Teste\n",
      "num_entidade: 308\n",
      "num_entidade_total: 5453\n",
      "\n",
      "Gravando arquivo de train para entidade Tratamento\n",
      "num_entidade: 1191\n",
      "num_entidade_total: 13801\n",
      "\n",
      "Gravando arquivo de dev para entidade Tratamento\n",
      "num_entidade: 377\n",
      "num_entidade_total: 4009\n",
      "\n",
      "Gravando arquivo de test para entidade Tratamento\n",
      "num_entidade: 450\n",
      "num_entidade_total: 5453\n",
      "\n",
      "Gravando arquivo de train para entidade Anatomia\n",
      "num_entidade: 582\n",
      "num_entidade_total: 13801\n",
      "\n",
      "Gravando arquivo de dev para entidade Anatomia\n",
      "num_entidade: 139\n",
      "num_entidade_total: 4009\n",
      "\n",
      "Gravando arquivo de test para entidade Anatomia\n",
      "num_entidade: 250\n",
      "num_entidade_total: 5453\n"
     ]
    }
   ],
   "source": [
    "entidades = ['Problema','Teste','Tratamento','Anatomia']\n",
    "#entidades = ['Anatomia']\n",
    "for entidade in entidades:\n",
    "    gravarArquivosBinarios(entidade, dic_sentencesTrain, 'train')\n",
    "    gravarArquivosBinarios(entidade, dic_sentencesDev, 'dev')\n",
    "    gravarArquivosBinarios(entidade, dic_sentencesTest, 'test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gravar arquivo para multilabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gravarArquivosMultilabel(dic_sentences, tipo):\n",
    "    # gerar arquivo treinamento\n",
    "    f_entidade = open(r'multilabel/nested_'+tipo+'.conll', 'w', encoding='utf-8')\n",
    "\n",
    "    num_entidade_total=0\n",
    "\n",
    "    print('\\nGravando arquivo multilabel de {}'.format(tipo))\n",
    "\n",
    "    entidades = ['Problema','Teste','Tratamento','Anatomia']\n",
    "    for i in range(len(dic_sentences)):\n",
    "        tokens = dic_sentences[i][0]\n",
    "        ents = dic_sentences[i][1]\n",
    "        #print('tokens:', tokens)\n",
    "        #print('ents:', ents)\n",
    "        for token in tokens:\n",
    "            listaTokensLabels = list()\n",
    "            #print('token:', token)\n",
    "            indiceToken = token[1]\n",
    "            #print('indiceToken:', indiceToken)\n",
    "            labels = list()\n",
    "            for ent in ents:\n",
    "                #print('ent:', ent)\n",
    "                #print('indiceToken:', indiceToken)\n",
    "                if indiceToken in ent[1]:\n",
    "                    for entidade in entidades:\n",
    "                        tag = ent[2]\n",
    "                        if tag==entidade:\n",
    "                            labels.append(tag)\n",
    "                            num_entidade_total=num_entidade_total+1\n",
    "                        else:\n",
    "                            labels.append('O')\n",
    "            if len(labels)==0:\n",
    "                labels = ['O','O','O','O']\n",
    "            tokenGravar = token[0].replace(' ','')\n",
    "            tokenGravar = tokenGravar.strip()\n",
    "            f_entidade.write(tokenGravar+'\\t'+'\\t'.join(labels)+'\\n')\n",
    "            #print(tokenGravar+'\\t'+'\\t'.join(labels))\n",
    "\n",
    "        \n",
    "        f_entidade.write('\\n')\n",
    "\n",
    "    f_entidade.close()\n",
    "\n",
    "    print('num_entidade_total:', num_entidade_total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gravando arquivo multilabel de train\n",
      "num_entidade_total: 4830\n",
      "\n",
      "Gravando arquivo multilabel de dev\n",
      "num_entidade_total: 1226\n",
      "\n",
      "Gravando arquivo multilabel de test\n",
      "num_entidade_total: 1803\n"
     ]
    }
   ],
   "source": [
    "gravarArquivosMultilabel(dic_sentencesTrain, 'train')\n",
    "gravarArquivosMultilabel(dic_sentencesDev, 'dev')\n",
    "gravarArquivosMultilabel(dic_sentencesTest, 'test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parte 2- Gerar arquivo treinamento para SpanClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def areConsecutive(arr):\n",
    "    # Sort the array\n",
    "    arr.sort()\n",
    "    n = len(arr)\n",
    "    # checking the adjacent elements\n",
    "    for i in range (1,n):\n",
    "        if(arr[i]!=arr[i-1]+1):\n",
    "            return False;\n",
    "             \n",
    "    return True;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_sentencesTest = load_obj('dic_sentencesTest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCombinacaoEntidades(dic_predictions, filtro_postagger, dicPosTagger, taxaDownsampling):\n",
    "    num=0\n",
    "    erro_corpus=0\n",
    "    num_frases_sem_entidade=0\n",
    "    lista_erro_corpus=list()\n",
    "    combinacaoEntidadesAll = list()\n",
    "    combinacaoEntidades = list()\n",
    "    pulando_termos_postagger = list()\n",
    "    if filtro_postagger:\n",
    "        print('Com filtro-postagger')\n",
    "    else:\n",
    "        print('Sem filtro-postagger')\n",
    "    if taxaDownsampling>0:\n",
    "        print('Com taxa de Downsampling de ', taxaDownsampling)\n",
    "    else:\n",
    "        print('Sem taxa de Downsampling')\n",
    "    for key, value in dic_predictions.items():\n",
    "        num=num+1\n",
    "        combinacaoEntidades = list()\n",
    "        tokens=value[0].copy()\n",
    "        so_tokens = [t[0] for t in tokens]\n",
    "        entidades=value[1].copy()\n",
    "        num_positivas=0\n",
    "        for entidade in entidades:\n",
    "            erros_entidade = list()\n",
    "            texto_entidade=entidade[0].strip()\n",
    "            indices = entidade[1]\n",
    "            tipo_entidade = entidade[2]\n",
    "            frase = so_tokens.copy()\n",
    "            inicio=indices[0]\n",
    "            fim=indices[-1]\n",
    "            frase.insert(inicio, '<e1>')\n",
    "            frase.insert(fim+2, '</e1>')\n",
    "            if texto_entidade=='-' or texto_entidade=='=' or texto_entidade=='+' or texto_entidade==':' or texto_entidade==',' or texto_entidade==\"'\" or texto_entidade=='\"' or texto_entidade=='.' or texto_entidade==';' or texto_entidade=='/' or texto_entidade=='(' or texto_entidade==')' or texto_entidade=='[' or texto_entidade==']':\n",
    "                pass\n",
    "            texto_entidade_comparar=texto_entidade.replace('/','').replace(')','').replace('(','').replace(']','').replace('[','').replace(',','').replace('.','').replace(';','').replace('-','').replace('+','').replace(\"'\",'')\n",
    "            texto_entidade_comparar = replaceWhiteSpaces(texto_entidade_comparar)\n",
    "            texto_frase_comparar = ' '.join(frase[inicio+1:fim+2]).strip().replace('/','').replace(')','').replace('(','').replace(']','').replace('[','').replace(',','').replace('.','').replace(';','').replace('-','').replace('+','').replace(\"'\",'')\n",
    "            texto_frase_comparar = replaceWhiteSpaces(texto_frase_comparar)\n",
    "            texto_entidade_comparar = texto_entidade_comparar.lower()\n",
    "            texto_frase_comparar = texto_frase_comparar.lower()\n",
    "            if (texto_entidade_comparar == texto_frase_comparar):\n",
    "                num_positivas=num_positivas+1\n",
    "                combinacaoEntidades.append([' '.join(frase).strip(), tipo_entidade]) # apendando entidades reais\n",
    "            else:\n",
    "                print('erro, key:', key)\n",
    "                erro_corpus=erro_corpus+1\n",
    "                erros_entidade.append(indices)\n",
    "                lista_erro_corpus.append([' '.join(frase).strip(), tipo_entidade, ' '.join(so_tokens), entidade])\n",
    "\n",
    "        for entidade in entidades:\n",
    "                indices = entidade[1]\n",
    "                #print('indices:', indices)\n",
    "                if indices in erros_entidade:\n",
    "                    continue\n",
    "                inicio=indices[0]\n",
    "                fim=indices[-1]\n",
    "                # agora, fazer a combinacao entre eles.. todas a seguir serão do tipo 'O'           \n",
    "                for indice in indices:\n",
    "                    for i in range(indice, fim+1):\n",
    "                        # ver se nao tem antes\n",
    "                        frase = so_tokens.copy()\n",
    "                        #termo = frase[indice:i+2]\n",
    "                        termo = frase[indice:i+1]\n",
    "                        #print('--termo--:', termo)\n",
    "                        frase.insert(indice, '<e1>')\n",
    "                        frase.insert(i+2, '</e1>')\n",
    "                        frase_string=' '.join(frase).strip()\n",
    "                        #print('frase_string:', frase_string)\n",
    "                        devePular = 0\n",
    "                        if '. </e1>' in frase_string or ', </e1>' in frase_string  or '; </e1>' in frase_string or '- </e1>' in frase_string  or ': </e1>' in frase_string  or '= </e1>' in frase_string  or '/ </e1>' in frase_string  or '( </e1>' in frase_string  or ') </e1>' in frase_string  or '[ </e1>' in frase_string  or '] </e1>' in frase_string  or ': </e1>' in frase_string or 'and </e1>' in frase_string or 'or </e1>' in frase_string:\n",
    "                            devePular=1\n",
    "                        if '<e1> .' in frase_string or '<e1> ,' in frase_string  or '<e1> ;' in frase_string or '<e1> -' in frase_string  or '<e1> :' in frase_string  or '<e1> =' in frase_string  or '<e1> /' in frase_string  or '<e1> (' in frase_string  or '<e1> )' in frase_string  or '<e1> [' in frase_string  or '<e1> ]' in frase_string  or '<e1> :' in frase_string  or '<e1> and' in frase_string  or '<e1> or' in frase_string:\n",
    "                            devePular=1\n",
    "                        if re.search(\"<e1> [0-9]* </e1>\", frase_string):\n",
    "                            devePular=1\n",
    "                        if filtro_postagger==True:\n",
    "                            pos_tagger_termo = tipoPostaggerTokens(termo, dicPosTagger)\n",
    "                            if pos_tagger_termo not in lista_postaggers_entidades:\n",
    "                                pulando_termos_postagger.append([termo, pos_tagger_termo])\n",
    "                                devePular=1\n",
    "                \n",
    "                        tem_frase = 0\n",
    "                        for frase in combinacaoEntidades:\n",
    "                            if frase[0] == frase_string:\n",
    "                                tem_frase=''\n",
    "                                break\n",
    "                        if tem_frase==0 and devePular==0:\n",
    "                            combinacaoEntidades.append([frase_string, 'O'])\n",
    "        # shuffle no combinacaoEntidades\n",
    "        # taxaDownsampling, ex 2 para o dobro, 1 para mesma quantidade\n",
    "        if len(combinacaoEntidades)>0:\n",
    "            if taxaDownsampling>0:\n",
    "                combinacaoEntidades = combinacaoEntidades[:(num_positivas*taxaDownsampling)+num_positivas]\n",
    "            random.shuffle(combinacaoEntidades)\n",
    "            combinacaoEntidadesAll.append([' '.join(so_tokens).strip(), combinacaoEntidades])\n",
    "        else:\n",
    "            num_frases_sem_entidade = num_frases_sem_entidade+1\n",
    "            combinacaoEntidadesAll.append([])\n",
    "        combinacaoEntidades = list()\n",
    "        if (num % 1000) ==0:\n",
    "            print('key:', key)\n",
    "\n",
    "    print('erro_corpus:', erro_corpus)\n",
    "    print('num_frases_sem_entidade:', num_frases_sem_entidade)\n",
    "    print('len(combinacaoEntidadesAll:)',len(combinacaoEntidadesAll))\n",
    "    print('len(pulando_termos_postagger):', len(pulando_termos_postagger))\n",
    "    \n",
    "    return combinacaoEntidadesAll, pulando_termos_postagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels sempre tem que começar com zero, senao da erro no treinamento\n",
    "# RuntimeError: CUDA error: device-side assert triggered\n",
    "def getCombinacaoEntidadesSoPositivos(dic_predictions):\n",
    "    num=0\n",
    "    erro_corpus=0\n",
    "    num_frases_sem_entidade=0\n",
    "    lista_erro_corpus=list()\n",
    "    combinacaoEntidadesAll = list()\n",
    "    combinacaoEntidades = list()\n",
    "    print('Só positivos')\n",
    "    for key, value in dic_predictions.items():\n",
    "        num=num+1\n",
    "        combinacaoEntidades = list()\n",
    "        tokens=value[0].copy()\n",
    "        so_tokens = [t[0] for t in tokens]\n",
    "        entidades=value[1].copy()\n",
    "        num_positivas=0\n",
    "        for entidade in entidades:\n",
    "            erros_entidade = list()\n",
    "            texto_entidade=entidade[0].strip()\n",
    "            indices = entidade[1]\n",
    "            tipo_entidade = entidade[2]\n",
    "            #print('tipo_entidade:', tipo_entidade)\n",
    "            frase = so_tokens.copy()\n",
    "            inicio=indices[0]\n",
    "            fim=indices[-1]\n",
    "            frase.insert(inicio, '<e1>')\n",
    "            frase.insert(fim+2, '</e1>')\n",
    "            if texto_entidade=='-' or texto_entidade=='=' or texto_entidade=='+' or texto_entidade==':' or texto_entidade==',' or texto_entidade==\"'\" or texto_entidade=='\"' or texto_entidade=='.' or texto_entidade==';' or texto_entidade=='/' or texto_entidade=='(' or texto_entidade==')' or texto_entidade=='[' or texto_entidade==']':\n",
    "                pass\n",
    "            texto_entidade_comparar=texto_entidade.replace('/','').replace(')','').replace('(','').replace(']','').replace('[','').replace(',','').replace('.','').replace(';','').replace('-','').replace('+','').replace(\"'\",'')\n",
    "            texto_entidade_comparar = replaceWhiteSpaces(texto_entidade_comparar)\n",
    "            texto_frase_comparar = ' '.join(frase[inicio+1:fim+2]).strip().replace('/','').replace(')','').replace('(','').replace(']','').replace('[','').replace(',','').replace('.','').replace(';','').replace('-','').replace('+','').replace(\"'\",'')\n",
    "            texto_frase_comparar = replaceWhiteSpaces(texto_frase_comparar)\n",
    "            texto_entidade_comparar = texto_entidade_comparar.lower()\n",
    "            texto_frase_comparar = texto_frase_comparar.lower()\n",
    "            if (texto_entidade_comparar == texto_frase_comparar):\n",
    "                num_positivas=num_positivas+1\n",
    "                combinacaoEntidades.append([' '.join(frase).strip(), tipo_entidade]) # apendando entidades reais\n",
    "            else:\n",
    "                print('erro, key:', key)\n",
    "                erro_corpus=erro_corpus+1\n",
    "                erros_entidade.append(indices)\n",
    "                lista_erro_corpus.append([' '.join(frase).strip(), tipo_entidade, ' '.join(so_tokens), entidade])\n",
    "\n",
    "        # shuffle no combinacaoEntidades\n",
    "        if len(combinacaoEntidades)>0:\n",
    "            random.shuffle(combinacaoEntidades)\n",
    "            combinacaoEntidadesAll.append([' '.join(so_tokens).strip(), combinacaoEntidades])\n",
    "        else:\n",
    "            num_frases_sem_entidade = num_frases_sem_entidade+1\n",
    "            combinacaoEntidadesAll.append([])\n",
    "        combinacaoEntidades = list()\n",
    "  \n",
    "    print('erro_corpus:', erro_corpus)\n",
    "    print('num_frases_sem_entidade:', num_frases_sem_entidade)\n",
    "    print('len(combinacaoEntidadesAll:)',len(combinacaoEntidadesAll))\n",
    "    \n",
    "    return combinacaoEntidadesAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Train--\n",
      "Só positivos\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 237\n",
      "len(combinacaoEntidadesAll:) 1319\n",
      "\n",
      "--Dev--\n",
      "Só positivos\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 89\n",
      "len(combinacaoEntidadesAll:) 416\n",
      "\n",
      "--Test--\n",
      "Só positivos\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 107\n",
      "len(combinacaoEntidadesAll:) 506\n"
     ]
    }
   ],
   "source": [
    "print('--Train--')\n",
    "combinacaoEntidadesTrainPos= getCombinacaoEntidadesSoPositivos(dic_sentencesTrain)\n",
    "print('\\n--Dev--')\n",
    "combinacaoEntidadesDevPos = getCombinacaoEntidadesSoPositivos(dic_sentencesDev)\n",
    "print('\\n--Test--')\n",
    "combinacaoEntidadesTestPos = getCombinacaoEntidadesSoPositivos(dic_sentencesTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Train--\n",
      "Sem filtro-postagger\n",
      "Sem taxa de Downsampling\n",
      "key: 999\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 237\n",
      "len(combinacaoEntidadesAll:) 1319\n",
      "len(pulando_termos_postagger): 0\n",
      "\n",
      "--Dev--\n",
      "Sem filtro-postagger\n",
      "Sem taxa de Downsampling\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 89\n",
      "len(combinacaoEntidadesAll:) 416\n",
      "len(pulando_termos_postagger): 0\n",
      "\n",
      "--Test--\n",
      "Sem filtro-postagger\n",
      "Sem taxa de Downsampling\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 107\n",
      "len(combinacaoEntidadesAll:) 506\n",
      "len(pulando_termos_postagger): 0\n"
     ]
    }
   ],
   "source": [
    "print('--Train--')\n",
    "combinacaoEntidadesTrain, _ = getCombinacaoEntidades(dic_sentencesTrain, False, '', 0)\n",
    "print('\\n--Dev--')\n",
    "combinacaoEntidadesDev, _ = getCombinacaoEntidades(dic_sentencesDev, False, '', 0)\n",
    "print('\\n--Test--')\n",
    "combinacaoEntidadesTest, _ = getCombinacaoEntidades(dic_sentencesTest, False, '', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aumento moderado de átrio esquerdo .',\n",
       " [['aumento <e1> moderado de átrio </e1> esquerdo .', 'O'],\n",
       "  ['aumento moderado <e1> de </e1> átrio esquerdo .', 'O'],\n",
       "  ['<e1> aumento moderado de átrio esquerdo </e1> .', 'Problema'],\n",
       "  ['<e1> aumento moderado de </e1> átrio esquerdo .', 'O'],\n",
       "  ['aumento moderado <e1> de átrio esquerdo </e1> .', 'O'],\n",
       "  ['aumento <e1> moderado de </e1> átrio esquerdo .', 'O'],\n",
       "  ['<e1> aumento moderado </e1> de átrio esquerdo .', 'O'],\n",
       "  ['aumento moderado de átrio <e1> esquerdo </e1> .', 'O'],\n",
       "  ['aumento moderado de <e1> átrio esquerdo </e1> .', 'Anatomia'],\n",
       "  ['<e1> aumento moderado de átrio </e1> esquerdo .', 'O'],\n",
       "  ['aumento <e1> moderado </e1> de átrio esquerdo .', 'O'],\n",
       "  ['<e1> aumento </e1> moderado de átrio esquerdo .', 'O'],\n",
       "  ['aumento <e1> moderado de átrio esquerdo </e1> .', 'O'],\n",
       "  ['aumento moderado <e1> de átrio </e1> esquerdo .', 'O'],\n",
       "  ['aumento moderado de <e1> átrio </e1> esquerdo .', 'O']]]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinacaoEntidadesTest[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['CONTRAÇÃO', 0, 507],\n",
       "  ['SEGMENTAR', 1, 517],\n",
       "  ['DO', 2, 527],\n",
       "  ['VE', 3, 530],\n",
       "  ['ALTERADA', 4, 533],\n",
       "  ['.', 5, 541]],\n",
       " [['CONTRAÇÃO SEGMENTAR DO VE ALTERADA', [0, 1, 2, 3, 4], 'Problema'],\n",
       "  ['VE', [3], 'Anatomia']]]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# o erro é que o ponto faz com que quebre a frase, mas a entidade continua...\n",
    "dic_sentencesTrain[829]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DSLP em uso de sinvastatina , marevan 1 cp / dia seg - sab para no alvo sic .',\n",
       " [['DSLP em uso de <e1> sinvastatina </e1> , marevan 1 cp / dia seg - sab para no alvo sic .',\n",
       "   'Tratamento'],\n",
       "  ['DSLP em uso de sinvastatina , <e1> marevan </e1> 1 cp / dia seg - sab para no alvo sic .',\n",
       "   'Tratamento'],\n",
       "  ['<e1> DSLP </e1> em uso de sinvastatina , marevan 1 cp / dia seg - sab para no alvo sic .',\n",
       "   'Problema']]]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinacaoEntidadesTest[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CONTRAÇÃO SEGMENTAR DO VE ALTERADA .',\n",
       " [['CONTRAÇÃO SEGMENTAR <e1> DO </e1> VE ALTERADA .', 'O'],\n",
       "  ['CONTRAÇÃO SEGMENTAR DO VE <e1> ALTERADA </e1> .', 'O'],\n",
       "  ['<e1> CONTRAÇÃO </e1> SEGMENTAR DO VE ALTERADA .', 'O'],\n",
       "  ['CONTRAÇÃO <e1> SEGMENTAR DO VE </e1> ALTERADA .', 'O'],\n",
       "  ['CONTRAÇÃO SEGMENTAR DO <e1> VE ALTERADA </e1> .', 'O'],\n",
       "  ['<e1> CONTRAÇÃO SEGMENTAR DO VE ALTERADA </e1> .', 'Problema'],\n",
       "  ['CONTRAÇÃO <e1> SEGMENTAR DO </e1> VE ALTERADA .', 'O'],\n",
       "  ['<e1> CONTRAÇÃO SEGMENTAR DO </e1> VE ALTERADA .', 'O'],\n",
       "  ['<e1> CONTRAÇÃO SEGMENTAR </e1> DO VE ALTERADA .', 'O'],\n",
       "  ['CONTRAÇÃO SEGMENTAR <e1> DO VE </e1> ALTERADA .', 'O'],\n",
       "  ['CONTRAÇÃO <e1> SEGMENTAR DO VE ALTERADA </e1> .', 'O'],\n",
       "  ['CONTRAÇÃO SEGMENTAR DO <e1> VE </e1> ALTERADA .', 'Anatomia'],\n",
       "  ['CONTRAÇÃO <e1> SEGMENTAR </e1> DO VE ALTERADA .', 'O'],\n",
       "  ['<e1> CONTRAÇÃO SEGMENTAR DO VE </e1> ALTERADA .', 'O'],\n",
       "  ['CONTRAÇÃO SEGMENTAR <e1> DO VE ALTERADA </e1> .', 'O']]]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinacaoEntidadesTrain[829]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HAS , ICC , nega DM .',\n",
       " [['HAS , <e1> ICC </e1> , nega DM .', 'Problema'],\n",
       "  ['<e1> HAS </e1> , ICC , nega DM .', 'Problema'],\n",
       "  ['HAS , ICC , nega <e1> DM </e1> .', 'Problema']]]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinacaoEntidadesDev[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gravarArquivosTreinamento(path, combinacaoEntidadesTrain, combinacaoEntidadesDev, combinacaoEntidadesTest):\n",
    "\n",
    "    # já ir gravando arquivos treinamento, test e dev...\n",
    "    # pra fazer um teste sem descontinuas\n",
    "    numTotalEntidades=0\n",
    "    numTotalEntidadesTrain=0\n",
    "    numTotalEntidadesDev=0\n",
    "    numTotalEntidadesTest=0\n",
    "\n",
    "    existeDir = os.path.exists(path)\n",
    "    if not existeDir:\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    f_train = open(path+r'\\span.train', 'w', encoding='utf-8')\n",
    "\n",
    "    for i, combinacaoEntidades in enumerate(combinacaoEntidadesTrain):\n",
    "        if len(combinacaoEntidades)>0:\n",
    "            frase = combinacaoEntidades[0]\n",
    "            frases_entidade = combinacaoEntidades[1]\n",
    "            f_train.write(frase+'\\n')\n",
    "            for frase_entidade in frases_entidade:\n",
    "                f_train.write(frase_entidade[1]+'\\t'+frase_entidade[0]+'\\n')\n",
    "                numTotalEntidades=numTotalEntidades+1\n",
    "                numTotalEntidadesTrain=numTotalEntidadesTrain+1\n",
    "\n",
    "    f_train.close()\n",
    "\n",
    "    f_dev = open(path+r'\\span.dev', 'w', encoding='utf-8')\n",
    "\n",
    "    for i, combinacaoEntidades in enumerate(combinacaoEntidadesDev):\n",
    "        if len(combinacaoEntidades)>0:\n",
    "            frase = combinacaoEntidades[0]\n",
    "            frases_entidade = combinacaoEntidades[1]\n",
    "            f_dev.write(frase+'\\n')\n",
    "            for frase_entidade in frases_entidade:\n",
    "                f_dev.write(frase_entidade[1]+'\\t'+frase_entidade[0]+'\\n')\n",
    "                numTotalEntidades=numTotalEntidades+1\n",
    "                numTotalEntidadesDev=numTotalEntidadesDev+1\n",
    "\n",
    "    f_dev.close()\n",
    "\n",
    "    f_test = open(path+r'\\span.test', 'w', encoding='utf-8')\n",
    "    for i, combinacaoEntidades in enumerate(combinacaoEntidadesTest):\n",
    "        #print(dicSentences[i])\n",
    "        if len(combinacaoEntidades)>0:\n",
    "            frase = combinacaoEntidades[0]\n",
    "            frases_entidade = combinacaoEntidades[1]\n",
    "            f_test.write(frase+'\\n')\n",
    "            for frase_entidade in frases_entidade:\n",
    "                f_test.write(frase_entidade[1]+'\\t'+frase_entidade[0]+'\\n')\n",
    "                numTotalEntidades=numTotalEntidades+1\n",
    "                numTotalEntidadesTest=numTotalEntidadesTest+1\n",
    "\n",
    "    f_test.close()\n",
    "\n",
    "    print('numTotalEntidades:', numTotalEntidades)\n",
    "    print('numTotalEntidadesTrain:', numTotalEntidadesTrain)\n",
    "    print('numTotalEntidadesDev:', numTotalEntidadesDev)\n",
    "    print('numTotalEntidadesTest:', numTotalEntidadesTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numTotalEntidades: 14133\n",
      "numTotalEntidadesTrain: 9079\n",
      "numTotalEntidadesDev: 2001\n",
      "numTotalEntidadesTest: 3053\n"
     ]
    }
   ],
   "source": [
    "gravarArquivosTreinamento('sem_filtro',combinacaoEntidadesTrain, combinacaoEntidadesDev, combinacaoEntidadesTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numTotalEntidades: 4204\n",
      "numTotalEntidadesTrain: 2510\n",
      "numTotalEntidadesDev: 703\n",
      "numTotalEntidadesTest: 991\n"
     ]
    }
   ],
   "source": [
    "gravarArquivosTreinamento('so_positivos',combinacaoEntidadesTrainPos, combinacaoEntidadesDevPos, combinacaoEntidadesTestPos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gravar arquivo para sentence pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCombinacaoEntidadesSentence(dic_predictions):\n",
    "    #labels = {0:'O', 1:'Problema', 2:'Tratamento', 3:'Teste', 4:'Anatomia'}\n",
    "    labels = {'O':0, 'Problema':1, 'Tratamento':2, 'Teste':3, 'Anatomia':4}\n",
    "    num=0\n",
    "    erro_corpus=0\n",
    "    num_frases_sem_entidade=0\n",
    "    lista_erro_corpus=list()\n",
    "    combinacaoEntidades = list()\n",
    "    print('Sentence Pairs - Só positivos')\n",
    "    for key, value in dic_predictions.items():\n",
    "        num=num+1\n",
    "        tokens=value[0].copy()\n",
    "        so_tokens = [t[0] for t in tokens]\n",
    "        entidades=value[1].copy()\n",
    "        num_positivas=0\n",
    "        for entidade in entidades:\n",
    "            erros_entidade = list()\n",
    "            texto_entidade=entidade[0].strip()\n",
    "            indices = entidade[1]\n",
    "            tipo_entidade = entidade[2]\n",
    "            frase = so_tokens.copy()\n",
    "            inicio=indices[0]\n",
    "            fim=indices[-1]\n",
    "            #entidade_frase=frase[inicio:fim+1] # texto_entidade\n",
    "            entidade_frase=texto_entidade\n",
    "            #print('entidade_frase:', entidade_frase)\n",
    "            #print('frase:', frase)\n",
    "            #print('texto_entidade:', texto_entidade)\n",
    "            if texto_entidade=='-' or texto_entidade=='=' or texto_entidade=='+' or texto_entidade==':' or texto_entidade==',' or texto_entidade==\"'\" or texto_entidade=='\"' or texto_entidade=='.' or texto_entidade==';' or texto_entidade=='/' or texto_entidade=='(' or texto_entidade==')' or texto_entidade=='[' or texto_entidade==']':\n",
    "                pass\n",
    "            texto_entidade_comparar=texto_entidade.replace('/','').replace(')','').replace('(','').replace(']','').replace('[','').replace(',','').replace('.','').replace(';','').replace('-','').replace('+','').replace(\"'\",'')\n",
    "            texto_entidade_comparar = replaceWhiteSpaces(texto_entidade_comparar)\n",
    "            texto_frase_comparar = ' '.join(frase[inicio:fim+1]).strip().replace('/','').replace(')','').replace('(','').replace(']','').replace('[','').replace(',','').replace('.','').replace(';','').replace('-','').replace('+','').replace(\"'\",'')\n",
    "            texto_frase_comparar = replaceWhiteSpaces(texto_frase_comparar)\n",
    "            texto_entidade_comparar = texto_entidade_comparar.lower()\n",
    "            texto_frase_comparar = texto_frase_comparar.lower()\n",
    "            #print('texto_entidade_comparar:', texto_entidade_comparar)\n",
    "            #print('texto_frase_comparar:', texto_frase_comparar)\n",
    "            if (texto_entidade_comparar == texto_frase_comparar):\n",
    "                num_positivas=num_positivas+1\n",
    "                combinacaoEntidades.append([entidade_frase, ' '.join(frase).strip(), labels[tipo_entidade]]) # apendando entidades reais\n",
    "            else:\n",
    "                print('erro, key:', key)\n",
    "                erro_corpus=erro_corpus+1\n",
    "                erros_entidade.append(indices)\n",
    "                lista_erro_corpus.append([' '.join(frase).strip(), tipo_entidade, ' '.join(so_tokens), entidade])\n",
    "\n",
    "        # shuffle no combinacaoEntidades\n",
    "        #if len(combinacaoEntidades)>0:\n",
    "        #    random.shuffle(combinacaoEntidades)\n",
    "        #    combinacaoEntidadesAll.append(combinacaoEntidades)\n",
    "        #else:\n",
    "        #    num_frases_sem_entidade = num_frases_sem_entidade+1\n",
    "            #combinacaoEntidadesAll.append([])\n",
    "        #combinacaoEntidades = list()\n",
    "        \n",
    "    random.shuffle(combinacaoEntidades)\n",
    "  \n",
    "    print('erro_corpus:', erro_corpus)\n",
    "    print('num_frases_sem_entidade:', num_frases_sem_entidade)\n",
    "    print('len(combinacaoEntidades:)',len(combinacaoEntidades))\n",
    "    \n",
    "    return combinacaoEntidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 4), match='12mg'>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(\"^[0-9]*mg\", '12mg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCombinacaoEntidadesSentence(dic_predictions, filtro_postagger, dicPosTagger, taxaDownsampling):\n",
    "    #labels = {0:'O', 1:'Problema', 2:'Tratamento', 3:'Teste', 4:'Anatomia'}\n",
    "    labels = {'O':0, 'Problema':1, 'Tratamento':2, 'Teste':3, 'Anatomia':4}\n",
    "    num=0\n",
    "    erro_corpus=0\n",
    "    num_frases_sem_entidade=0\n",
    "    lista_erro_corpus=list()\n",
    "    combinacaoEntidadesPos = list()\n",
    "    combinacaoEntidadesNeg = list()\n",
    "    combinacaoEntidades = list()\n",
    "    pulando_termos_postagger = list()\n",
    "    if filtro_postagger:\n",
    "        print('Sentence Pairs - Com filtro-postagger')\n",
    "    else:\n",
    "        print('Sentence Pairs - Sem filtro-postagger')\n",
    "    if taxaDownsampling>0:\n",
    "        print('Sentence Pairs - Com taxa de Downsampling de ', taxaDownsampling)\n",
    "    else:\n",
    "        print('Sentence Pairs - Sem taxa de Downsampling')\n",
    "\n",
    "    for key, value in dic_predictions.items():\n",
    "        num=num+1\n",
    "        tokens=value[0].copy()\n",
    "        so_tokens = [t[0] for t in tokens]\n",
    "        entidades=value[1].copy()\n",
    "        num_positivas=0\n",
    "        for entidade in entidades:\n",
    "            erros_entidade = list()\n",
    "            texto_entidade=entidade[0].strip()\n",
    "            indices = entidade[1]\n",
    "            tipo_entidade = entidade[2]\n",
    "            frase = so_tokens.copy()\n",
    "            inicio=indices[0]\n",
    "            fim=indices[-1]\n",
    "            #entidade_frase=frase[inicio:fim+1] # texto_entidade\n",
    "            entidade_frase=texto_entidade\n",
    "            #print('entidade_frase:', entidade_frase)\n",
    "            #print('frase:', frase)\n",
    "            #print('texto_entidade:', texto_entidade)\n",
    "            if texto_entidade=='-' or texto_entidade=='=' or texto_entidade=='+' or texto_entidade==':' or texto_entidade==',' or texto_entidade==\"'\" or texto_entidade=='\"' or texto_entidade=='.' or texto_entidade==';' or texto_entidade=='/' or texto_entidade=='(' or texto_entidade==')' or texto_entidade=='[' or texto_entidade==']':\n",
    "                pass\n",
    "            texto_entidade_comparar=texto_entidade.replace('/','').replace(')','').replace('(','').replace(']','').replace('[','').replace(',','').replace('.','').replace(';','').replace('-','').replace('+','').replace(\"'\",'')\n",
    "            texto_entidade_comparar = replaceWhiteSpaces(texto_entidade_comparar)\n",
    "            texto_frase_comparar = ' '.join(frase[inicio:fim+1]).strip().replace('/','').replace(')','').replace('(','').replace(']','').replace('[','').replace(',','').replace('.','').replace(';','').replace('-','').replace('+','').replace(\"'\",'')\n",
    "            texto_frase_comparar = replaceWhiteSpaces(texto_frase_comparar)\n",
    "            texto_entidade_comparar = texto_entidade_comparar.lower()\n",
    "            texto_frase_comparar = texto_frase_comparar.lower()\n",
    "            #print('texto_entidade_comparar:', texto_entidade_comparar)\n",
    "            #print('texto_frase_comparar:', texto_frase_comparar)\n",
    "            if (texto_entidade_comparar == texto_frase_comparar):\n",
    "                num_positivas=num_positivas+1\n",
    "                combinacaoEntidadesPos.append([entidade_frase, ' '.join(frase).strip(), labels[tipo_entidade]]) # apendando entidades reais\n",
    "            else:\n",
    "                print('erro, key:', key)\n",
    "                erro_corpus=erro_corpus+1\n",
    "                erros_entidade.append(indices)\n",
    "                lista_erro_corpus.append([' '.join(frase).strip(), tipo_entidade, ' '.join(so_tokens), entidade])\n",
    "        # agora, os negativos\n",
    "        for entidade in entidades:\n",
    "                indices = entidade[1]\n",
    "                #print('indices:', indices)\n",
    "                if indices in erros_entidade:\n",
    "                    continue\n",
    "                inicio=indices[0]\n",
    "                fim=indices[-1]\n",
    "                # agora, fazer a combinacao entre eles.. todas a seguir serão do tipo 'O'           \n",
    "                for indice in indices:\n",
    "                    for i in range(indice, fim+1):\n",
    "                        # ver se nao tem antes\n",
    "                        frase = so_tokens.copy()\n",
    "                        termo = frase[indice:i+1]\n",
    "                        #termo=entidade[0].strip()\n",
    "                        #print('--termo--:', termo)\n",
    "                        #frase.insert(indice, '<e1>')\n",
    "                        #frase.insert(i+2, '</e1>')\n",
    "                        #frase_string=' '.join(frase).strip()\n",
    "                        #frase_string=texto_entidade\n",
    "                        #frase_string=termo[0]\n",
    "                        frase_string=' '.join(termo).strip()\n",
    "                        #print('frase_string:', frase_string)\n",
    "                        devePular = 0\n",
    "                        if '.' in frase_string[-1:] or ',' in frase_string[-1:]  or ';' in frase_string[-1:] or '-' in frase_string[-1:]  or ':' in frase_string[-1:]  or '=' in frase_string[-1:]  or '/' in frase_string[-1:]  or '(' in frase_string[-1:]  or ')' in frase_string[-1:]  or '[' in frase_string[-1:]  or ']' in frase_string[-1:]  or ':' in frase_string[-1:]:\n",
    "                            devePular=1\n",
    "                        if '.' in frase_string[:1] or ',' in frase_string[:1]  or ';' in frase_string[:1] or '-' in frase_string[:1]  or ':' in frase_string[:1]  or '=' in frase_string[:1] or '/' in frase_string[:1]  or '(' in frase_string[:1]  or ')' in frase_string[:1] or '[' in frase_string[:1]  or ']' in frase_string[:1]  or ':' in frase_string[:1]:\n",
    "                            devePular=1\n",
    "                        if re.search(\"^[0-9]*mg\", frase_string):\n",
    "                            devePular=1\n",
    "                            \n",
    "                        if filtro_postagger==True:\n",
    "                            pos_tagger_termo = tipoPostaggerTokens(termo, dicPosTagger)\n",
    "                            if pos_tagger_termo not in lista_postaggers_entidades:\n",
    "                                pulando_termos_postagger.append([termo, pos_tagger_termo])\n",
    "                                devePular=1\n",
    "                \n",
    "                        tem_frase = 0\n",
    "                        for frase_l in combinacaoEntidadesPos:\n",
    "                            if frase_l[0] == frase_string:\n",
    "                                tem_frase='1'\n",
    "                                break\n",
    "                        if tem_frase==0 and devePular==0:\n",
    "                        #print('tem_frase:', tem_frase)\n",
    "                        #if tem_frase==0:\n",
    "                            #print('aaaaaaaaaaaa, frase_string:', frase_string)\n",
    "                            combinacaoEntidadesNeg.append([frase_string, ' '.join(frase).strip(), labels['O']])\n",
    "                        #combinacaoEntidadesNeg.append([frase_string, ' '.join(frase).strip(), labels['O']])\n",
    "                        \n",
    "        # shuffle no combinacaoEntidades\n",
    "        #if len(combinacaoEntidades)>0:\n",
    "        #    random.shuffle(combinacaoEntidades)\n",
    "        #    combinacaoEntidadesAll.append(combinacaoEntidades)\n",
    "        #else:\n",
    "        #    num_frases_sem_entidade = num_frases_sem_entidade+1\n",
    "            #combinacaoEntidadesAll.append([])\n",
    "        #combinacaoEntidades = list()\n",
    "        \n",
    "        # shuffle no combinacaoEntidades\n",
    "        # taxaDownsampling, ex 2 para o dobro, 1 para mesma quantidade\n",
    "        if len(combinacaoEntidadesPos)>0:\n",
    "            if taxaDownsampling>0:\n",
    "                combinacaoEntidadesNeg = combinacaoEntidadesNeg[:(num_positivas*taxaDownsampling)]\n",
    "            random.shuffle(combinacaoEntidadesNeg)\n",
    "        else:\n",
    "            num_frases_sem_entidade = num_frases_sem_entidade+1\n",
    "        if (num % 1000) ==0:\n",
    "            print('key:', key)\n",
    "\n",
    "        #print('combinacaoEntidadesNeg:',combinacaoEntidadesNeg)\n",
    "        combinacaoEntidades = combinacaoEntidades+combinacaoEntidadesPos+combinacaoEntidadesNeg\n",
    "        combinacaoEntidadesPos=list()\n",
    "        combinacaoEntidadesNeg=list()\n",
    "  \n",
    "    print('erro_corpus:', erro_corpus)\n",
    "    print('num_frases_sem_entidade:', num_frases_sem_entidade)\n",
    "    print('len(combinacaoEntidades:)',len(combinacaoEntidades))\n",
    "    \n",
    "    return combinacaoEntidades\n",
    "\n",
    "#combinacaoEntidadesTestSentence = getCombinacaoEntidadesSentence(dic_sentencesTest, True, dicPosTagger, 1)\n",
    "#combinacaoEntidadesTrain, pulando_termos_postaggerTrain = getCombinacaoEntidades(dic_sentencesTrain, True, dicPosTagger, 0)\n",
    "#combinacaoEntidadesTestSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCombinacaoEntidadesSentencePos(dic_predictions):\n",
    "    labels = {'Problema':0, 'Tratamento':1, 'Teste':2, 'Anatomia':3}\n",
    "    num=0\n",
    "    erro_corpus=0\n",
    "    num_frases_sem_entidade=0\n",
    "    lista_erro_corpus=list()\n",
    "    combinacaoEntidadesPos = list()\n",
    "    combinacaoEntidades = list()\n",
    "    pulando_termos_postagger = list()\n",
    "    print('Sentence Pairs - So positivos')\n",
    "\n",
    "    for key, value in dic_predictions.items():\n",
    "        num=num+1\n",
    "        tokens=value[0].copy()\n",
    "        so_tokens = [t[0] for t in tokens]\n",
    "        entidades=value[1].copy()\n",
    "        num_positivas=0\n",
    "        for entidade in entidades:\n",
    "            erros_entidade = list()\n",
    "            texto_entidade=entidade[0].strip()\n",
    "            indices = entidade[1]\n",
    "            tipo_entidade = entidade[2]\n",
    "            frase = so_tokens.copy()\n",
    "            inicio=indices[0]\n",
    "            fim=indices[-1]\n",
    "            #entidade_frase=frase[inicio:fim+1] # texto_entidade\n",
    "            entidade_frase=texto_entidade\n",
    "            #print('entidade_frase:', entidade_frase)\n",
    "            #print('frase:', frase)\n",
    "            #print('texto_entidade:', texto_entidade)\n",
    "            if texto_entidade=='-' or texto_entidade=='=' or texto_entidade=='+' or texto_entidade==':' or texto_entidade==',' or texto_entidade==\"'\" or texto_entidade=='\"' or texto_entidade=='.' or texto_entidade==';' or texto_entidade=='/' or texto_entidade=='(' or texto_entidade==')' or texto_entidade=='[' or texto_entidade==']':\n",
    "                pass\n",
    "            texto_entidade_comparar=texto_entidade.replace('/','').replace(')','').replace('(','').replace(']','').replace('[','').replace(',','').replace('.','').replace(';','').replace('-','').replace('+','').replace(\"'\",'')\n",
    "            texto_entidade_comparar = replaceWhiteSpaces(texto_entidade_comparar)\n",
    "            texto_frase_comparar = ' '.join(frase[inicio:fim+1]).strip().replace('/','').replace(')','').replace('(','').replace(']','').replace('[','').replace(',','').replace('.','').replace(';','').replace('-','').replace('+','').replace(\"'\",'')\n",
    "            texto_frase_comparar = replaceWhiteSpaces(texto_frase_comparar)\n",
    "            texto_entidade_comparar = texto_entidade_comparar.lower()\n",
    "            texto_frase_comparar = texto_frase_comparar.lower()\n",
    "            #print('texto_entidade_comparar:', texto_entidade_comparar)\n",
    "            #print('texto_frase_comparar:', texto_frase_comparar)\n",
    "            if (texto_entidade_comparar == texto_frase_comparar):\n",
    "                num_positivas=num_positivas+1\n",
    "                combinacaoEntidadesPos.append([entidade_frase, ' '.join(frase).strip(), labels[tipo_entidade]]) # apendando entidades reais\n",
    "            else:\n",
    "                print('erro, key:', key)\n",
    "                erro_corpus=erro_corpus+1\n",
    "                erros_entidade.append(indices)\n",
    "                lista_erro_corpus.append([' '.join(frase).strip(), tipo_entidade, ' '.join(so_tokens), entidade])\n",
    "\n",
    "        if len(combinacaoEntidadesPos)>0:\n",
    "            random.shuffle(combinacaoEntidadesPos)\n",
    "        else:\n",
    "            num_frases_sem_entidade = num_frases_sem_entidade+1\n",
    "        if (num % 1000) ==0:\n",
    "            print('key:', key)\n",
    "\n",
    "        #print('combinacaoEntidadesNeg:',combinacaoEntidadesNeg)\n",
    "        combinacaoEntidades = combinacaoEntidades+combinacaoEntidadesPos\n",
    "        combinacaoEntidadesPos=list()\n",
    "  \n",
    "    print('erro_corpus:', erro_corpus)\n",
    "    print('num_frases_sem_entidade:', num_frases_sem_entidade)\n",
    "    print('len(combinacaoEntidades:)',len(combinacaoEntidades))\n",
    "    \n",
    "    return combinacaoEntidades\n",
    "\n",
    "#combinacaoEntidadesTestSentence = getCombinacaoEntidadesSentencePos(dic_sentencesTest)\n",
    "#combinacaoEntidadesTrain, pulando_termos_postaggerTrain = getCombinacaoEntidades(dic_sentencesTrain, True, dicPosTagger, 0)\n",
    "#combinacaoEntidadesTestSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Train--\n",
      "Sentence Pairs - Sem filtro-postagger\n",
      "Sentence Pairs - Sem taxa de Downsampling\n",
      "key: 999\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 237\n",
      "len(combinacaoEntidades:) 9696\n",
      "\n",
      "--Dev--\n",
      "Sentence Pairs - Sem filtro-postagger\n",
      "Sentence Pairs - Sem taxa de Downsampling\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 89\n",
      "len(combinacaoEntidades:) 2159\n",
      "\n",
      "--Test--\n",
      "Sentence Pairs - Sem filtro-postagger\n",
      "Sentence Pairs - Sem taxa de Downsampling\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 107\n",
      "len(combinacaoEntidades:) 3233\n"
     ]
    }
   ],
   "source": [
    "print('--Train--')\n",
    "combinacaoEntidadesTrainSentence= getCombinacaoEntidadesSentence(dic_sentencesTrain, False, '', 0)\n",
    "print('\\n--Dev--')\n",
    "combinacaoEntidadesDevSentence = getCombinacaoEntidadesSentence(dic_sentencesDev, False, '', 0)\n",
    "print('\\n--Test--')\n",
    "combinacaoEntidadesTestSentence = getCombinacaoEntidadesSentence(dic_sentencesTest, False, '', 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gravarArquivosTreinamentoSentence(path, combinacaoEntidadesTrain, combinacaoEntidadesDev, combinacaoEntidadesTest):\n",
    "\n",
    "    # já ir gravando arquivos treinamento, test e dev...\n",
    "    # pra fazer um teste sem descontinuas\n",
    "    numTotalEntidades=0\n",
    "    numTotalEntidadesTrain=0\n",
    "    numTotalEntidadesDev=0\n",
    "    numTotalEntidadesTest=0\n",
    "\n",
    "    existeDir = os.path.exists(path)\n",
    "    if not existeDir:\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    f_train = open(path+r'\\sentence_pairs.train', 'w', encoding='utf-8')\n",
    "\n",
    "    for i, combinacaoEntidades in enumerate(combinacaoEntidadesTrain):\n",
    "        if len(combinacaoEntidades)>0:\n",
    "            termo = combinacaoEntidades[0]\n",
    "            frase = combinacaoEntidades[1]\n",
    "            label = str(combinacaoEntidades[2])\n",
    "            f_train.write(termo+'\\t'+frase+'\\t'+label+'\\n')\n",
    "            numTotalEntidades=numTotalEntidades+1\n",
    "            numTotalEntidadesTrain=numTotalEntidadesTrain+1\n",
    "    f_train.close()\n",
    "\n",
    "    f_dev = open(path+r'\\sentence_pairs.dev', 'w', encoding='utf-8')\n",
    "\n",
    "    for i, combinacaoEntidades in enumerate(combinacaoEntidadesDev):\n",
    "        if len(combinacaoEntidades)>0:\n",
    "            termo = combinacaoEntidades[0]\n",
    "            frase = combinacaoEntidades[1]\n",
    "            label = str(combinacaoEntidades[2])\n",
    "            f_dev.write(termo+'\\t'+frase+'\\t'+label+'\\n')\n",
    "            numTotalEntidades=numTotalEntidades+1\n",
    "            numTotalEntidadesDev=numTotalEntidadesDev+1\n",
    "    f_dev.close()\n",
    "\n",
    "    f_test = open(path+r'\\sentence_pairs.test', 'w', encoding='utf-8')\n",
    "    for i, combinacaoEntidades in enumerate(combinacaoEntidadesTest):\n",
    "        #print(dicSentences[i])\n",
    "        if len(combinacaoEntidades)>0:\n",
    "            termo = combinacaoEntidades[0]\n",
    "            frase = combinacaoEntidades[1]\n",
    "            label = str(combinacaoEntidades[2])\n",
    "            f_test.write(termo+'\\t'+frase+'\\t'+label+'\\n')\n",
    "            numTotalEntidades=numTotalEntidades+1\n",
    "            numTotalEntidadesTest=numTotalEntidadesTest+1\n",
    "    f_test.close()\n",
    "\n",
    "    print('numTotalEntidades:', numTotalEntidades)\n",
    "    print('numTotalEntidadesTrain:', numTotalEntidadesTrain)\n",
    "    print('numTotalEntidadesDev:', numTotalEntidadesDev)\n",
    "    print('numTotalEntidadesTest:', numTotalEntidadesTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numTotalEntidades: 15088\n",
      "numTotalEntidadesTrain: 9696\n",
      "numTotalEntidadesDev: 2159\n",
      "numTotalEntidadesTest: 3233\n"
     ]
    }
   ],
   "source": [
    "gravarArquivosTreinamentoSentence('sentence-pairs-all',combinacaoEntidadesTrainSentence, combinacaoEntidadesDevSentence, combinacaoEntidadesTestSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Train--\n",
      "Sentence Pairs - So positivos\n",
      "key: 999\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 237\n",
      "len(combinacaoEntidades:) 2510\n",
      "\n",
      "--Dev--\n",
      "Sentence Pairs - So positivos\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 89\n",
      "len(combinacaoEntidades:) 703\n",
      "\n",
      "--Test--\n",
      "Sentence Pairs - So positivos\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 107\n",
      "len(combinacaoEntidades:) 991\n",
      "numTotalEntidades: 4204\n",
      "numTotalEntidadesTrain: 2510\n",
      "numTotalEntidadesDev: 703\n",
      "numTotalEntidadesTest: 991\n"
     ]
    }
   ],
   "source": [
    "print('--Train--')\n",
    "combinacaoEntidadesTrainSentencePos= getCombinacaoEntidadesSentencePos(dic_sentencesTrain)\n",
    "print('\\n--Dev--')\n",
    "combinacaoEntidadesDevSentencePos = getCombinacaoEntidadesSentencePos(dic_sentencesDev)\n",
    "print('\\n--Test--')\n",
    "combinacaoEntidadesTestSentencePos = getCombinacaoEntidadesSentencePos(dic_sentencesTest)\n",
    "\n",
    "gravarArquivosTreinamentoSentence('sentence-pairs-positivos',combinacaoEntidadesTrainSentencePos, combinacaoEntidadesDevSentencePos, combinacaoEntidadesTestSentencePos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Train--\n",
      "Sentence Pairs - Com filtro-postagger\n",
      "Sentence Pairs - Com taxa de Downsampling de  1\n",
      "key: 999\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 237\n",
      "len(combinacaoEntidades:) 4062\n",
      "\n",
      "--Dev--\n",
      "Sentence Pairs - Com filtro-postagger\n",
      "Sentence Pairs - Com taxa de Downsampling de  1\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 89\n",
      "len(combinacaoEntidades:) 1048\n",
      "\n",
      "--Test--\n",
      "Sentence Pairs - Com filtro-postagger\n",
      "Sentence Pairs - Com taxa de Downsampling de  1\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 107\n",
      "len(combinacaoEntidades:) 1555\n",
      "numTotalEntidades: 6665\n",
      "numTotalEntidadesTrain: 4062\n",
      "numTotalEntidadesDev: 1048\n",
      "numTotalEntidadesTest: 1555\n"
     ]
    }
   ],
   "source": [
    "print('--Train--')\n",
    "combinacaoEntidadesTrainSentence= getCombinacaoEntidadesSentence(dic_sentencesTrain, True, dicPosTagger, 1)\n",
    "print('\\n--Dev--')\n",
    "combinacaoEntidadesDevSentence = getCombinacaoEntidadesSentence(dic_sentencesDev, True, dicPosTagger, 1)\n",
    "print('\\n--Test--')\n",
    "combinacaoEntidadesTestSentence = getCombinacaoEntidadesSentence(dic_sentencesTest, True, dicPosTagger, 1)\n",
    "gravarArquivosTreinamentoSentence('sentence-pairs-downsampling',combinacaoEntidadesTrainSentence, combinacaoEntidadesDevSentence, combinacaoEntidadesTestSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Train--\n",
      "Sentence Pairs - Com filtro-postagger\n",
      "Sentence Pairs - Com taxa de Downsampling de  1\n",
      "key: 999\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 237\n",
      "len(combinacaoEntidades:) 5037\n",
      "\n",
      "--Dev--\n",
      "Sentence Pairs - Com filtro-postagger\n",
      "Sentence Pairs - Com taxa de Downsampling de  1\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 89\n",
      "len(combinacaoEntidades:) 1264\n",
      "\n",
      "--Test--\n",
      "Sentence Pairs - Com filtro-postagger\n",
      "Sentence Pairs - Com taxa de Downsampling de  1\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 107\n",
      "len(combinacaoEntidades:) 1917\n"
     ]
    }
   ],
   "source": [
    "#OLD\n",
    "print('--Train--')\n",
    "combinacaoEntidadesTrainSentence= getCombinacaoEntidadesSentence(dic_sentencesTrain, True, dicPosTagger, 1)\n",
    "print('\\n--Dev--')\n",
    "combinacaoEntidadesDevSentence = getCombinacaoEntidadesSentence(dic_sentencesDev, True, dicPosTagger, 1)\n",
    "print('\\n--Test--')\n",
    "combinacaoEntidadesTestSentence = getCombinacaoEntidadesSentence(dic_sentencesTest, True, dicPosTagger, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numTotalEntidades: 8218\n",
      "numTotalEntidadesTrain: 5037\n",
      "numTotalEntidadesDev: 1264\n",
      "numTotalEntidadesTest: 1917\n"
     ]
    }
   ],
   "source": [
    "#gravarArquivosTreinamentoSentence('sentence-pairs-filtro',combinacaoEntidadesTrainSentence, combinacaoEntidadesDevSentence, combinacaoEntidadesTestSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Train--\n",
      "Sentence Pairs - Com filtro-postagger\n",
      "Sentence Pairs - Sem taxa de Downsampling\n",
      "key: 999\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 237\n",
      "len(combinacaoEntidades:) 6753\n",
      "\n",
      "--Dev--\n",
      "Sentence Pairs - Com filtro-postagger\n",
      "Sentence Pairs - Sem taxa de Downsampling\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 89\n",
      "len(combinacaoEntidades:) 1635\n",
      "\n",
      "--Test--\n",
      "Sentence Pairs - Com filtro-postagger\n",
      "Sentence Pairs - Sem taxa de Downsampling\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 107\n",
      "len(combinacaoEntidades:) 2478\n",
      "numTotalEntidades: 10866\n",
      "numTotalEntidadesTrain: 6753\n",
      "numTotalEntidadesDev: 1635\n",
      "numTotalEntidadesTest: 2478\n"
     ]
    }
   ],
   "source": [
    "print('--Train--')\n",
    "combinacaoEntidadesTrainSentence= getCombinacaoEntidadesSentence(dic_sentencesTrain, True, dicPosTagger, 0)\n",
    "print('\\n--Dev--')\n",
    "combinacaoEntidadesDevSentence = getCombinacaoEntidadesSentence(dic_sentencesDev, True, dicPosTagger, 0)\n",
    "print('\\n--Test--')\n",
    "combinacaoEntidadesTestSentence = getCombinacaoEntidadesSentence(dic_sentencesTest, True, dicPosTagger, 0)\n",
    "gravarArquivosTreinamentoSentence('sentence-pairs-filtro',combinacaoEntidadesTrainSentence, combinacaoEntidadesDevSentence, combinacaoEntidadesTestSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Train--\n",
      "Sentence Pairs - Com filtro-postagger\n",
      "Sentence Pairs - Sem taxa de Downsampling\n",
      "key: 999\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 237\n",
      "len(combinacaoEntidades:) 6753\n",
      "\n",
      "--Dev--\n",
      "Sentence Pairs - Com filtro-postagger\n",
      "Sentence Pairs - Sem taxa de Downsampling\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 89\n",
      "len(combinacaoEntidades:) 1635\n",
      "\n",
      "--Test--\n",
      "Sentence Pairs - Com filtro-postagger\n",
      "Sentence Pairs - Sem taxa de Downsampling\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 107\n",
      "len(combinacaoEntidades:) 2478\n",
      "numTotalEntidades: 10866\n",
      "numTotalEntidadesTrain: 6753\n",
      "numTotalEntidadesDev: 1635\n",
      "numTotalEntidadesTest: 2478\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "print('--Train--')\n",
    "combinacaoEntidadesTrainSentence= getCombinacaoEntidadesSentence(dic_sentencesTrain, True, dicPosTagger, 0)\n",
    "print('\\n--Dev--')\n",
    "combinacaoEntidadesDevSentence = getCombinacaoEntidadesSentence(dic_sentencesDev, True, dicPosTagger, 0)\n",
    "print('\\n--Test--')\n",
    "combinacaoEntidadesTestSentence = getCombinacaoEntidadesSentence(dic_sentencesTest, True, dicPosTagger, 0)\n",
    "gravarArquivosTreinamentoSentence('sentence-pairs-downsampling',combinacaoEntidadesTrainSentence, combinacaoEntidadesDevSentence, combinacaoEntidadesTestSentence)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinacaoEntidadesTestSentence[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parte 2.2 - tentar tirar verbos, CC, etc do O para nao ficar com mto FP\n",
    "### depois, com nosso pos tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3484"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"pucpr-br/postagger-bio-portuguese\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('pucpr-br/postagger-bio-portuguese')\n",
    "\n",
    "nlp_token_class = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy='max')\n",
    "\n",
    "def getDicPosTagger(dic_sentencesTrainDev):\n",
    "    dicPostagger = load_obj('dic_postagger')\n",
    "    allFrases = load_obj('allFrases')\n",
    "    if dicPostagger==None or allFrases==None:\n",
    "        dicPostagger = {}\n",
    "        allFrases=[]\n",
    "        for key, value in dic_sentencesTrainDev.items():\n",
    "            tokens=value[0]\n",
    "            frase = [t[0] for t in tokens]\n",
    "            frase = ' '.join(frase)\n",
    "            allFrases.append(frase)\n",
    "            #print(frase)\n",
    "\n",
    "        #print(allFrases)\n",
    "        doc = nlp_token_class(allFrases)\n",
    "        #print(doc)\n",
    "        for frase in doc:\n",
    "            for d in frase:\n",
    "                #print(d)\n",
    "                pos = d['entity_group']\n",
    "                #print(pos)\n",
    "                token=d['word']\n",
    "                if pos=='PREP+ART':\n",
    "                    pos='ART'\n",
    "                if pos=='NPROP':\n",
    "                    pos='N'\n",
    "                if 'ADV' in pos: # ADV-KS, ADV-KS-REL\t\n",
    "                    pos='ADV'\n",
    "                if 'PRON' in pos: # PRO-KS\t, PRO-KS-REL, PROPESS, PROPSUB\n",
    "                    pos='PRON'\n",
    "                if pos=='VAUX'or pos=='PCP': #participio\n",
    "                    pos='V'\n",
    "                dicPostagger[token] = pos    \n",
    "\n",
    "        save_obj('dicPostagger', dicPostagger)\n",
    "        save_obj('allFrases', allFrases)\n",
    "    return dicPostagger, allFrases\n",
    "\n",
    "dic_sentencesTrainDev = load_obj('dic_sentencesTrainDev')\n",
    "\n",
    "dicPosTagger, _ = getDicPosTagger(dic_sentencesTrainDev)\n",
    "#dicPosTagger, _ = getDicPosTagger(dic_sentencesTest)\n",
    "len(dicPosTagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-N-PREP-ART-N-ADV-PU'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tipoPostaggerTokens(entidade_token, dicPostagger):\n",
    "    postagger = ''\n",
    "    for p in entidade_token:\n",
    "        #print('p:', p)\n",
    "        if p.lower() in dicPostagger.keys():\n",
    "            postagger = postagger + '-' + dicPostagger.get(p.lower())\n",
    "        else:\n",
    "            #print('nao tem:', p)\n",
    "            # se nao tem, considera N\n",
    "            postagger = postagger + '-' + 'N'\n",
    "    return postagger\n",
    "tipoPostaggerTokens(['ola','como','aos','você','hoje', '?'], dicPosTagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_sentencesTrainDev = load_obj('dic_sentencesTrainDev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-N-ADJ-ART-N',\n",
       " '-N-N-N-ART-N-ART-N',\n",
       " '-N',\n",
       " '-N-ADJ',\n",
       " '-ADJ',\n",
       " '-V',\n",
       " '-N-NUM',\n",
       " '-N-PU-ADJ-N',\n",
       " '-ADJ-N-N-ADJ-PREP-N',\n",
       " '-N-PREP-N-ART-N']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getListaPostaggerEntidades(dic_sentencesTrainDev, dicPosTagger):\n",
    "    lista_postaggers_entidades = []\n",
    "    for key, value in dic_sentencesTrainDev.items():\n",
    "        entidades = value[1]\n",
    "        for entidade in entidades:\n",
    "            #print(entidade[0])\n",
    "            pos_tagger=tipoPostaggerTokens(entidade[0].split(), dicPosTagger)\n",
    "            if pos_tagger not in lista_postaggers_entidades:\n",
    "                lista_postaggers_entidades.append(pos_tagger)\n",
    "    return lista_postaggers_entidades\n",
    "\n",
    "lista_postaggers_entidades = getListaPostaggerEntidades(dic_sentencesTrainDev, dicPosTagger)\n",
    "lista_postaggers_entidades[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-N-ADJ-ADJ-N-N-N',\n",
       " '-N-ADJ-N-N-N',\n",
       " '-ADJ-N-N-N',\n",
       " '-N-ADV-N-N-PU-NUM-N',\n",
       " '-ADJ-N-PREP-N',\n",
       " '-N-PREP-N-N-ADV',\n",
       " '-V-PDEN-PREP-ADJ-N',\n",
       " '-N-N-PREP-N-PU-N-PU-N-PU-N-PREP-N',\n",
       " '-ADJ-N-PREP-N-ADJ']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_postaggers_entidades[-10:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print('-KC' in lista_postaggers_entidades) # e\n",
    "print('-PREP+ART' in lista_postaggers_entidades) \n",
    "print('-PREP' in lista_postaggers_entidades)\n",
    "print('-ART' in lista_postaggers_entidades)\n",
    "print('-V' in lista_postaggers_entidades)\n",
    "print('-ADJ' in lista_postaggers_entidades)\n",
    "print('-PU' in lista_postaggers_entidades)\n",
    "print('-PCP' in lista_postaggers_entidades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Train--\n",
      "Com filtro-postagger\n",
      "Sem taxa de Downsampling\n",
      "key: 999\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 237\n",
      "len(combinacaoEntidadesAll:) 1319\n",
      "len(pulando_termos_postagger): 3925\n",
      "\n",
      "--Dev--\n",
      "Com filtro-postagger\n",
      "Sem taxa de Downsampling\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 89\n",
      "len(combinacaoEntidadesAll:) 416\n",
      "len(pulando_termos_postagger): 745\n",
      "\n",
      "--Test--\n",
      "Com filtro-postagger\n",
      "Sem taxa de Downsampling\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 107\n",
      "len(combinacaoEntidadesAll:) 506\n",
      "len(pulando_termos_postagger): 1183\n"
     ]
    }
   ],
   "source": [
    "print('--Train--')\n",
    "combinacaoEntidadesTrain, pulando_termos_postaggerTrain = getCombinacaoEntidades(dic_sentencesTrain, True, dicPosTagger, 0)\n",
    "print('\\n--Dev--')\n",
    "combinacaoEntidadesDev, pulando_termos_postaggerDev= getCombinacaoEntidades(dic_sentencesDev, True, dicPosTagger, 0)\n",
    "print('\\n--Test--')\n",
    "combinacaoEntidadesTest, pulando_termos_postaggerTest= getCombinacaoEntidades(dic_sentencesTest, True, dicPosTagger, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aumento moderado de átrio esquerdo .',\n",
       " [['aumento moderado <e1> de átrio esquerdo </e1> .', 'O'],\n",
       "  ['aumento <e1> moderado </e1> de átrio esquerdo .', 'O'],\n",
       "  ['aumento moderado <e1> de </e1> átrio esquerdo .', 'O'],\n",
       "  ['aumento <e1> moderado de </e1> átrio esquerdo .', 'O'],\n",
       "  ['<e1> aumento moderado de átrio esquerdo </e1> .', 'Problema'],\n",
       "  ['aumento moderado de <e1> átrio esquerdo </e1> .', 'Anatomia'],\n",
       "  ['aumento moderado de átrio <e1> esquerdo </e1> .', 'O'],\n",
       "  ['aumento <e1> moderado de átrio esquerdo </e1> .', 'O'],\n",
       "  ['aumento moderado <e1> de átrio </e1> esquerdo .', 'O'],\n",
       "  ['aumento <e1> moderado de átrio </e1> esquerdo .', 'O'],\n",
       "  ['<e1> aumento </e1> moderado de átrio esquerdo .', 'O'],\n",
       "  ['aumento moderado de <e1> átrio </e1> esquerdo .', 'O']]]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinacaoEntidadesTest[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['25'], '-NUM'],\n",
       " [['ventrículo', 'esquerdo', 'com'], '-N-ADJ-PREP'],\n",
       " [['ventrículo', 'esquerdo', 'com', 'hipertrofia', 'concentrica', 'de'],\n",
       "  '-N-ADJ-PREP-N-N-PREP'],\n",
       " [['ventrículo',\n",
       "   'esquerdo',\n",
       "   'com',\n",
       "   'hipertrofia',\n",
       "   'concentrica',\n",
       "   'de',\n",
       "   'grau'],\n",
       "  '-N-ADJ-PREP-N-N-PREP-N'],\n",
       " [['ventrículo',\n",
       "   'esquerdo',\n",
       "   'com',\n",
       "   'hipertrofia',\n",
       "   'concentrica',\n",
       "   'de',\n",
       "   'grau',\n",
       "   'discreto'],\n",
       "  '-N-ADJ-PREP-N-N-PREP-N-ADJ'],\n",
       " [['esquerdo', 'com'], '-ADJ-PREP'],\n",
       " [['esquerdo', 'com', 'hipertrofia'], '-ADJ-PREP-N'],\n",
       " [['esquerdo', 'com', 'hipertrofia', 'concentrica'], '-ADJ-PREP-N-N'],\n",
       " [['esquerdo', 'com', 'hipertrofia', 'concentrica', 'de'],\n",
       "  '-ADJ-PREP-N-N-PREP'],\n",
       " [['esquerdo', 'com', 'hipertrofia', 'concentrica', 'de', 'grau'],\n",
       "  '-ADJ-PREP-N-N-PREP-N'],\n",
       " [['esquerdo', 'com', 'hipertrofia', 'concentrica', 'de', 'grau', 'discreto'],\n",
       "  '-ADJ-PREP-N-N-PREP-N-ADJ'],\n",
       " [['com', 'hipertrofia', 'concentrica', 'de'], '-PREP-N-N-PREP'],\n",
       " [['com', 'hipertrofia', 'concentrica', 'de', 'grau'], '-PREP-N-N-PREP-N'],\n",
       " [['com', 'hipertrofia', 'concentrica', 'de', 'grau', 'discreto'],\n",
       "  '-PREP-N-N-PREP-N-ADJ'],\n",
       " [['hipertrofia', 'concentrica', 'de'], '-N-N-PREP'],\n",
       " [['hipertrofia', 'concentrica', 'de', 'grau', 'discreto'], '-N-N-PREP-N-ADJ'],\n",
       " [['aumento', 'moderado'], '-V-N'],\n",
       " [['aumento', 'moderado', 'de'], '-V-N-PREP'],\n",
       " [['aumento', 'moderado', 'de', 'átrio'], '-V-N-PREP-N'],\n",
       " [['aumento', 'moderado', 'de', 'átrio', 'esquerdo'], '-V-N-PREP-N-ADJ'],\n",
       " [['calcificação', 'mitral', 'e'], '-N-ADJ-KC'],\n",
       " [['calcificação', 'mitral', 'e', 'aórtica'], '-N-ADJ-KC-N'],\n",
       " [['calcificação', 'mitral', 'e', 'aórtica', 'com'], '-N-ADJ-KC-N-PREP'],\n",
       " [['calcificação', 'mitral', 'e', 'aórtica', 'com', 'refluxo'],\n",
       "  '-N-ADJ-KC-N-PREP-N'],\n",
       " [['calcificação', 'mitral', 'e', 'aórtica', 'com', 'refluxo', 'leve'],\n",
       "  '-N-ADJ-KC-N-PREP-N-ADJ'],\n",
       " [['mitral', 'e'], '-ADJ-KC'],\n",
       " [['mitral', 'e', 'aórtica'], '-ADJ-KC-N'],\n",
       " [['mitral', 'e', 'aórtica', 'com'], '-ADJ-KC-N-PREP'],\n",
       " [['mitral', 'e', 'aórtica', 'com', 'refluxo'], '-ADJ-KC-N-PREP-N'],\n",
       " [['mitral', 'e', 'aórtica', 'com', 'refluxo', 'leve'],\n",
       "  '-ADJ-KC-N-PREP-N-ADJ'],\n",
       " [['e'], '-KC'],\n",
       " [['e', 'aórtica'], '-KC-N'],\n",
       " [['e', 'aórtica', 'com'], '-KC-N-PREP'],\n",
       " [['e', 'aórtica', 'com', 'refluxo'], '-KC-N-PREP-N'],\n",
       " [['e', 'aórtica', 'com', 'refluxo', 'leve'], '-KC-N-PREP-N-ADJ'],\n",
       " [['nos', 'MMII'], '-ART-N'],\n",
       " [['comprometimento', 'difuso', 'do'], '-N-N-ART'],\n",
       " [['comprometimento', 'difuso', 'do', 'VE', 'grau'], '-N-N-ART-N-N'],\n",
       " [['comprometimento', 'difuso', 'do', 'VE', 'grau', 'moderado'],\n",
       "  '-N-N-ART-N-N-N'],\n",
       " [['difuso', 'do', 'VE', 'grau', 'moderado'], '-N-ART-N-N-N'],\n",
       " [['do', 'VE'], '-ART-N'],\n",
       " [['do', 'VE', 'grau'], '-ART-N-N'],\n",
       " [['do', 'VE', 'grau', 'moderado'], '-ART-N-N-N'],\n",
       " [['fraqueza', 'intermitente', 'em'], '-N-N-PREP'],\n",
       " [['queixas', 'urinarias', 'e'], '-N-N-KC'],\n",
       " [['queixas', 'urinarias', 'e', 'gastrointestinais'], '-N-N-KC-N'],\n",
       " [['urinarias', 'e', 'gastrointestinais'], '-N-KC-N'],\n",
       " [['e'], '-KC'],\n",
       " [['e', 'gastrointestinais'], '-KC-N'],\n",
       " [['/'], '-PU']]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pulando_termos_postaggerTest[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Dispneia', 'importante', 'aos'], '-N-ADJ-ART'],\n",
       " [['importante', 'aos'], '-ADJ-ART'],\n",
       " [['importante', 'aos', 'esforços'], '-ADJ-ART-N'],\n",
       " [['aos', 'esforços'], '-ART-N'],\n",
       " [['dor', 'tipo', 'peso', 'no'], '-N-N-N-ART'],\n",
       " [['dor', 'tipo', 'peso', 'no', 'peito'], '-N-N-N-ART-N'],\n",
       " [['dor', 'tipo', 'peso', 'no', 'peito', 'no'], '-N-N-N-ART-N-ART'],\n",
       " [['tipo', 'peso', 'no'], '-N-N-ART'],\n",
       " [['tipo', 'peso', 'no', 'peito', 'no'], '-N-N-ART-N-ART'],\n",
       " [['tipo', 'peso', 'no', 'peito', 'no', 'esforço'], '-N-N-ART-N-ART-N'],\n",
       " [['peso', 'no', 'peito', 'no'], '-N-ART-N-ART'],\n",
       " [['no', 'peito'], '-ART-N'],\n",
       " [['no', 'peito', 'no'], '-ART-N-ART'],\n",
       " [['no', 'peito', 'no', 'esforço'], '-ART-N-ART-N'],\n",
       " [['no', 'esforço'], '-ART-N'],\n",
       " [['100'], '-NUM'],\n",
       " [['25'], '-NUM'],\n",
       " [['175'], '-NUM'],\n",
       " [['40'], '-NUM'],\n",
       " [['20'], '-NUM']]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pulando_termos_postaggerTrain[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numTotalEntidades: 10626\n",
      "numTotalEntidadesTrain: 6578\n",
      "numTotalEntidadesDev: 1611\n",
      "numTotalEntidadesTest: 2437\n"
     ]
    }
   ],
   "source": [
    "gravarArquivosTreinamento('com_filtro',combinacaoEntidadesTrain, combinacaoEntidadesDev, combinacaoEntidadesTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numTotalEntidades: 8965\n",
      "numTotalEntidadesTrain: 5533\n",
      "numTotalEntidadesDev: 1389\n",
      "numTotalEntidadesTest: 2043\n"
     ]
    }
   ],
   "source": [
    "#OLD\n",
    "#gravarArquivosTreinamento('com_filtro',combinacaoEntidadesTrain, combinacaoEntidadesDev, combinacaoEntidadesTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Train--\n",
      "Com filtro-postagger\n",
      "Com taxa de Downsampling de  1\n",
      "key: 999\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 237\n",
      "len(combinacaoEntidadesAll:) 1319\n",
      "len(pulando_termos_postagger): 3925\n",
      "\n",
      "--Dev--\n",
      "Com filtro-postagger\n",
      "Com taxa de Downsampling de  1\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 89\n",
      "len(combinacaoEntidadesAll:) 416\n",
      "len(pulando_termos_postagger): 745\n",
      "\n",
      "--Test--\n",
      "Com filtro-postagger\n",
      "Com taxa de Downsampling de  1\n",
      "erro_corpus: 0\n",
      "num_frases_sem_entidade: 107\n",
      "len(combinacaoEntidadesAll:) 506\n",
      "len(pulando_termos_postagger): 1183\n",
      "numTotalEntidades: 6664\n",
      "numTotalEntidadesTrain: 4055\n",
      "numTotalEntidadesDev: 1050\n",
      "numTotalEntidadesTest: 1559\n"
     ]
    }
   ],
   "source": [
    "# com downsampling\n",
    "print('--Train--')\n",
    "combinacaoEntidadesTrain, pulando_termos_postaggerTrain = getCombinacaoEntidades(dic_sentencesTrain, True, dicPosTagger, 1)\n",
    "print('\\n--Dev--')\n",
    "combinacaoEntidadesDev, pulando_termos_postaggerDev= getCombinacaoEntidades(dic_sentencesDev, True, dicPosTagger, 1)\n",
    "print('\\n--Test--')\n",
    "combinacaoEntidadesTest, pulando_termos_postaggerTest= getCombinacaoEntidades(dic_sentencesTest, True, dicPosTagger, 1)\n",
    "\n",
    "gravarArquivosTreinamento('com-filtro-downsampling',combinacaoEntidadesTrain, combinacaoEntidadesDev, combinacaoEntidadesTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agora, tratar descontinuas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1568"
      ]
     },
     "execution_count": 837,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combinacaoEntidadesAll_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16614"
      ]
     },
     "execution_count": 838,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combinacaoEntidadesAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numTreinamento: 13291\n",
      "numTeste: 1661\n",
      "numTotalEntidades: 165571\n",
      "numTotalEntidadesTrain: 131376\n",
      "numTotalEntidadesDev: 16986\n",
      "numTotalEntidadesTest: 17209\n"
     ]
    }
   ],
   "source": [
    "# já ir gravando arquivos treinamento, test e dev...\n",
    "# pra fazer um teste sem descontinuas\n",
    "numTreinamento = len(combinacaoEntidadesAll)*0.8\n",
    "numTreinamento = int(numTreinamento)\n",
    "numTeste = (len(combinacaoEntidadesAll) - numTreinamento)/2\n",
    "numTeste = int(numTeste)\n",
    "numTotalEntidades=0\n",
    "numTotalEntidadesTrain=0\n",
    "numTotalEntidadesDev=0\n",
    "numTotalEntidadesTest=0\n",
    "\n",
    "print('numTreinamento:', numTreinamento)\n",
    "print('numTeste:', numTeste)\n",
    "\n",
    "f_train = open('genia.train', 'w')\n",
    "f_dev = open('genia.dev', 'w')\n",
    "f_test = open('genia.test', 'w')\n",
    "f_dev_entidades = open('genia_entidades.dev', 'w')\n",
    "f_test_entidades = open('genia_entidades.test', 'w')\n",
    "\n",
    "for i, combinacaoEntidades in enumerate(combinacaoEntidadesAll):\n",
    "    #print(dicSentences[i])\n",
    "    frase = combinacaoEntidades[0]\n",
    "    frases_entidade = combinacaoEntidades[1]\n",
    "    if i<=numTreinamento:\n",
    "        f_train.write(frase+'\\n')\n",
    "        for frase_entidade in frases_entidade:\n",
    "            f_train.write(frase_entidade[1]+'\\t'+frase_entidade[0]+'\\n')\n",
    "            numTotalEntidades=numTotalEntidades+1\n",
    "            numTotalEntidadesTrain=numTotalEntidadesTrain+1\n",
    "    elif i <= numTreinamento + numTeste:\n",
    "        f_dev.write(frase+'\\n')\n",
    "        for frase_entidade in frases_entidade:\n",
    "            f_dev.write(frase_entidade[1]+'\\t'+frase_entidade[0]+'\\n')\n",
    "            f_dev_entidades.write(frase_entidade[1]+'\\t'+frase_entidade[0]+'\\n')\n",
    "            numTotalEntidades=numTotalEntidades+1\n",
    "            numTotalEntidadesDev=numTotalEntidadesDev+1\n",
    "    else:\n",
    "        f_test.write(frase+'\\n')\n",
    "        for frase_entidade in frases_entidade:\n",
    "            f_test.write(frase_entidade[1]+'\\t'+frase_entidade[0]+'\\n')\n",
    "            f_test_entidades.write(frase_entidade[1]+'\\t'+frase_entidade[0]+'\\n')\n",
    "            numTotalEntidades=numTotalEntidades+1\n",
    "            numTotalEntidadesTest=numTotalEntidadesTest+1\n",
    "\n",
    "f_train.close()\n",
    "f_test.close()\n",
    "f_dev.close()\n",
    "f_test_entidades.close()\n",
    "f_dev_entidades.close()\n",
    "\n",
    "print('numTotalEntidades:', numTotalEntidades)\n",
    "print('numTotalEntidadesTrain:', numTotalEntidadesTrain)\n",
    "print('numTotalEntidadesDev:', numTotalEntidadesDev)\n",
    "print('numTotalEntidadesTest:', numTotalEntidadesTest)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
